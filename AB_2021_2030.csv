AB,NO
"visible (vis) and infrared (ir) image fusion (viif) is a technique used to synthesize the fused image of high visual perception. the existing fusion methods typically work by discovering the commons underlying the two modalities and fusing them in the common space. however, these methods often ignore the modality differences, such as fuzzy details in the ir image, and their well-designed architectures also lead to slow fusion speed. to address these issues, we propose a real-time end-to-end viif model based on layer decomposition and re-parameterization (ldrepfm). this model is composed of a layer decomposition guidance network (ldgnet) and a re-parameterization fusion network (repfnet). first, the ldgnet is used to alleviate the visual quality degradation of the fused image by decomposing the ir image into structural layer and fuzzy layer. second, in order to achieve a favorable trade-off between the fusion speed and evaluation metrics, the repfnet is utilized to decouple the training-time multibranch and inference-time plain architecture. third, the structural layer that has been decomposed by ldgnet is utilized in constructing the guidance fusion loss, which is aimed at optimizing repfnet. finally, experiments conducted on the publicly available tno, roadscene, m3fd, and regdb datasets demonstrate the performance of the proposed method to be comparable to the state of the art (sota) in terms of both visual effect and quantitative metrics. the code is publically available at https://github.com/luming1314/ldrepfm.",AB_0203
"in recent years, convolutional neural networks (cnns) have been widely used in hyperspectral image (hsi) classification because of their exceptional performance in local feature extraction. however, due to the local join and weight sharing properties of the convolution kernel, cnns have limitations in long-distance modeling, and deeper networks tend to increase computational costs. to address these issues, this article proposes a vision transformer (vit) based on the light self-gaussian-attention (lsga) mechanism, which extracts global deep semantic features. first, the hybrid spatial-spectral tokenizer module extracts shallow spatial-spectral features and expands image patches to generate tokens. next, the light self-attention uses q (query), x (origin input), and x instead of q, k (key), and v (value) to reduce the computation and parameters. furthermore, to avoid the lack of location information resulting in the aliasing of central and neighborhood features, we devise gaussian absolute position bias to simulate hsi data distribution and make the attention weight closer to the central query block. several experiments verify the effectiveness of the proposed method, which outperforms state-of-the-art methods on four datasets. specifically, we observed a 0.62% accuracy improvement over a2s2k and a 0.11% improvement over ssftt. in conclusion, the proposed lsga-vit method demonstrates promising results in the hsi classification and shows potential in addressing the issues of location-aware long-distance modeling and computational cost. our codes are available at https://github.com/machao132/lsga-vit.",AB_0203
"are we on the right way for remote sensing image understanding (rsiu) by training models in a supervised data-dependent and task-dependent manner, instead of original human vision in a label-free and task-independent way? we argue that a more desirable rsiu model should be trained with intrinsic structure from data rather than extrinsic human labels to realize generalizability across a wide range of rsiu tasks. according to this hypothesis, we proposed the original vision (tov) model in the remote sensing field. trained by massive unlabeled optical data along a human-like self-supervised learning (ssl) path that is from general knowledge to specialized knowledge, tov model can be easily adapted to various rsiu tasks, including scene classification, object detection, and semantic segmentation, and outperforms dominant imagenet supervised pretrained method as well as two recently proposed ssl pretrained methods on the majority of 12 publicly available benchmarks. moreover, we analyze the influences of two key factors on the performance of building tov model for rsiu, including the influence of using different data sampling methods and the selection of learning paths during self-supervised optimization. we believe that a general model which is trained in a label-free and task-independent way may be the next paradigm for rsiu and hope the insights distilled from this study can help to foster the development of an original vision model for rsiu. the source code is available at https://github.com/geox-lab/g-rsim/tree/main/tov_v1.",AB_0203
"as samples of steel defects are industrially limited, it is challenging for most deep learning methods that rely on ample labeled data to identify steel surface defects. recently, contrastive learning has achieved good performance in natural image classification tasks with few labeled samples, yet two obstacles prevent its effective application to steel surface defect images. one is that due to the presence of inter-class and intra-class similar samples in steel surface defect, the fixed contrast strength in contrastive learning will destroy the potential semantic information of defect samples. another is that contrastive learning requires a large amount of unlabeled data, whereas steel surface defect samples are insufficient. to overcome the above-mentioned problems, a novel framework named flexible and diverse contrastive learning (fdcl) is proposed. this framework consists of two parts, flexible contrast (fico) and diverse generative adversarial networks (dgans). diverse images generated by the dgan and real images are fed into fico for representation learning. in the fico, the contrast strength among samples is flexibly adjusted by the proposed variable temperature discrimination and feature reconstruction (fr). in addition, the output features (of) of fico will be used as input to the dgan generator to improve image quality, thus further facilitating representation learning. the proposed fdcl is implemented on four standard steel surface defect data sets, and the experimental results demonstrated that it achieves superior performance over state-of-the-art methods. our code is available at: https://github.com-/jiacongc/fdcl.",AB_0203
"automated exercise assessment is of great importance for patients under rehabilitation exercise who require professional guidance. among the existing approaches, the skeleton-based assessment model that classifies the correctness of the exercise has attracted much attention due to its relative ease of implementation and convenience in use. however, there are two problems with this approach. the first problem is its sensitivity to the orientation of the human skeleton. to solve this problem, we propose a novel rotation-invariant descriptor, the dot product matrix of the human skeleton, and prove mathematically that this descriptor discards only the orientation message that we do not expect while preserving all other useful information. lack of feedback from the system is the second problem, because the exercisers do not know which parts of their exercises are incorrect. therefore, we develop a visualization method for our system based on gradient-weighted class activation mapping (grad-cam) and an quantitative metric called overlap ratio (ovr) to measure the quality of the visualization result. to demonstrate the effect of our method, we conduct experiments on two public datasets and a self-generated push-up dataset. the experimental results show that our rotation-invariant descriptor can achieve absolute robustness to orientation even under severe angle perturbations. in terms of accuracy and ovr, our method even outperforms previous works in most cases, indicating that the rotation-invariant descriptor helps the assessment model to extract more stable features. the visualization results are also informative to correct the movements; some examples are presented in this paper. the code of this paper and our push-up dataset are publicly available at https://github.com/kelly510/rehabexerassess.",AB_0203
"aspect-level sentiment classification aims to determine the sentiment polarity of a sentence toward a given aspect term or aspect category. for sentiment classification toward a given aspect term, some opinions may exist that are not the given aspect term's modifiers because a sentence may contain more than one aspect term. hence, it is necessary to capture relevant opinion for a certain aspect term. to capture the nearest opinion of the aspect term, researchers have used the relative distance between an aspect term and all other words in a sentence. however, this can be infeasible when the sentence has a complex syntactic structure. in this paper, we introduce dependency relation to detect the dependency-related sentiment feature for the aspect term in the dependency parse tree, and integrate this relationship into the convolutional neural network and bidirectional long short-term memory. experiments show that the related sentiment features for an aspect term help models discriminate its sentiment polarity. the proposed models achieve state-of-the-art results among neural networks. the codes and datasets are released on https://github.com/littlesummer114/dw-cnn.",AB_0203
"jet fuel leaks not only waste resources and increase costs but also pose a risk of emergency landings and aviation accidents. with the blossom and implementation of deep learning, crack segmentation techniques have been rapidly developed in many fields. however, it is struggle to get accurate and complete crack segmentation results because of images' complex background environment. to address this issue, we collected and labeled a dataset of 2824 crack images from the surface of aircraft fuel tank, named caft2800. and this article presents an atrous spatial pyramid fusion (aspf) and hybrid attention network based on deep learning to deal with the complicated environment. the backbone of network uses a hierarchical structure swin transformer to extract features. in the neck of the network, an aspf module is proposed to further capture contextual information in multiple scale. unlike the atrous spatial pyramid pooling (aspp) module, which aggregates image-level information with pooling, we abandon this operation according to the characteristics of crack. and the neck structure is designed for fusing high-level semantic information to all scales. at the gate between the neck and the decoder head, a selective kernel (sk) block is embedded into the network to recalibrates channel-wise feature responses. due to the morphological characteristics of crack, we propose an evaluation index, thinning f1 score (tfscore), which is more meaningful compared to the commonly used f1 score. sufficient control experiments were conducted on the caft2800 dataset and two complicated environment benchmarks (deepcrack and gaps) to test the effectiveness of the network, and our method achieved superior performance. source code and the caft2800 are available at https://github.com/gu-eh/caft2800.",AB_0203
"stationary computed tomography (ct) system can not only improve image quality with motion artifact reduction, but also can help to visualize dynamic processes. how to develop a reliable stationary ct is a very challenging task. to address this challenge, we proposed a totally new addressable cold cathode flat-panel x-ray source-based stationary ct architecture, entitled a flat-panel addressable nanowire-cold-cathode stationary (fans) ct scanner. the fans scanner is a small and cheap system since the low-cost compact flat-panel x-ray source is used. additionally, it is also a stationary ct system since the flat-panel x-ray source has the ability to fast switch. finally, our fans system provides extinguished image quality by incorporating superior regularization priors. the experiments on numerical and clinical datasets further demonstrate the feasibility of our proposed fans ct imaging system. our code is released at https://zenodo.org/record/7751804#.zbg2qmjbzey.",AB_0203
"monitoring the operating status of the pipeline and determining the location of the leak in time are very important to ensure the safe operation of the pipeline. least squares twin support vector machine (lstsvm) is a classic fast classification method that has been used to identify different pipeline conditions; however, lstsvm assumes that all samples share the same weight when generating the hyperplane, including data points that may be polluted in the sample (i.e., outliers), and outlier samples with equal weights will mislead the generation of the hyperplane. inspired by the above research, this article proposes a weighted least squares twin bounded support vector machine based on gaussian mixture models (gmms), referred to as g-wlstsvm. the proposed g-wlstsvm introduces a weight matrix for the objective function through gmm and assigns a larger weight to the normal samples and a smaller weight to the outliers, which reduces the impact effect of the outliers on the generation of the classification hyperplane. furthermore, since lstsvm only considers the empirical risk minimization principle, it may lead to overfitting. the proposed g-wlstsvm introduces an extra regularization term based on the margin maximization idea to realize the principle of structural risk minimization, which improves the generalization performance of the model. finally, since the practical problems are mostly multiclassification problems, the g-wlstsvm for binary classification cannot be satisfied. therefore, the proposed g-wlstsvm combined with a one-versus-one strategy is extended to handle multiclassification problems, namely the multiclass g-wlstsvm. we evaluate the effectiveness of the multiclass g-wlstsvm in identifying different pipeline conditions and localizing the identified leakage. numerical experimental results on several university of california irvine (uci) datasets further demonstrate that compared with other related methods, the proposed g-wlstsvm not only retains the advantages of simplicity and speed of the lstsvm but also improves the classification accuracy and generalization ability. the code for this article is available at https://github.com/cmq-456/glstsvm.",AB_0203
"the event camera asynchronously produces the event stream with a high temporal resolution, discarding redundant visual information and bringing new possibilities for moving object detection. nevertheless, the existing object detectors cannot make the most of the spatial-temporal asynchronous nature and high temporal resolution of the event stream. for one thing, existing methods fail to consider objects with different velocities relative to the event camera's motion, resulting from the global synchronized time window with the whole spatial slice. for another, most of the existing methods rely on heavy models and boost the detection performance with low frame rates, failing to utilize the high temporal resolution characteristic of the event stream. in this work, we propose a motion robust and high-speed detection pipeline which better leverages the event data. first, we design an event stream representation called temporal active focus (taf), which efficiently utilizes the spatial-temporal asynchronous event stream, constructing event tensors robust to object motions. then, we propose a module called the bifurcated folding module (bfm), which encodes the rich temporal information in the taf tensor at the input layer of the detector. following this, we design a high-speed lightweight detector called agile event detector (aed) plus a simple but effective data augmentation method, to enhance the detection accuracy and reduce the model's parameter. experiments on two typical real-scene event camera object detection datasets show that our method is competitive in terms of accuracy, efficiency, and the number of parameters. by classifying objects into multiple motion levels based on the optical flow density metric, we further illustrated the robustness of our method for objects with different velocities relative to the camera. the codes and trained models are available at https://github.com/harmonialeo/frlw-evd.",AB_0203
