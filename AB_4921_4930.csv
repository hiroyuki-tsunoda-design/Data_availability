AB,NO
"there is an urgent need to diversify the pipeline for discovering novel natural products due to the increase in multi-drug resistant infections. like bacteria, fungi also produce secondary metabolites that have potent bioactivity and rich chemical diversity. to avoid self-toxicity, fungi encode resistance genes which are often present within the biosynthetic gene clusters (bgcs) of the corresponding bioactive compounds. recent advances in genomemining tools have enabled the detection and prediction of bgcs responsible for the biosynthesis of secondary metabolites. the main challenge now is to prioritize themost promising bgcs that produce bioactive compounds with novel modes of action. with targetdirected genome mining methods, it is possible to predict the mode of action of a compound encoded in an uncharacterized bgc based on the presence of resistant target genes. here, we introduce the `fungal bioactive compound resistant target seeker' (funarts) available at https://funarts.ziemertlab.com. this is a specific and efficient mining tool for the identification of fungal bioactive compounds with interesting and novel targets. funarts rapidly links housekeeping and known resistance genes to bgc proximity and duplication events, allowing for automated, target-directed mining of fungal genomes. additionally, funarts generates gene cluster networking by comparing the similarity of bgcs from multi-genomes. [graphics] .",AB_0493
"background: solid-phase peptide synthesis (spps) is a mature technique widely used in research and in production. there are different approaches that fulfill the diverse requirements, regarding the number, quantity and quality of peptides. we have implemented three laboratory protocols of synthesis that cover these needs. these protocols have been tested, and the results analyzed with two different sequences used in previous works. results: the peptide synthesis protocols such as tea bag, microwave synthesis and manual synthesis have allowed obtaining specific yields of 8, 43 and 64% for the nbc112 peptide and specific yields of 36, 46 and 78% for the nbc759 peptide with the three protocols, respectively. each protocol has different application contexts with advantages and disadvantages in each case. conclusions: the three protocols allow the obtention of the two peptides with good purity and can be used according to specific needs and requirements. this article includes an interactive 360-degree video. to view it correctly, it is necessary to scroll through the screen to navigate across the laboratory where you will find 10 interactive points. for an immersive experience, a head-mounted display can be used. please, visit this url: http://ejbiotechnol-ogy.info/public/360view/2023/vtpcardenas1v1/index.html.",AB_0493
"deep-learning based periocular recognition systems typically use overparameterized deep neural networks asso-ciated with high computational costs and memory requirements. this is especially problematic for mobile and embedded devices in shared resource environments. to perform model quantization for lightweight periocular recognition in a privacy-aware manner, we propose and release syper, a synthetic dataset and generation model of periocular images. to enable this, we propose to perform the knowledge transfer in the quantization process on the embedding level and thus not identity-labeled data. this does not only allow the use of synthetic data for quantization, but it also successfully allows to perform the quantization on different domains to addition-ally boost the performance in new domains. in a variety of experiments on a diverse set of model backbones, we demonstrate the ability to build compact and accurate models through an embedding-level knowledge transfer using synthetic data. we also demonstrate very successfully the use of embedding-level knowledge transfer for near-infrared quantized models towards accurate and efficient periocular recognition on near-infrared images. the syper dataset, together with the evaluation protocol, the training code, and model checkpoints are made publicly available at https://github.com/jankolf/syper.(c) 2023 elsevier b.v. all rights reserved.",AB_0493
"the task of emotion recognition in conversations (erc) benefits from the availability of multiple modal-ities, as provided, for example, in the video-based multimodal emotionlines dataset (meld). however, only a few research approaches use both acoustic and visual information from the meld videos. there are two reasons for this: first, label-to-video alignments in meld are noisy, making those videos an unre-liable source of emotional speech data. second, conversations can involve several people in the same scene, which requires the localisation of the utterance source. in this paper, we introduce meld with fixed audiovisual information via realignment (meld-fair) by using recent active speaker detection and automatic speech recognition models, we are able to realign the videos of meld and capture the facial expressions from speakers in 96.92% of the utterances provided in meld. experiments with a self-supervised voice recognition model indicate that the realigned meld-fair videos more closely match the transcribed utterances given in the meld dataset. finally, we devise a model for emotion recognition in conversations trained on the realigned meld-fair videos, which outperforms state-of -the-art models for erc based on vision alone. this indicates that localising the source of speaking activ-ities is indeed effective for extracting facial expressions from the uttering speakers and that faces provide more informative visual cues than the visual features state-of-the-art models have been using so far. the meld-fair realignment data, and the code of the realignment procedure and of the emotional recogni-tion, are available at https://github.com/knowledgetechnologyuhh/meld-fair.(c) 2023 the author(s). published by elsevier b.v. this is an open access article under the cc by-nc-nd license ().",AB_0493
"the segmentation of histopathological whole slide images into tumourous and non-tumourous types of tissue is a challenging task that requires the consideration of both local and global spatial contexts to classify tumourous regions precisely. the identification of subtypes of tumour tissue complicates the issue as the sharpness of separation decreases and the pathologist's reasoning is even more guided by spatial context. however, the identification of detailed tissue types is crucial for providing personalized cancer therapies. due to the high resolution of whole slide images, existing semantic segmentation methods, restricted to isolated image sections, are incapable of processing context information beyond. to take a step towards better context comprehension, we propose a patch neighbour attention mechanism to query the neighbouring tissue context from a patch embedding memory bank and infuse context embeddings into bottleneck hidden feature maps. our memory attention framework (maf) mimics a pathologist's annotation procedure - zooming out and considering surrounding tissue context. the framework can be integrated into any encoder-decoder segmentation method. we evaluate the maf on two public breast cancer and liver cancer data sets and an internal kidney cancer data set using famous segmentation models (u-net, deeplabv3) and demonstrate the superiority over other context-integrating algorithms - achieving a substantial improvement of up to 17% on dice score. the code is publicly available at https://github.com/tio-ikim/valuing-vicinity.",AB_0493
"the package performs molecular-dynamics-like agent-based simulations for models of aligning self-propelled particles in two dimensions such as e.g. the seminal vicsek model or variants of it. in one class of the covered models, the microscopic dynamics is determined by certain time discrete interaction rules. thus, it is no hamiltonian dynamics and quantities such as energy are not defined. in the other class of considered models (that are generally believed to behave qualitatively the same) brownian dynamics is considered. however, also there, the forces are not derived from a hamiltonian. furthermore, in most cases, the forces depend on the state of all particles and can not be decomposed into a sum of forces that only depend on the states of pairs of particles. due to the above specified features of the microscopic dynamics of such models, they are not implemented in major molecular dynamics simulation frameworks to the best of the authors knowledge. models that are covered by this package have been studied with agent-based simulations by dozens of papers. however, no simulation framework of such models seems to be openly available. the program is provided as a python package. the simulation code is written in c. in the current version, parallelization is not implemented.program summaryprogram title: aligning active particles py packagecpc library link to program files: https://doi .org /10 .17632 /gghrf6sz8t .1developer's repository link: https://github .com /kuersten /aappp licensing provisions: mitprogramming language: c / pythonnature of problem: perform molecular-dynamics-like agent-based simulations of models for aligning active particles with interaction rules that are not following hamiltonian dynamics and that are not restricted to pair-interactions.solution method: uses cell lists to find interacting particles. additional comments including restrictions and unusual features: does not run in parallel. allows the usage of reflecting boundary conditions.(c) 2023 elsevier b.v. all rights reserved.",AB_0493
"internet of things (iot) services and devices have raised numerous challenges such as connectivity, computation, and security. therefore, networks should provide and maintain quality services. nowadays, distributed denialof-service (ddos) attack is the most important network attacks according to recent studies. among the variety of ddos detection methods, machine learning (ml) algorithms have attracted researchers. in ml, the selection of optimal subset of features can have a significant role to enhance the classification rate. this problem called the feature selection problem is in the class of np-hard problems and exact algorithms cannot obtain the best results in acceptable time. therefore, approximate algorithms like meta-heuristic algorithms are employed to solve the problem. since these algorithms do not search all solution space, they fall in local optima and provide a premature convergence rate. several methods have been introduced so far to address these challenges but researchers try to find new strategies for enhancing the performance of methods. in this study, a binary improved african vulture optimization algorithm (sin-cos-biavoa) is proposed to select effective features of ddos attacks. the method applies a novel compound transfer function (sin-cos) to increase exploration. to select the optimal subset of features, gravitational fixed radius nearest neighbor (gfrnn) is employed as the classifier in the method. moreover, avoa is improved in three phases including exploration, balancing exploration and exploitation, and exploitation phases. hence, sin-cos-biavoa explores promising areas to achieve the best solution and avoid the local optima traps. the proposed method's performance is compared with some recent stateof-the-art in two datasets, cic-ddos2019 and nsl-kdd for the ddos attack detection. the experiment results show that the proposed method achieves the minimum feature selection rate (0.0184) with the high average accuracy (99.9979%), precision (99.9979%), recall (100.00%), and f-measure (99.9989%) compared with competitors in the first scenario with 1% attack rate in cic-ddos2019 dataset. in addition, the results of friedman test based on fitness functions indicate that sin-cos-biavoa has the first rank among comparative algorithms. the source code of sin-cos-biavoa is publicly available at https://www.mathworks.com/matlabcentral/fileexchange/129409-sin-cos-biavoa-a-new-feature-selection-method.",AB_0493
"purpose: patient-reported outcomes have not been suffciently implemented into the routine care of cancer patients because the existing instruments are often too long and complex or not cancer-specific. the aim of this study is the determination of psychometric properties and item reduction of a newly developed health-related quality of life (hrqol) questionnaire for use in oncological clinical routines. methods: this observational study with a repeatedmeasurements design included oncological inpatients and outpatients. a total of 630 patients participated at the first point of measurement and 404 at the second point of measurement. to evaluate the instrument, we conducted hierarchical confirmative factor analyses and for further validation correlated the resulting factors with standardized and validated hrqol measurements. test-retest reliability and responsiveness to change were tested. results: the developed questionnaire help-6 (hamburg inventory for measuring quality of life in oncological patients) has a six-factor structure and has moderate-to-good convergent validity (r =-0.25-0.68). test-retest reliability was moderate-to-good (r =0.56-0.81, p<0.001). indications for responsiveness to change were found for three dimensions. the final version of the questionnaire help-6 has six dimensions with one item each. conclusion: with the help-6 instrument formeasuring hrqol in cancer patients, we provide a short and practical patient-reported outcome instrument. though responsiveness to change could not be confirmed for all dimensions in this study, the help-6 includes time-effcient completion and evaluation and is informative in relevant hrqol dimensions of cancer patients. therefore, the help-6 poses an important addition to inpatient and outpatient routine cancer care. trial registration: this study was registered at open science framework (https://osf.io/y7xce/), on 9 june 2018",AB_0493
"motivation: inferring taxonomy in mass spectrometry-based shotgun proteomics is a complex task. in multi-species or viral samples of unknown taxonomic origin, the presence of proteins and corresponding taxa must be inferred from a list of identified peptides, which is often complicated by protein homology: many proteins do not only share peptides within a taxon but also between taxa. however, the correct taxonomic inference is crucial when identifying different viral strains with high-sequence homology-considering, e.g., the different epidemiological characteristics of the various strains of severe acute respiratory syndrome-related coronavirus-2. additionally, many viruses mutate frequently, further complicating the correct identification of viral proteomic samples. results: we present pepgm, a probabilistic graphical model for the taxonomic assignment of virus proteomic samples with strain-level resolution and associated confidence scores. pepgm combines the results of a standard proteomic database search algorithm with belief propagation to calculate the marginal distributions, and thus confidence scores, for potential taxonomic assignments. we demonstrate the performance of pepgm using several publicly available virus proteomic datasets, showing its strain-level resolution performance. in two out of eight cases, the taxonomic assignments were only correct on the species level, which pepgm clearly indicates by lower confidence scores. availability and implementation: pepgm is written in python and embedded into a snakemake workflow. it is available at https://github.com/ bamescience/pepgm.",AB_0493
"training deep networks for semantic segmentation requires large amounts of labeled training data, which presents a major challenge in practice, as labeling segmentation masks is a highly labor-intensive process. to address this issue, we present a framework for semi-supervised and domain-adaptive semantic segmentation, which is enhanced by self-supervised monocular depth estimation (sde) trained only on unlabeled image sequences. in particular, we utilize sde as an auxiliary task comprehensively across the entire learning framework: first, we automatically select the most useful samples to be annotated for semantic segmentation based on the correlation of sample diversity and difficulty between sde and semantic segmentation. second, we implement a strong data augmentation by mixing images and labels using the geometry of the scene. third, we transfer knowledge from features learned during sde to semantic segmentation by means of transfer and multi-task learning. and fourth, we exploit additional labeled synthetic data with cross-domain depthmix and matching geometry sampling to align synthetic and real data. we validate the proposed model on the cityscapes dataset, where all four contributions demonstrate significant performance gains, and achieve state-of-the-art results for semi-supervised semantic segmentation as well as for semi-supervised domain adaptation. in particular, with only 1/30 of the cityscapes labels, our method achieves 92% of the fully-supervised baseline performance and even 97% when exploiting additional data from gta. the source code is available at https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.",AB_0493
