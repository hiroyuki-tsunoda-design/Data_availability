AB,NO
"existing deep neural network (dnn)-based methods have produced good pansharpened images. however, supervised dnn-based pansharpening methods suffer from performance degradation when fusing low spatial resolution multispectral (lr ms) and panchromatic (pan) images at full resolution. unsupervised dnn-based methods alleviate the issue, but their training becomes difficult owing to the absence of reference images. this article establishes a novel semi-supervised framework to jointly learn the reconstructions of fused images from reduced- and full-resolution datasets. specifically, we propose a cross-resolution semi-supervised adversarial learning network (crossnet), which is composed of a supervised module and an unsupervised module. in these two modules, reduced- or full-resolution source images are disentangled as resolution-invariant components and resolution-aware components by reconstructing the fused images. moreover, cross-resolution fused images are synthesized to enhance the disentanglement of the two kinds of components. through the reconstruction of cross-resolution fused images, supervised and unsupervised modules are also coupled efficiently. then, the semi-supervised framework can simultaneously make use of the supervised information in reduced-resolution datasets and mitigate the performance degradation via full-resolution datasets. besides, adversarial learning is employed to improve the consistency between resolution-invariant components of source images at different resolutions. finally, extensive experiments on quickbird, geoeye-1, worldview-2, and worldview-3 datasets demonstrate that the proposed crossnet can produce state-of-the-art fusion results in terms of qualitative and quantitative evaluations. the source code is available at https://github.com/rsmagneto/crossnet.",AB_0289
"detecting infrared dim and small targets is one crucial step for many tasks such as early warning. it remains a continuing challenge since characteristics of infrared small targets, usually represented by only a few pixels, are generally not salient. even though many traditional methods have significantly advanced the community, their robustness or efficiency is still lacking. most recently, convolutional neural network (cnn)-based object detection has achieved remarkable performance and some researchers focus on it. however, these methods are not computationally efficient when implemented on some cpu-only machines and a few datasets are available publicly. to promote the detection of infrared small targets in complex backgrounds, we propose a new lightweight cnn-based architecture. the network contains three modules: the feature extraction module is designed for representing multiscale and multilevel features, the grid resample operation module is proposed to fuse features from all scales, and a decoupled head to distinguish infrared small targets from backgrounds. moreover, we collect a brand-new infrared small target detection dedicated dataset, which consists of 68 311 practical captured images with complex backgrounds for alleviating the data dilemma. to validate the proposed model, 54 758 images are used for training and 13 553 images are used for testing. extensive experimental results demonstrate that the proposed method outperforms all traditional methods by a large margin and runs much faster than other cnn methods with high precision. the proposed model can be implemented on the intel i7-10850h cpu (2.3 ghz) platform and jetson nano for real-time infrared small target detection at 44 and 27 fps, respectively. it can be even deployed on an atom x5-z8500 (1.44 ghz) machine at about 25 fps with 128 x 128 local images. the source codes and the dataset have been made publicly available at https://github.com/seahifly/infrared-small-target.",AB_0289
"oil spill detection has attracted increasing attention in recent years, since marine oil spill accidents severely affect environments, natural resources, and the lives of coastal inhabitants. hyperspectral remote sensing images provide rich spectral information which is beneficial for the monitoring of oil spills in complex ocean scenarios. however, most of the existing approaches are based on supervised and semi-supervised frameworks to detect oil spills from hyperspectral images (hsis), which require a massive amount of effort to annotate a certain number of high-quality training sets. in this study, we make the first attempt to develop an unsupervised oil spill detection method based on isolation forest (iforest) for hsis. first, a gaussian statistical model is designed to remove the bands corrupted by severe noise. then, kernel principal component analysis (kpca) is employed to reduce the high dimensionality of the hsis. next, the probability of each pixel belonging to one of the classes of seawater and oil spills is estimated with the iforest, and a set of pseudolabeled training samples is automatically produced using the clustering algorithm on the detected probability. finally, an initial detection map can be obtained by performing the support vector machine (svm) on the dimension-reduced data, and the initial detection result is further optimized with the extended random walker (erw) model so as to improve the detection accuracy of oil spills. experiments on hyperspectral oil spill database (hosd) created by ourselves demonstrate that the proposed method obtains superior detection performance with respect to other state-of-the-art detection approaches. we will make hosd and our developed library for oil spill detection publicly available at https://github.com/puhongduan/hosd to further promote this research topic.",AB_0289
"self-supervised learning (ssl) has gained wide-spread attention in the remote sensing (rs) and earth observation (eo) communities owing to its ability to learn task-agnostic representations without human-annotated labels. nevertheless, most existing rs ssl methods are limited to learning either global semantic separable or local spatial perceptible representations. we argue that this learning strategy is suboptimal in the realm of rs since the required representations for different rs downstream tasks are often varied and complex. in this study, we proposed a unified ssl framework that is better suited for rs image representation learning. the proposed ssl framework, contrastive mask image distillation (cmid), is capable of learning representations with both global semantic separability and local spatial perceptibility by combining contrastive learning (cl) with masked image modeling (mim) in a self-distillation way. furthermore, our cmid learning framework is architecture-agnostic, which is compatible with both convolutional neural networks (cnns) and vision transformers (vits), allowing cmid to be easily adapted to a variety of deep learning (dl) applications for rs understanding. comprehensive experiments have been carried out on four downstream tasks (i.e., scene classification, semantic segmentation, object detection, and change detection) and the results show that models pretrained using cmid achieve a better performance than other state-of-the-art ssl methods on multiple downstream tasks. the code and pretrained models will be made available at https://github.com/nju-lhrs/official-cmidhttps://github.com/nju-lhrs/official-cmid to facilitate ssl research and speed up the development of rs images dl applications.",AB_0289
"hyperspectral image (hsi) super-resolution generally means the fusion of low-spatial-resolution hsi (lrhsi) and high-spatial-resolution multispectral/panchromatic image (hrmpi) to get high-spatial-resolution hsi (hrhsi). existing fusion methods have not sufficiently considered the huge spectral and spatial resolution difference between the lrhsi and hrmpi. in addition, most deep learning (dl)-based methods that adopt the convolutional neural network (cnn) structure are limited by its local feature learning, and it is difficult to exploit the global dependence of image features. to fully adapt to the huge modality difference between lrhsi and hrmpi and release the limitation of local feature learning, we design the cross spectral-scale and shift-window-based cross spatial-scale nonlocal attention network (cssnet) to effectively fuse the lrhsi and hrmpi. these two networks could explicitly learn the spectral and spatial correlations between two input images. these correlations are then used to reconstruct the hrhsi feature, which makes the obtained hrhsi feature to maintain the spectral and spatial feature consistency with the input images. finally, a feature aggregation module is designed to aggregate the image features from these two networks and output the fused hrhsi. extensive experimental results on both hm-fusion [fusion with multispectral (msi)] and hp-fusion (fusion with panchromatic (pan) image) tasks demonstrate cssnet's state-of-the-art (sota) performance compared to other fusion methods. the codes could be available at https://github.com/rs-lsl/cssnet.",AB_0289
"steganography based on fixed codebook has become one of the most important branches of speech steganography due to its high imperceptibility and having the largest available carrier space. as its countermeasure technique, this paper presents a novel steganalysis method based on separable convolution network (sepstenet) with dual-stream pyramid enhanced strategy (dpes). specifically, to better acquire discriminative representations, we design the pulse-aware separable block to capture the pulse correspondence along independent levels of pulse positions, where the pulse-aware excitation module is plugged to avoid noisy clue accumulation by adaptively emphasizing the salient part. moreover, the global attending block is introduced to enhance correspondence features through calculating global responses at distinct subframes. in addition, to eliminate the negative impact of sample content, dpes is leveraged to incorporate cross-domain coherence features by the inverted connected dual-stream branches. with the original and calibration speech samples, two branches enable the correspondence of two detection feature domains to interact with each other to generate coherence features independent of sample content, thereby improving the detection performance. the performance of the presented method is comprehensively evaluated and compared with the state of the arts. the experimental results demonstrate that the presented method significantly outperforms the existing ones. furthermore, dpes is shown to be a general enhancement strategy that can effectively improve the performance of the existing deep neural network for speech steganalysis. the source code for this work is publicly available on https://github.com/barryxxz/sepstenetwithdpes.",AB_0289
"this paper proposes a decoder-side cross resolution synthesis (crs) module to pursue better compression efficiency beyond the latest versatile video coding (vvc), where we encode intra frames at original high resolution (hr), compress inter frames at a lower resolution (lr), and then super-resolve decoded lr inter frames with the help from preceding hr intra and neighboring lr inter frames. for a lr inter frame, a motion alignment and aggregation network (man) is devised to produce temporally aggregated motion representation to best guarantee the temporal smoothness; another texture compensation network (tcn) is utilized to generate texture representation from decoded hr intra frame for better augmenting spatial details; finally, a similarity-driven fusion engine synthesizes motion and texture representations to upscale lr inter frames for the removal of compression and resolution re-sampling noises. we enhance the vvc using proposed crs, showing averaged 8.76% and 11.93% bjontegaard delta rate (bd-rate) gains against the latest vvc anchor in random access (ra) and low-delay p (ldp) settings respectively. in addition, experimental comparisons to the state-of-the-art super-resolution (sr) based vvc enhancement methods, and ablation studies are conducted to further report superior efficiency and generalization of the proposed algorithm. all materials will be made to public at https://njuvision.github.io/crs for reproducible research.",AB_0289
"automatic road detection from remote sensing images has always been a significant research topic. it is of great value to many practical applications. however, there are still some problems that need to be solved. first, most of the existing road detection methods are inefficient because of sequential processing of the decoder head. second, some existing methods are unable to detect occluded road areas effectively. for this reason, we focus on the speed and occlusion problems in road detection network and propose a new lightweight road detection method based on multiscale convolutional attention network (mscan) and coupled decoder head, lrdnet. in particular, lrdnet adopts mscan with large receptive field for feature extraction to solve the occlusion problem and decode the road surface, road edge, and road centerline in a coupled way to improve the speed of road detection and ensure that the road surface detection results have fewer burrs at the road edge. we have performed several experiments on the roadnet benchmark dataset (rnbd). compared with some state-of-the-art methods, the experimental results prove the validity of the proposed lrdnet. the code will be released soon on the site of https://github.com/dyl96/lrdnet.",AB_0289
"deep neural networks (dnns) are widely used in synthetic aperture radar (sar) image classification and recognition, achieving state-of-the-art performance. but it remains a challenging task to recognize occluded targets. in this letter, we propose a novel robust sar recognition method against occlusion. specifically, we design a scattering excitation learning (sel) module that encourages the network to learn more robust features responding to the scattering centers of targets. in addition, we adopt a random feature channel dropout technique which can further improve robustness to occlusion. our method makes the network more robust against occlusion but without any occlusion-simulated data for training. experimental results on the moving and stationary target acquisition and recognition (mstar) dataset show that our proposed method achieves remarkably improved robustness even under severe occlusions. the code is made available at https://github.com/koervcor/ sel-cd.",AB_0289
"synthetic aperture radar (sar) image change detection is a hot but challenging task due to sar images' complex contents and inherent speckle noises. the expected change detection methods should reduce the influence of speckle noises, obtain the discriminative feature representations, and generate accurate change maps simultaneously. to these ends, we propose a new sar image change detection method named feature fusion of information transfer network (ffitn). first, we develop a hybrid convolution block (hcb) to depress the speckle noise impacts and explore the valuable information from sar images. thus, the feature extraction module (fem) is constructed to obtain the multilevel features. then, an information transfer module (itm) is proposed to capture the salient regions from various aspects. also, the salient knowledge is transferred among features at different levels to enhance their discrimination. next, a self-attention-based feature fusion module (saffm) is introduced to fuse various features. finally, a change map generation module (cmgm) with the clustering algorithm and specific loss functions is designed to produce the pseudo labels and change maps. experimental results on three public sar datasets demonstrate the model's effectiveness. our source codes are available at https://github.com/tangxu-group/ffitn.",AB_0289
