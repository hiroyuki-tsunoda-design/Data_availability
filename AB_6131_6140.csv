AB,NO
"multiomics profiling provides a holistic picture of a condition being examined and captures the complexity of signaling events, beginning from the original cause (environmental or genetic), to downstream functional changes at multiple molecular layers. pathway enrichment analysis has been used with multiomics data sets to characterize signaling mechanisms. however, technical and biological variability between these layered data limit an integrative computational analyses. we present a boolean network-based method, multiomics boolean omics network invariant-time analysis (mbonita), to integrate omics data sets that quantify multiple molecular layers. mbonita utilizes prior knowledge networks to perform topology-based pathway analysis. in addition, mbonita identifies genes that are consistently modulated across molecular measurements by combining observed fold-changes and variance, with a measure of node (i.e., gene or protein) influence over signaling, and a measure of the strength of evidence for that gene across data sets. we used mbonita to integrate multiomics data sets from ramos b cells treated with the immunosuppressant drug cyclosporine a under varying o2 tensions to identify pathways involved in hypoxia-mediated chemotaxis. we compare mbonita's performance with 6 other pathway analysis methods designed for multiomics data and show that mbonita identifies a set of pathways with evidence of modulation across all omics layers. mbonita is freely available at https://github.com/thakar-lab/mbonita.",AB_0614
"would you rather search for a line inside a cube or a point inside a square? physics-based simulations and wet-lab experiments often have symmetries (degeneracies) that allow reducing problem dimensionality or search space, but constraining these degeneracies is often unsupported or difficult to implement in many optimization packages, requiring additional time and expertise. so, are the possible improvements in efficiency worth the cost of implementation? we demonstrate that the compactness of a search space (to what extent and how degenerate solutions and non-solutions are removed) affects bayesian optimization search efficiency. here, we use the adaptive experimentation (ax) platform by metatm and a physics-based particle packing simulation with eight or nine tunable parameters, depending on the search space compactness. these parameters represent three truncated log-normal distributions of particle sizes which exhibit compositional-invariance and permutation-invariance characteristic of formulation problems (e.g., chemical formulas, composite materials, alloys). we assess a total of four search space types which range from none up to both constraint types imposed simultaneously. in general, the removal of degeneracy through problem reformulation (as seen by the optimizer's surrogate model) improves optimization efficiency. the code is hosted at https://github. com/sparks-baird/bayes-opt-particle-packing. we recommend that optimization practitioners in the physical sciences carefully consider the trade-off between implementation cost and search efficiency before running expensive optimization campaigns.",AB_0614
"longitudinal bulk and single-cell omics data is increasingly generated for biological and clinical research but is challenging to analyze due to its many intrinsic types of variations. we present palmo (https://github.com/ aifimmunology/palmo), a platform that contains five analytical modules to examine longitudinal bulk and single-cell multi-omics data from multiple perspectives, including decomposition of sources of variations within the data, collection of stable or variable features across timepoints and participants, identification of up- or down-regulatedmarkers across timepoints of individual participants, and investigation on samples of same participants for possible outlier events. we have tested palmo performance on a complex longitudinal multi-omics dataset of five data modalities on the same samples and six external datasets of diverse background. both palmo and our longitudinal multi-omics dataset can be valuable resources to the scientific community.",AB_0614
"background: gene co-expression networks represent modules of genes with shared biological function, and have been widely used to model biological pathways in gene expression data. co-expression networks associated with a specific trait can be constructed and identified using weighted gene co-expression network analysis (wgcna), which is especially useful for the study of transcriptional signatures in disease. wgcna networks are typically constructed using both disease and wildtype samples, so molecular pathways associated with disease are identified. however, it would be advantageous to study such co-expression networks in their disease context across spatiotemporal conditions, but currently there is no comprehensive software implementation for this type of analysis. results: here, we introduce a wgcna-based procedure, multiwgcna, that is tailored to datasets with variable spatial or temporal traits. as well as constructing the combined network, multiwgcna also generates a network for each condition separately, and subsequently maps these modules between and across designs, and performs relevant downstream analyses, including module-trait correlation and module preservation. when applied to astrocyte-specific rna-sequencing (rna-seq) data from various brain regions of mice with experimental autoimmune encephalitis, multiwgcna resolved the de novo formation of the neurotoxic astrocyte transcriptional program exclusively in the disease setting. using time-course rna-seq from mice with tau pathology (rtg4510), we demonstrate how multiwgcna can also be used to study the temporal evolution of pathological modules over the course of disease progression. conclusion: the multiwgcna r package can be applied to expression data with two dimensions, which is especially useful for the study of disease-associated modules across time or space. the source code and functions are freely available at: https://github.com/fogellab/multiwgcna.",AB_0614
"modelling the growth histories of specific galaxies often involves generating the entire population of objects that arise in a given cosmology and selecting systems with appropriate properties. this approach is highly inefficient when targeting rare systems such as the extremely luminous high-redshift galaxy candidates detected by jwst. here, we present a novel framework for generating merger trees with branches that are guaranteed to achieve a desired halo mass at a chosen redshift. this method augments extended press schechter theory solutions with constrained random processes known as brownian bridges and is implemented in the open-source semi-analytic model galacticus. we generate ensembles of constrained merger trees to predict the growth histories of seven high-redshift jwst galaxy candidates, finding that these systems most likely merge approximate to 2 gyr after the observation epoch and occupy haloes of mass greater than or similar to 10(14) m-circle dot today. these calculations are thousands of times more efficient than existing methods, are analytically controlled, and provide physical insights into the evolution of haloes with rapid early growth. our constrained merger tree implementation is publicly available at https://github.com/galacticusorg/galacticus. .",AB_0614
"background: biochemical reaction prediction tools leverage enzymatic promiscuity rules to generate reaction networks containing novel compounds and reactions. the resulting reaction networks can be used for multiple applications such as designing novel biosynthetic pathways and annotating untargeted metabolomics data. it is vital for these tools to provide a robust, user-friendly method to generate networks for a given application. however, existing tools lack the flexibility to easily generate net-works that are tailor-fit for a user's application due to lack of exhaustive reaction rules, restriction to pre-computed networks, and difficulty in using the software due to lack of documentation.results: here we present pickaxe, an open-source, flexible software that provides a user-friendly method to generate novel reaction networks. this software iteratively applies reaction rules to a set of metabolites to generate novel reactions. users can select rules from the prepackaged jn1224min ruleset, derived from metacyc, or define their own custom rules. additionally, filters are provided which allow for the pruning of a network on-the-fly based on compound and reaction properties. the filters include chemical similarity to target molecules, metabolomics, thermodynamics, and reaction feasibility filters. example applications are given to highlight the capabilities of pickaxe: the expansion of common biological databases with novel reactions, the generation of industrially useful chemicals from a yeast metabolome database, and the annotation of untargeted metabolomics peaks from an e. coli dataset.conclusion: pickaxe predicts novel metabolic reactions and compounds, which can be used for a variety of applications. this software is open-source and available as part of the mine database python package (https://pypi.org/project/minedatabase/) or on github (https://github.com/tyo-nu/mine-database). documentation and examples can be found on read the docs (https://mine-database.readthedocs.io/en/latest/). through its documentation, pre-pack aged features, and customizable nature, pickaxe allows users to generate novel reaction networks tailored to their application.",AB_0614
"background: stable isotope resolved metabolomics (sirm) is a new biological approach that uses stable isotope tracers such as uniformly 13c-enriched glucose ( 13c6-glc) to trace metabolic pathways or networks at the atomic level in complex biological systems. non-steady-state kinetic modeling based on sirm data uses sets of simultane-ous ordinary differential equations (odes) to quantitatively characterize the dynamic behavior of metabolic networks. it has been increasingly used to understand the regulation of normal metabolism and dysregulation in the development of diseases. however, fitting a kinetic model is challenging because there are usually multiple sets of parameter values that fit the data equally well, especially for large-scale kinetic models. in addition, there is a lack of statistically rigorous methods to compare kinetic model parameters between different experimental groups.results: we propose a new bayesian statistical framework to enhance parameter estimation and hypothesis testing for non-steady-state kinetic modeling of sirm data. for estimating kinetic model parameters, we leverage the prior distribution not only to allow incorporation of experts' knowledge but also to provide robust parameter esti-mation. we also introduce a shrinkage approach for borrowing information across the ensemble of metabolites to stably estimate the variance of an individual isotopomer. in addition, we use a component-wise adaptive metropolis algorithm with delayed rejection to perform efficient monte carlo sampling of the posterior distribution over high-dimensional parameter space. for comparing kinetic model parameters between experimental groups, we propose a new reparameterization method that converts the complex hypothesis testing problem into a more tractable parameter estimation problem. we also propose an inference procedure based on credible interval and cred-ible value. our method is freely available for academic use at https://github.com/xuzha ng0131/mcmcflux.conclusions: our new bayesian framework provides robust estimation of kinetic model parameters and enables rigorous comparison of model parameters between experimental groups. simulation studies and application to a lung cancer study dem-onstrate that our framework performs well for non-steady-state kinetic modeling of sirm data.",AB_0614
"the osu/pnnl superfund research program (srp) represents a longstanding collaboration to quantify polycyclic aromatic hydrocarbons (pahs) at various superfund sites in the pacific northwest and assess their potential impact on human health. to link the chemical measurements to biological activity, we describe the use of the zebrafish as a high-throughput developmental toxicity model that provides quantitative measurements of the exposure to chemicals. toward this end, we have linked over 150 pahs found at superfund sites to the effect of these same chemicals in zebrafish, creating a rich dataset that links environmental exposure to biological response. to quantify this response, we have implemented a dose-response modelling pipeline to calculate benchmark dose parameters which enable potency comparison across over 500 chemicals and 12 of the phenotypes measured in zebrafish. we provide a rich dataset for download and analysis as well as a web portal that provides public access to this dataset via an interactive web site designed to support exploration and re-use of these data by the scientific community at http://srp.pnnl.gov.",AB_0614
"recent advances in protein structure prediction have generated accurate structures of previously uncharacterized human proteins. identifying domains in these predicted structures and classifying them into an evolutionary hierarchy can reveal biological insights. here, we describe the detection and classification of domains from the human proteome. our classification indicates that only 62% of residues are located in globular domains. we further classify these globular domains and observe that the majority (65%) can be classified among known folds by sequence, with a smaller fraction (33%) requiring structural data to refine the domain boundaries and/or to support their homology. a relatively small number (966 domains) cannot be confi-dently assigned using our automatic pipelines, thus demanding manual inspection. we classify 47,576 domains, of which only 23% have been included in experimental structures. a portion (6.3%) of these classified globular domains lack sequence-based annotation in interpro. a quarter (23%) have not been structurally modeled by homology, and they contain 2,540 known disease-causing single amino acid vari-ations whose pathogenesis can now be inferred using af models. a comparison of classified domains from a series of model organisms revealed expansions of several immune response-related domains in humans and a depletion of olfactory receptors. finally, we use this classification to expand well-known protein families of biological significance. these classifications are presented on the ecod website (http://prodata. swmed.edu/ecod/index_human.php).",AB_0614
"background: epigenetic modification of chromatin plays a pivotal role in regulating gene expression during cell differentiation. the scale and complexity of epigenetic data pose significant challenges for biologists to identify the regulatory events controlling cell differentiation.results: to reduce the complexity, we developed a package, called snapshot, for clustering and visualizing candidate cis-regulatory elements (ccres) based on their epigenetic signals during cell differentiation. this package first introduces a binarized indexing strategy for clustering the ccres. it then provides a series of easily interpretable figures for visualizing the signal and epigenetic state patterns of the ccres clusters during the cell differentiation. it can also use different hierarchies of cell types to highlight the epigenetic history specific to any particular cell lineage. we demonstrate the utility of snapshot using data from a consortium project for validated systematic integration (vision) of epigenomic data in hematopoiesis.conclusion: the package snapshot can identify all distinct clusters of genomic locations with unique epigenetic signal patterns during cell differentiation. it outperforms other methods in terms of interpreting and reproducing the identified ccres clusters. the package of snapshot is available at github: https://github.com/guanjue/snapshot.",AB_0614
