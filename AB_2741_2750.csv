AB,NO
"single-modal image registration methods are generally not feasible for visible and infrared images. besides, multimodal image registration methods still suffer from uneven distribution of extracted features, low repeatability, and ambiguous features. to address these issues, a coarse-to-fine infrared and visible image registration approach for a dual-sensor unmanned aerial vehicle (uav) imaging system is proposed, which is resilient to the difference in focal lengths and field of view. first, in the coarse registration step, the infrared image is transformed to the same scale as the visible image by using the similarity transformation. this operation makes the proposed method robust to the variation of the field of view. then, the feature point pairs are initialized using feature detectors in the infrared image's blocked phase congruency feature map. next, the feature point pairs are optimized by estimating the offset based on the relationship between the constructed feature descriptors. finally, using elastic deformation, the pixel-level registered infrared image is obtained. extensive experiments demonstrate the superior performance of the proposed coarse-to-fine image registration methodology in real infrared-visible image pairs. the code and dataset are available at https://drive.google.com/drive/folders/1mpuwwhubktrbdornmnrrnujclduac7nu?usp=sharing.",AB_0275
"deep learning-based hyperspectral image super-resolution (sr) methods have achieved remarkable success, which can improve the spatial resolution of hyperspectral images with abundant spectral information. however, most of them utilize 2-d or 3-d convolutions to extract local features while ignoring the rich global spatial-spectral information. in this article, we propose a novel method called the multiscale deformable transformer (msdformer) for single hyperspectral image sr (shsr). the proposed method incorporates the strengths of the convolutional neural network (cnn) for local spatial-spectral information and the transformer structure for global spatial-spectral information. specifically, a multiscale spectral attention module (msam) based on dilated convolution is designed to extract local multiscale spatial-spectral information, which leverages shared module parameters to exploit the intrinsic spatial redundancy and spectral attention mechanism to accentuate the subtle differences between different spectral groups. then a deformable convolution-based transformer module (dctm) is proposed to further extract the global spatial-spectral information from the local multiscale features of the previous stage, which can explore the diverse long-range dependencies among all spectral bands. extensive experiments on three hyperspectral datasets demonstrate that the proposed method achieves excellent sr performance and outperforms the state-of-the-art methods in terms of quantitative quality and visual results. the code is available at https://github.com/tomchenshi/msdformer.git.",AB_0275
"remote sensing image change detection refers to finding the changed regions from a pair of registered images. it has important applications in many fields. however, most methods based on convolutional neural networks and transformers have high complexity and cannot be effectively deployed on satellites or drones in practical applications. to address this issue, we propose an extremely lightweight change detection algorithm called elw_cdnet. its inference speed is very fast. this method is based on the extremely lightweight shufflenetv2. moreover, considering that both global as well as local features play an important role in change detection, we design a light global-local feature enhancement module (lglfem) for reinforcing the features extracted by the backbone. specifically, the global feature extraction module in lglfem is implemented using separable self-attention (ssa), which has linear complexity and very low computational effort. we conduct experiments on two change detection datasets. compared with some state-of-the-art methods, the proposed method can achieve superior performance with extremely fast inference speed. on the levir-cd dataset, it achieves an f1 score of 90.47%, an intersection over union (iou) of 82.60%, and a frame per second (fps) of 914 with 1.75 m parameters and 1.91 gflops. the code will be released soon on the site of https://github.com/dyl96/elw_cdnet.",AB_0275
"the traditional strategy of acquiring satellite images involves transmitting compressed satellite data to ground stations solely via the downlink, without utilizing the uplink. in this article, we propose an enhanced remote-sensing (rs) image compression approach that utilizes uplink assistance to improve compression efficiency. by leveraging the uplink, historical images from ground stations can serve as reference images for on-orbit compression, effectively eliminating spatiotemporal redundancy in rs images. however, due to radiation variations among rs images captured on different dates, pixel-wise referencing as employed in the prior codec paradigm is insufficient. to address this, we propose a novel dual-end referencing downsampling-based coding (refdbc) framework. at the encoder, relevance embedding (re) evaluates reconstructability and records information to restore texture details from the reference before downsampling. at the decoder, relevance-based super-resolution (sr) uses the identical reference and recorded relevance information to reconstruct the decoded low-resolution (lr) image. by incorporating relevance referencing, refdbc effectively mitigates fake texture generation caused by downsampling and compression, achieving significant bitrate savings ranging from 35% to 70% compared to standard, learning-based, and dbc compression baselines in experiments on spot-5 and luojia3 images. code, data, and pretrained models are available online at https://github.com/whw1233/refdbc",AB_0275
"the formation of underwater images is a complex physical process that often suffers from various degradation factors, such as blurriness, low contrast, and color casts, which pose challenges for underwater object detection and recognition tasks. because of the absence of reference images, learning-based methods that rely on unpaired images have been employed to enhance the underwater images. however, these methods may lose their effectiveness in real-world complex underwater environments. in this article, we propose a model-driven cycle-consistent generative adversarial network (cyclegan) model, which is inspired by the underwater image formation model to estimate the background light, transmission map, scene depth, and attenuation coefficient directly. comprehensive experiments have demonstrated that our approach surpasses the compared underwater image restoration methods in both qualitative and quantitative aspects, providing restored images with satisfactory color saturation and brightness. we also conduct experiments on underwater object detection to illustrate the effectiveness of our cyclegan in improving the detection accuracy. all our source codes and data are available at https://github.com/duanlab123/uw-cyclegan.",AB_0275
"in aerial image scenes, the objects have properties of arbitrary orientation, large-scale range, and dense distribution. thus, the object detector uses an oriented bounding box (obb) to locate objects, which is more complex and challenging than a horizontal bounding box (hbb) detector. mainstream obb detectors mostly use a one-to-many label assignment strategy to predict multiple bounding boxes for the same object and filter out repeat predictions by nonmaximum suppression (nms). nms ranks with confidence and drops the detection box with intersection over union (iou) higher than the threshold, which makes it easy to get the local optimum result. the clustered synthesis method gets more accurate results than the original nms, but applying it to the obb detector leads to border shift, which arises from the angular discontinuity problem. therefore, we use gaussian obb (g-obb) to deal with the angular discontinuity and thus eliminate the offset generated by direct synthesis. g-obb is not easy to understand and describe representation. for this reason, we analyze the properties of g-obb and design a decoding method to convert a g-obb to a rotated rectangular box, further discussing its conditions. based on the decoding method, we propose a gaussian synthesis (gaus) algorithm, which transforms the obb into gaussian space, followed by synthesis, and finally transforms the synthesis result back into a new obb. we have derived the synthesis and decoding methods and further verified their effectiveness. the extensive experiments on several existing models show that gaus takes very little computation and improves the detector's high-precision performance. extensive experiments verify the effectiveness, stability, and universality of the proposed algorithm. in addition, the rtmdet using gaus achieves a performance of 81.61 ap50 and gains a 0.39% improvement in mean average precision (map), which achieves the state-of-the-art (sota) performance. our implementation is available at https://github.com/lzh420202/gaus.",AB_0275
"deep learning has emerged as a powerful method for hyperspectral image (hsi) classification. however, a significant prerequisite for hsi classification using deep learning is enough labeled samples, which is both time-consuming and labor-intensive. yet, labeled samples are essential for training deep learning models. this article proposes an hsi classification method based on the self-supervised learning of spectral masking (sslsm). the method mainly includes two steps: self-supervised pretraining and fine-tuning. first, considering the rich spectral information of hsi, we propose masked spectral reconstruction as the pretext task. the unmasked data are input into the encoder and decoder sequentially, which are composed of a multilayer transformer, for feature learning of masked spectral reconstruction. second, we use reference samples to fine-tune the network, and the encoder and decoder are innovatively cascaded for deep semantic feature extraction, which can further improve the ability of feature extraction in the downstream classification tasks. the experimental results show that, compared with other methods, the sslsm obtains the highest classification accuracy of 96.52%, 97.03%, and 96.70% on the indian pines dataset, pavia university dataset, and yancheng wetlands dataset, respectively. our method can also be applied to other hsi datasets, and the codes will be available from https://github.com/cirsm-group/2023-tgrs-sslsm.",AB_0275
"with the continuous improvement of satellite sensor performance, it is becoming easier to obtain different types of remote sensing (rs) data from multiple sensors, and the fusion of hyperspectral (hs) images and light detection and ranging (lidar) for land use/land cover (lulc) classification has become a research hotspot. however, the current mainstream methods still have defects in feature extraction and feature fusion. in the feature extraction stage, previous methods usually use a single-scale patch as input and a fixed convolution kernel for feature extraction, which makes it difficult to extract features in line with different land cover types at the same time and to obtain high-quality features. although multiscale feature extraction can solve the one-sidedness problem of single-scale features, it also brings the challenge of high-dimensional multiscale features. in the feature fusion stage, the current fusion methods are relatively simple. therefore, we propose a dynamic scale hierarchical fusion network (dshfnet) for fusion classification of hs images and lidar data. by calculating the similarity in the scale space and judging the information at different scales through the threshold value, the appropriate scale features are dynamically selected, the small-scale features are integrated into the large-scale features, and the dimensionality of the features is reduced. this method solves the unreliability problem of single-scale features and the high-dimensional problem of multiscale features. in the feature fusion process, different attention modules are used for hierarchical fusion, spatial attention modules are used for shallow fusion and joint feature extraction, and modal attention modules are used for deep fusion of joint features and features from different sensors to achieve complete complementarity of features. experimental evaluations on three real rs datasets demonstrate the superiority of the proposed method compared with existing methods. the source code can be downloaded at https://github.com/syfyn0317/dshfnet.",AB_0275
"label assignment is a crucial process in object detection, which significantly influences the detection performance by determining positive or negative samples during training process. however, existing label assignment strategies barely consider the characteristics of targets in remote sensing images (rsis) thoroughly, e.g., large variations in scales and aspect ratios, leading to insufficient and imbalanced sampling and introducing more low-quality samples, thereby limiting detection performance. to solve the above problems, an elliptical distribution aided adaptive rotation label assignment (earl) is proposed to select high-quality positive samples adaptively in anchor-free detectors. specifically, an adaptive scale sampling (ads) strategy is presented to select samples adaptively among multilevel feature maps according to the scales of targets, which achieves sufficient sampling with more balanced scale-level sample distribution. in addition, a dynamic elliptical distribution (ded) aided sampling strategy is proposed to make the sample distribution more flexible to fit the shapes and orientations of targets and filter out low-quality samples. furthermore, a spatial distance weighting (sdw) module is introduced to integrate the adaptive distance weighting into loss function, which makes the detector more focused on the high-quality samples. extensive experiments on several popular datasets demonstrate the effectiveness and superiority of our proposed earl, where without bells and whistles it can be easily applied to different detectors and achieve state-of-the-art performance. the source code will be available at: https://github.com/justlovesmile/earl.",AB_0275
"the sparse regression method is known for its ability to unmix hyperspectral data, but it can be computationally expensive and accurately insufficient due to the large scale and high coherence of the spectral library. to address this issue, a new approach called layered sparse unmixing termed lsu has been proposed in this article. this method involves breaking down the sparse unmixing process into multilayers, each of which interactively learns a row-sparsity-promoting abundance matrix and fine-tunes active library atoms based on measured activeness. by doing so, lsu outputs both a learned abundance matrix and an optimal library that can best model each mixed pixel in the scene. the proposed lsu can be efficiently solved by the alternating direction method of the multipliers framework. experimental results obtained from simulated and real hyperspectral images demonstrate the effectiveness of lsu. the demo of the proposed lsu will be publicly available at https://github.com/xiangfeishen/layered_sparse_regression_unmixing.",AB_0275
