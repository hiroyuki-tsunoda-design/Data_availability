AB,NO
"pixel-wise image segmentation is key for many computer vision applications. the training of deep neural networks for this task has expensive pixel-level annotation requirements, thus, motivating a growing interest on synthetic data to provide unlimited data and its annotations. in this paper, we focus on the generation and application of synthetic data as representative training corpuses for semantic segmentation of urban scenes. first, we propose a synthetic data generation protocol, which identifies key features affecting performance and provides datasets with variable complexity. second, we adapt two popular weakly supervised domain adaptation approaches (combined training, fine-tuning) to employ synthetic and real data. moreover, we analyze several backbone models, real/synthetic datasets and their proportions when combined. third, we propose a new curriculum learning strategy to employ several synthetic and real datasets. our major findings suggest the high performance impact of pace and order of synthetic and real data presentation, achieving state of the art results for well-known models. the results by training with the proposed dataset outperform popular alternatives, thus demonstrating the effectiveness of the proposed protocol. our code and dataset are available at http://www-vpu.eps.uam.es/publications/wsda semantic/",AB_0498
"the fortran program sbethe calculates the stopping power of materials for swift charged particles with small charges (electrons, muons, protons, their antiparticles, and alphas). the electronic stopping power is computed from the corrected bethe formula, with the shell correction derived from numerical calculations with the plane-wave born approximation (pwba) for atoms, which were based on an independent-electron model with the dirac-hartree-fock-slater self-consistent potential for the ground -state configuration of the target atom. the density effect correction is evaluated from an empirical optical oscillator strength (oos) model based on atomic subshell contributions obtained from pwba calculations. for projectiles heavier than the electron, the barkas correction is evaluated from the oos model, and the lindhard-sorensen correction is estimated from an accurate parameterization of its numerical values. the calculated electronic stopping power is completely determined by a single empirical parameter, the mean excitation energy or i value of the material. the radiative stopping power for electrons, and positrons, is evaluated by means of seltzer and berger's cross section tables for bremsstrahlung emission. the program yields reliable stopping powers and particle ranges for arbitrary materials and projectiles with kinetic energy larger than a certain cutoff value ecut, which is specific of each projectile kind. the program is accompanied by an extensive database that contains tables of relevant energy-dependent atomic quantities for all the elements from hydrogen to einsteinium. sbethe may be used to generate basic information for dosimetry calculations and monte carlo simulations of radiation transport, and as a pedagogical tool.program summaryprogram title: sbethecpc library link to program files: https://doi .org /10 .17632 /7zw25f428t .1licensing provisions: cc by nc 3.0 programming language: fortran 90/95 nature of problem: the program calculates the stopping power of arbitrary materials for swift charged projectiles with small charges. the material is characterized by its chemical composition, mass density, and the empirical i value. the considered projectiles are electrons, positrons, negative muons, antimuons, protons, antiprotons, and alphas, which are described as point particles characterized by their mass and charge. if the actual i value of the material is known, the results from the program are expected to be reliable for projectiles with kinetic energy higher than a value ecut, of the order of 1 kev for electrons and positrons, 150 kev for muons and antimuons, 0.75 mev for protons and antiprotons, and 5 mev for alpha particles.solution method: the electronic stopping power is calculated by means of a corrected bethe formula [1], which combines the conventional bethe logarithm with the following corrections,1) the shell correction obtained from calculations based on the plane-wave born approximation with the self-consistent dirac-hartree-fock-slater (dhfs) potential of neutral atoms in their ground-state configuration [2],2) the density effect correction, which accounts for the reduction of the stopping power caused by the dielectric polarization of the medium,3) a parameterization of the lindhard-sorensen correction, which generalizes the bloch correction for relativistic projectiles, and4) the barkas correction, which accounts for differences between the stopping powers of particles and their antiparticles. the density-effect and the barkas corrections are calculated from a model of the optical oscillator strength (oos) of the material, which combines the contributions of inner atomic subshells calculated with the dhfs potential, with a classical oscillator model for the contribution of valence electrons, a simple extrapolation formula is used to extend the calculated electronic stopping power to energies less than ecut to allow the calculation of particle ranges. for electrons and positrons, the radiative stopping power is calculated from numerical tables prepared by seltzer and berger [3].additional comments including restrictions and unusual features: the calculated stopping power is determined by a single parameter, the mean excitation energy or i value. the program assigns to each material a default i value, derived from the recommendations in the icru report 37, which can be changed by the user. the distribution package includes text files with tables of atomic energy-dependent quantities (subshell optical oscillator strengths, shell corrections, scaled cross sections for bremsstrahlung emission) that are used in the calculations.references [1] f. salvat, phys. rev. a 106 (2022) 032809. [2] f. salvat, l. barjuan, p. andreo, phys. rev. a 105 (2022) 042813. [3] s.m. seltzer, m.j. berger, nucl. instrum. methods b 12 (1985) 95-134.(c) 2023 the author(s). published by elsevier b.v. this is an open access article under the cc by license ( .org /licenses /by/4.0/).",AB_0498
"the sylt roads pelagic time series covers physical and hydrochemical parameters at five neigh-boring stations in the sylt-r16m16 bight, wadden sea, north sea. since the beginning of the time series in 1973, sea surface temperature (sst), salinity, ammonium, nitrite, nitrate, and soluble reactive phosphorus (srp) have been measured twice a week. the other parameters were introduced later (dissolved silicate (si) since 1974, ph since 1979, dissolved organic nitrogen (don) since 1996, dissolved organic phosphorus (dop) since 2001, chlorophyll a since 1979, and suspended particulate matter (spm) since 1975), and in the case of dissolved oxygen, were already discontinued (1979-1983). in the years 1977, 1978, and 1983, no sam-pling took place. since the start of the continuous sampling in 1984, the sea surface temperature in the bight has risen by + 1.11 c-?, with the highest increases during the autumn months, while the ph and salinity de-creased by 0.23 and 0.33 units, respectively. summer and autumn salinities are generally significantly ele-vated compared to spring and winter conditions. dissolved nutrients (ammonium, nitrite, nitrate, and srp) have displayed periods of intense eutrophication (1973-1998) and de-eutrophication since 1999. silicate has shown significantly higher winter levels since 1999. interestingly, phytoplankton parameters did not mirror these large changes in nutrient concentrations, as a seasonal comparison of the two eutrophication periods showed no significant differences with regard to chlorophyll a. this phenomenon might be triggered by an important switch in nutrient limitation during the time series. with regard to nutrients, the phytoplankton was probably primarily limited by silicate until 1998, while, since 1999, the srp limitation has become increasingly important. all data are available in rick et al. (2017b-e, 2020a-o) from https://doi.org/10.1594/pangaea.150032,https://doi.org/10.1594/pangaea.873549, https://doi.org/10.1594/pangaea.873545, https://doi.org/10.1594/pangaea.873547, https://doi.org/10.1594/pangaea.918018, https://doi.org/10.1594/pangaea.918032, https://doi.org/10.1594/pangaea.918027,https://doi.org/10.1594/pangaea.918023, https://doi.org/10.1594/pangaea.918033, https://doi.org/10.1594/pangaea.918028, https://doi.org/10.1594/pangaea.918024, https://doi.org/10.1594/pangaea.918034, https://doi.org/10.1594/pangaea.918029, https://doi.org/10.1594/pangaea.918025, https://doi.org/10.1594/pangaea.918035, https://doi.org/10.1594/pangaea.918030, https://doi.org/10.1594/pangaea.918026, https://doi.org/10.1594/pangaea.918036, and https://doi.org/10.1594/pangaea.918031.",AB_0498
"background: massive amounts of data are produced by combining next-generation sequencing with complex biochemistry techniques to characterize regulatory genomics profiles, such as protein-dna interaction and chromatin accessibility. interpretation of such high-throughput data typically requires different computation methods. however, existing tools are usually developed for a specific task, which makes it challenging to analyze the data in an integrative manner.results: we here describe the regulatory genomics toolbox (rgt), a computational library for the integrative analysis of regulatory genomics data. rgt provides different functionalities to handle genomic signals and regions. based on that, we developed several tools to perform distinct downstream analyses, including the prediction of transcription factor binding sites using atac-seq data, identification of differential peaks from chip-seq data, and detection of triple helix mediated rna and dna interactions, visualization, and finding an association between distinct regulatory factors.conclusion: we present here rgt; a framework to facilitate the customization of computational methods to analyze genomic data for specific regulatory genomics problems. rgt is a comprehensive and flexible python package for analyzing high throughput regulatory genomics data and is available at: https://github.com/costa lab/reg-gen. the documentation is available at: https://reg-gen.readthedocs.io",AB_0498
"the rapid updates in error-resilient applications along with their quest for high throughput has motivated designing fast approximate functional units for field-programmable gate arrays (fpgas). studies have proposed various imprecise functional techniques, albeit posed with three shortcomings: first, most existing inexact multipliers and dividers are specialized for application-specific integrated circuit (asic) platforms. therefore, due to the architectural differences of underlying building blocks in fpga and asic, asic-customized designs have not yielded comparable improvements when directly synthesized and ported to fpgas. second, state-of-the-art (soa) approximate units are substituted, mostly in a single kernel of a multikernel application. moreover, the end-to-end assessment is adopted on the quality of results (qor), but not on the overall gained performance. finally, the existing imprecise components are not designed to support a pipelined approach, which could boost the operating frequency/throughput of, e.g., division-included applications. in this article, we propose rapid, the first pipelined approximate multiplier and divider architectures, customized for fpgas. the proposed units efficiently utilize 6-input look-up tables (6-luts) and fast carry chains to implement mitchell's approximate algorithms. our novel error refinement scheme not only has negligible overhead over the baseline mitchell's approach but also boosts its accuracy to 99.4% for arbitrary size of multiplication and division. experimental results obtained with xilinx vivado demonstrate the efficiency of the proposed pipelined and nonpipelined rapid multipliers and dividers over accurate counterparts. in particular, the 4-stage pipelined architecture of a 32-bit rapid multiplier (divider) enables 3.3x (5.1x) higher throughput, 2.3x (6.8x) higher throughput/watt, and 52% (31%) savings of look-up tables (luts), over their 4-stage pipelined, accurate intellectual property (ip) counterparts. moreover, the end-to-end evaluations of nonpipelined rapid, deployed in three multikernel applications in the domains of biosignal processing, image processing, and moving object tracking for unmanned aerial vehicles (uavs) indicate up to 35%, 33%, and 45% improvements in area, latency, and area-delay-product (adp), respectively, over accurate kernels, with negligible loss in qor. to springboard future research in reconfigurable and approximate computing communities, our implementations will be available and open sourced at https://cfaed.tu-dresden.de/pd-downloads.",AB_0498
"ensembles of convolutional neural networks have shown remarkable results in learning discriminative semantic features for image classification tasks. however, the models in the ensemble often concentrate on similar regions in images. this work proposes a novel method that forces a set of base models to learn different features for a classification task. these models are combined in an ensemble to make a collective classification. the key finding is that by forcing the models to concentrate on different features, the classification accuracy is increased. to learn different feature concepts, a so-called feature distance loss is implemented on the feature maps. the experiments on benchmark convolutional neural networks (vgg16, resnet, alexnet), popular datasets (cifar10, cifar100, miniimagenet, neu, bsd, tex), and different training samples (3, 5, 10, 20, 50, 100 per class) show the effectiveness of the proposed feature loss. the proposed method outperforms classical ensemble versions of the base models. the class activation maps explicitly prove the ability to learn different feature concepts. the code is available at: https://github.com/2obe/feature-distance-loss.git.",AB_0498
"the biological function of macromolecular complexes depends not only on large-scale transitions between conformations, but also on small-scale conformational fluctuations at equilibrium. information on the equi-librium dynamics of biomolecular complexes could, in principle, be obtained from local resolution (lr) data in cryo-electron microscopy (cryo-em) maps. however, this possibility had not been validated by comparing, for a same biomolecular complex, lr data with quantitative information on equilibrium dynam-ics obtained by an established solution technique. in this study we determined the cryo-em structure of the minute virus of mice (mvm) capsid as a model biomolecular complex. the lr values obtained corre-lated with crystallographic b factors and with hydrogen/deuterium exchange (hdx) rates obtained by mass spectrometry (hdx-ms), a gold standard for determining equilibrium dynamics in solution. this result validated a lr-based cryo-em approach to investigate, with high spatial resolution, the equilibrium dynamics of biomolecular complexes. as an application of this approach, we determined the cryo-em structure of two mutant mvm capsids and compared their equilibrium dynamics with that of the wild -type mvm capsid. the results supported a previously suggested linkage between mechanical stiffening and impaired equilibrium dynamics of a virus particle. cryo-em is emerging as a powerful approach for simultaneously acquiring information on the atomic structure and local equilibrium dynamics of biomolec-ular complexes.(c) 2023 the author(s). published by elsevier ltd. this is an open access article under the cc by-nc-nd license (http://crea-tivecommons.org/licenses/by-nc-nd/4.0/).",AB_0498
"objective de novo infections of the spine are an increasing healthcare problem. the decision for nonsurgical or surgical treatment is often made case by case on the basis of physician experience, specialty, or practice affiliation rather than evidence-based medicine. to create a more systematic foundation for surgical assessments of de novo spinal infections, the authors applied a formal validation process toward developing a spinal infection scoring system using prin-ciples gained from other spine severity scoring systems like the spine instability neoplastic score, thoracolumbar injury classification and severity score, and ao spine classification of thoracolumbar injuries. they utilized an expert panel and literature reviews to develop a severity scale called the spinal infection treatment evaluation score (site score). methods the authors conducted an evidence-based process of combining literature reviews, extracting key elements from previous scoring systems, and obtaining iterative expert panel input while following a formal delphi process. the resulting basic site scoring system was tested on selected de novo spinal infection cases and serially refined by an international multidisciplinary expert panel. intra-and interobserver reliabilities were calculated using the intraclass cor-relation coefficient (icc) and fleiss' and cohen's kappa, respectively. a receiver operating characteristic analysis was performed for cutoff value analysis. the predictive validity was assessed through cross-tabulation analysis.results the conceptual site scoring system combines the key variables of neurological symptoms, infection location, radiological variables for instability and impingement of neural elements, pain, and patient comorbidities. ten patients formed the first cohort of de novo spinal infections, which was used to validate the conceptual scoring system. a second cohort of 30 patients with de novo spinal infections, including the 10 patients from the first cohort, was utilized to validate the site score. mean scores of 6.73 +/- 1.5 and 6.90 +/- 3.61 were found in the first and second cohorts, respectively. the iccs for the total score were 0.989 (95% ci 0.975-0.997, p < 0.01) in the first round of scoring system validation, 0.992 (95% ci 0.981-0.998, p < 0.01) in the second round, and 0.961 (95% ci 0.929-0.980, p < 0.01) in the third round. the mean intraobserver reliability was 0.851 +/- 0.089 in the third validation round. the site score yielded a sensitivity of 97.77% +/- 3.87% and a specificity of 95.53% +/- 3.87% in the last validation round for the panel treatment decision.conclusions the site scoring concept showed statistically meaningful reliability parameters. hopefully, this effort will provide a foundation for a future evidence-based decision aid for treating de novo spinal infections. the site score showed promising inter-and intraobserver reliability. it could serve as a helpful tool to guide physicians' therapeutic deci- sions in managing de novo spinal infections and help in comparison studies to better understand disease severity and outcomes. https://thejns.org/doi/abs/10.3171/2022.11.spine22719",AB_0498
"light antinuclei, like antideuteron and antihelium-3, are ideal probes for new, exotic physics because their astrophysical backgrounds are suppressed at low energies. in order to exploit fully the inherent discovery potential of light antinuclei, a reliable description of their production cross sections in cosmic ray interactions is crucial. we provide therefore the cross sections of antideuteron and antihelium-3 production in pp, phe, hep, hehe, p over bar p and p over bar he collisions at energies relevant for secondary production in the milky way, in a tabulated form which is convinient to use. these predictions are based on qgsjetii-04m and the state of the art coalescence model wifunc, which evaluates the coalesence probability on an event-by-event basis, including both momentum correlations and the dependence on the emission volume. in addition, we comment on the importance of a monte carlo description of the antideuteron production and on the use of event generators in general. in particular, we discuss the effect of twoparticle momentum correlations provided by monte carlo event generators on antinuclei production. program summary program title: aafrag 2.01 cpc library link to program files: https://doi .org /10 .17632 /6f73jz6jx8 .2 developer's repository link: https://aafrag .sourceforge .io code ocean capsule: https://codeocean .com /capsule /1159734 licensing provisions: cc by-nc 4.0 programming language: the program exists both in a python 3 and fortran 90 version journal reference of previous version: comp. phys. comm. 245, 106846 (2019) does the new version supersede the previous version?: yes reasons for the new version: inclusion of antinuclei tables, and a python 3 version for the pedestrian. summary of revisions: improved formatting of tables, inclusion of new secondaries, inclusion of python version. nature of problem: calculation of secondaries (photons, neutrinos, electrons, positrons, protons, antiprotons, antideuterons and antihelium-3) produced in hadronic interactions. solution method: results from the monte carlo simulation qgsjet-ii-04m are interpolated. (c) 2023 the authors. published by elsevier b.v. this is an open access article under the cc by license ( .org /licenses /by /4 .0/).",AB_0498
"a set of routines to compute the magnetic vector potential and magnetic field of two types of current carriers is presented. the (infinitely thin) current carrier types are a straight wire segment and a circular wire loop. the routines are highly accurate and exhibit the correct asymptotic behavior far away from and close to the current carrier. a suitable global set of test points is introduced and the methods presented in this work are tested against results obtained using arbitrary-precision arithmetic on all test points. the results are accurate to approximately 16 decimal digits of precision when computed using 64 bit floating point arithmetic. there are a few exceptions where accuracy drops to 13 digits. these primitive current carrier types can be used to assemble more complex arrangements, such as a current along a polygon (by means of defining straight wire segments from point to point along the polygon) and a multi-winding coil with circular cross-section. reference data is provided along with the code for benchmarks with other implementations.program summaryprogram title: biot-savart routines with minimal floating point errorcpc library link to program files: https://doi .org /10 .17632 /zcwk7zzt9y.1developer's repository link: https://github .com /jonathanschilling /abscablicensing provisions: apache-2.0programming language: c, python, java, fortransupplementary material: reference output data for all methods described in this article.nature of problem: a common task in computational physics is to compute the magnetic field and magnetic vector potential originating from current-carrying wire arrangements. these current carriers are often approximated for computational simplicity as infinitely thin filaments following the center lines of the real current carrier. computational methods are thus needed to compute the magnetic field and the magnetic vector potential of a single straight wire segment or a single circular wire loop as basis for modeling more complex current carrier arrangements by superposition of the current carrier primitives. a current carrier path specified by (x, y, z) coordinates can be modeled as a set of straight wire segments from point to point along the polygon describing the current carrier geometry. closed circular wire loops are also commonly used as a proxy for a physical coil with helical windings.solution method: analytical expressions are derived in this work to accurately compute the magnetic field and the magnetic vector potential of a straight wire segment and a circular wire loop. the expressions consist of several special cases, which are automatically switched between depending on the location of the evaluation location in the coordinate system of the current carrier primitive. this approach guarantees that always the most accurate formulation available for a given evaluation location is used, including explicit use of simplifications in certain special cases for speed and accuracy.additional comments including restrictions and unusual features: for most test cases, the full relative accuracy of the floating point arithmetic chosen for implementation is retained throughout the computations (16 digits of precision in the ieee-754 binary64 implementation). there are a few",AB_0498
