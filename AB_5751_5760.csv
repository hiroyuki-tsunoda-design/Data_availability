AB,NO
"provenance encodes information that connects datasets, their generation workflows, and associated metadata (e.g., who or when executed a query). as such, it is instrumental for a wide range of critical governance applications (e.g., observability and auditing). unfortunately, in the context of database systems, extracting coarse-grained provenance is a long-standing problem due to the complexity and sheer volume of database workflows. provenance extraction from query event logs has been recently proposed as favorable because, in principle, can result in meaningful provenance graphs for provenance applications. current approaches, however, (a) add substantial overhead to the database and provenance extraction workflows and (b) extract provenance that is noisy, omits query execution dependencies, and is not rich enough for upstream applications. to address these problems, we introduce oneprovenance: an efficient provenance extraction system from query event logs. oneprovenance addresses the unique challenges of log-based extraction by (a) identifying query execution dependencies through efficient log analysis, (b) extracting provenance through novel event transformations that account for query dependencies, and (c) introducing effective filtering optimizations. our thorough experimental analysis shows that oneprovenance can improve extraction by up to similar to 18x compared to state-of-the-art baselines; our optimizations reduce the extraction noise and optimize performance even further. oneprovenance is deployed at scale by microsoft purview and actively supports customer provenance extraction needs (https://bit.ly/3n2jvgf).",AB_0576
"we present a deep learning method for composite and task-driven motion control for physically simulated characters. in contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a gan-like setup. in this process, there is no need of any manual work to produce composite reference motions for learning. instead, the control policy explores by itself how the composite motions can be combined automatically. we further account for multiple task-specific rewards and train a single, multi-objective control policy. to this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. in addition, as composite motions are typically augmentations of simpler behaviors, we introduce a sample-efficient method for training composite control policies in an incremental manner, where we reuse a pre-trained policy as the meta policy and train a cooperative policy that adapts the meta one for new composite tasks. we show the applicability of our approach on a variety of challenging multi-objective tasks involving both composite motion imitation and multiple goal-directed control. code is available at https://motion-lab.github.io/compositemotion.",AB_0576
"we propose factor matting, an alternative formulation of the video matting problem in terms of counterfactual video synthesis that is better suited for re-composition tasks. the goal of factor matting is to separate the contents of a video into independent components, each representing a counterfactual version of the scene where the contents of other components have been removed. we show that factor matting maps well to a more general bayesian framing of the matting problem that accounts for complex conditional interactions between layers. based on this observation, we present a method for solving the factor matting problem that learns augmented patch-based appearance priors to produce useful decompositions even for video with complex cross-layer interactions like splashes, shadows, and reflections. our method is trained per-video and does not require external training data or any knowledge about the 3d structure of the scene. through extensive experiments, we show that it is able to produce useful decompositions of scenes with such complex interactions while performing competitively on classical matting tasks as well. we also demonstrate the benefits of our approach on a wide range of downstream video editing tasks. our project website is at: https://factormatte.github.io/.",AB_0576
"we propose concavity-induced distance (cid) as a novel way to measure the dissimilarity between a pair of points in an unoriented point cloud. cid indicates the likelihood of two points or two sets of points belonging to different convex parts of an underlying shape represented as a point cloud. after analyzing its properties, we demonstrate how cid can benefit point cloud analysis without the need for meshing or normal estimation, which is beneficial for robotics applications when dealing with raw point cloud observations. by randomly selecting very few points for manual labeling, a cid-based point cloud instance segmentation via label propagation achieves comparable average precision as recent supervised deep learning approaches, on s3dis and scan-net datasets. moreover, cid can be used to group points into approximately convex parts whose convex hulls can be used as compact scene representations in robotics, and it outperforms the baseline method in terms of grouping quality. our project website is available at: https://ai4ce.github.io/cid/.",AB_0576
"connected and automated vehicles (cavs) are supposed to share the road with human-driven vehicles (hdvs) in a foreseeable future. therefore, considering the mixed traffic environment is more pragmatic, as the well-planned operation of cavs may be interrupted by hdvs. in the circumstance that human behaviors have significant impacts, cavs need to understand hdv behaviors to make safe actions. in this study, we develop a driver digital twin (ddt) for the online prediction of personalized lane-change behavior, allowing cavs to predict surrounding vehicles' behaviors with the help of the digital twin technology. ddt is deployed on a vehicle-edge-cloud architecture, where the cloud server models the driver behavior for each hdv based on the historical naturalistic driving data, while the edge server processes the real-time data from each driver with his/her digital twin on the cloud to predict the personalized lane-change maneuver. the proposed system is first evaluated on a human-in-the-loop co-simulation platform, and then in a field implementation with three passenger vehicles driving along an on/off ramp segment connecting to the edge server and cloud through the 4g/lte cellular network. the lane-change intention can be recognized in 6 s on average before the vehicle crosses the lane separation line, and the mean euclidean distance between the predicted trajectory and gps ground truth is 1.03 m within a 4-s prediction window. compared to the general model, using a personalized model can improve prediction accuracy by 27.8%. the demonstration video of the proposed system can be watched at https://youtu.be/5cbsabgiodm.",AB_0576
"background: in the united states, 37% of all opioids are prescribed in the surgical setting, many of which report initial exposure in the postoperative period. objective: this study aimed to assess the impact of a narcotic-sparing enhanced recovery after surgery protocol on postoperative narcotic use by patients and to assess its impact on the narcotic-prescribing practices of physicians. design: data regarding consecutive narcotic-naive patients who underwent a surgical procedure from january 2013 to august 2017 were retrospectively reviewed. settings: patients were divided into 2 cohorts: preimplementation (2013-2015) and postimplementation (2015-2017) of the enhanced recovery after surgery protocol. patients: this study included patients who underwent elective inpatient abdominal colorectal surgery at the university of florida health. main outcome measures: the primary outcome measure was 30-day postoperative narcotic use (inpatient and outpatient). other outcomes measured included pain scores, time to diet institution, length of hospital stay, cost of hospitalization, and postoperative complications. results: baseline characteristics were similar between the preprotocol group (n = 537) and postprotocol group (n = 790). protocol implementation was associated with a decrease in the total 30-day postoperative narcotic amount used by patients (2481 vs 31 morphine milligram equivalents; p = 0.05), inpatient patient-controlled analgesia use (63% vs 0.5%; p < 0.00001; dosage 1254 vs 5 morphine milligram equivalents), inpatient on-demand oral narcotic use (90% vs 32%; p = 0.001; dosage 47 vs 5 morphine milligram equivalents), and outpatient narcotic amount used (46 vs 6 morphine milligram equivalents; p = 0.001). average pain scores were similar. limitations: retrospective nature of the study and possible underestimation of pre- and postoperative narcotic use. conclusions: implementation of a narcotic-sparing enhanced recovery after surgery protocol was associated with a decrease in both inpatient and 30-day outpatient postoperative narcotic use. variation in resident physician prescribing practices suggests the need for ongoing education to accompany these protocols. see video abstract at http://links.lww.com/dcr/b936.",AB_0576
"szegedy developed a generic method for quantizing classical algorithms based on random walks (szegedy, in: 45th annual ieee symposium on foundations of computer science, pp 32-41, 2004. https://doi.org/ 10.1109/focs.2004.53). a major contribution of hisworkwas the construction of a walk unitary for any reversible random walk. such unitary posses two crucial properties: its eigenvector with eigenphase 0 is a quantum sample of the limiting distribution of the random walk and its eigenphase gap is quadratically larger than the spectral gap of the random walk. it was an open question if it is possible to generalize szegedy's quantization method for stochastic maps to quantum maps. we answer this in the affirmative by presenting an explicit construction of a szegedy walk unitary for detailed balanced lindbladians-generators of quantum markov semigroups-and detailed balanced quantum channels. we prove that our szegedy walk unitary has a purification of the fixed point of the lindbladian as eigenvector with eigenphase 0 and that its eigenphase gap is quadratically larger than the spectral gap of the lindbladian. to construct the walk unitary we leverage a canonical form for detailed balanced lindbladians showing that they are structurally related to davies generators. we also explain how the quantization method for lindbladians can be applied to quantum channels. we give an efficient quantum algorithm for quantizing davies generators that describe many important dynamics of open quantum systems, for instance, the relaxation of a quantum system coupled to a bath. our algorithm extends known techniques for simulating dynamics of quantum systems on a quantum computer.",AB_0576
"sea level rise (slr) may impose substantial economic costs to coastal communities worldwide, but characterizing its global impact remains challenging because slr costs depend heavily on natural characteristics and human investments at each location - including topography, the spatial distribution of assets, and local adaptation decisions. to date, several impact models have been developed to estimate the global costs of slr. yet, the limited availability of open-source and modular platforms that easily ingest up-to-date socioeconomic and physical data sources restricts the ability of existing systems to incorporate new insights transparently. in this paper, we present a modular, open-source platform designed to address this need, providing end-to-end transparency from global input data to a scalable least-cost optimization framework that estimates adaptation and net slr costs for nearly 10 000 global coastline segments and administrative regions. our approach accounts both for uncertainty in the magnitude of global mean sea level (g.m.s.l.) rise and spatial variability in local relative sea level rise. using this platform, we evaluate costs across 230 possible socioeconomic and slr trajectories in the 21st century. according to the latest intergovernmental panel on climate change assessment report (ar6), g.m.s.l. is likely to rise during the 21st century by 0.40-0.69 m if late-century warming reaches 2 degrees c and by 0.58-0.91 m with 4 degrees c of warming (fox-kemper et al., 2021). with no forward-looking adaptation, we estimate that annual costs of sea level rise associated with a 2 degrees c scenario will likely fall between usd 1.2 and 4.0 trillion (0.1% and 1.2% of gdp, respectively) by 2100, depending on socioeconomic and sea level rise trajectories. cost-effective, proactive adaptation would provide substantial benefits, lowering these values to between usd 110 and usd 530 billion (0.02 and 0.06 %) under an optimal adaptation scenario. for the likely slr trajectories associated with 4 degrees c warming, these costs range from usd 3.1 to 6.9 trillion (0.3% and 2.0%) with no forward-looking adaptation and usd 200 billion to usd 750 billion (0.04% to 0.09%) under optimal adaptation. the intergovernmental panel on climate change (ipcc) notes that deeply uncertain physical processes like marine ice cliff instability could drive substantially higher global sea level rise, potentially approaching 2.0m by 2100 in very high emission scenarios. accordingly, we also model the impacts of 1.5 and 2.0mg.m.s.l. rises by 2100; the associated annual cost estimates range from usd 11.2 to 30.6 trillion (1.2% and 7.6 %) under no forward-looking adaptation and usd 420 billion to 1.5 trillion (0.08% to 0.20 %) under optimal adaptation. our modeling platform used to generate these estimates is publicly available in an effort to spur research collaboration and support decision-making, with segment-level physical and socioeconomic input characteristics provided at https://doi.org/10.5281/zenodo.7693868 (bolliger et al., 2023a) and model results at https://doi.org/10.5281/zenodo.7693869 (bolliger et al., 2023b).",AB_0576
"we propose a statistical framework islet to infer individual-specific and cell-type-specific transcriptome reference panels. islet models the repeatedly measured bulk gene expression data, to optimize the usage of shared information within each subject. islet is the first available method to achieve individual-specific reference estimation in repeated samples. using simulation studies, we show outstanding performance of islet in the reference estimation and downstream cell-type-specific differentially expressed genes testing. we apply islet to longitudinal transcriptomes profiled from blood samples in a large observational study of young children and confirm the cell-type-specific gene signatures for pancreatic islet autoantibody. islet is avail-able at https://bioconductor.org/packages/islet.",AB_0576
"artificial meat is an eco-friendly alternative to real meat that is marketed to have a similar taste and feel. the mechanical properties of artificial meat significantly influence our perception of taste, but how precisely the mechanics of artificial meat compare to real meat remains insufficiently understood. here we perform mechanical tension, compression, and shear tests on isotropic artificial meat (tofurky & reg; plant-based deli slices), anisotropic artificial meat (daringtm chick'n pieces) and anisotropic real meat (chicken) and analyze the data using constitutive neural networks and automated model discovery. our study shows that, when deformed by 10%, artificial and real chicken display similar maximum stresses of 21.0 kpa and 21.8 kpa in tension,-7.2 kpa and-16.4 kpa in compression, and 2.4 kpa and 0.9 kpa in shear, while the maximum stresses for tofurky were 28.5 kpa,-38.3 kp, and 5.5 kpa. to discover the mechanics that best explain these data, we consulted two constitutive neural networks of ogden and valanis-landel type. both networks robustly discover models and parameters to explain the complex nonlinear behavior of artificial and real meat for individual tension, compression, and shear tests, and for all three tests combined. when constrained to the classical neo hooke, blatz ko, and mooney rivlin models, both networks discover shear moduli of 94.4 kpa for tofurky, 35.7 kpa for artificial chick'n, and 21.4 kpa for real chicken. our results suggests that artificial chicken succeeds in reproducing the mechanical properties of real chicken across all loading modes, while tofurky does not, and is about three times stiffer. strikingly, all three meat products display shear softening and their resistance to shear is about an order of magnitude lower than their resistance to tension and compression. we anticipate our study to inspire more quantitative, mechanistic comparisons of artificial and real meat. our automated-model-discovery based approach has the potential to inform the design of more authentic meat substitutes with an improved perception of taste, with the ultimate goal to reduce environmental impact, improve animal welfare, and mitigate climate change, while still offering the familiar taste and texture of traditional meat. our source code, data, and examples are available at https://github.com/livingmatterlab/canns. & copy; 2023 elsevier b.v. all rights reserved.",AB_0576
