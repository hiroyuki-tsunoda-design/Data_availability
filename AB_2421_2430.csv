AB,NO
"high-resolution (hr) medical images can provide rich details, which are important for discovering subtle lesions to make diagnoses. convolutional neural networks (cnns) are widely used in this field, but struggle to model long-range dependencies. although transformer-based methods have improved in this respect, this method requires large quantities of data. unfortunately, large quantities of low -resolution (lr) and hr medical image pairs may not always be available. in addition, most medical image superresolution (sr) methods are deterministic, while the degradation in real scenarios is stochastic. to address these problems, we introduce a probabilistic degradation model that combines natural and medical images for training. this design alleviates the problem of insufficient medical image pairs and learns the degradation process of the natural scene. in addition, we propose a new medical image sr model that consists of cnns and the swin transformer structure to excavate both local and global semantic features. moreover, to reduce computational stress, the spherical locality -sensitive hashing (slsh) module is employed in the nonlocal attention (nla) mechanism to form the enla module. this design enables the proposed sparse swin transformer (ssformer) model to generate hr medical images without extensive training images. experiments on diverse datasets (natural images and medical images) demonstrate that the proposed method is robust and effective, qualitatively and quantitatively outperforming other medical image sr methods. code is available at https://github.com/codehxj/ssformer.& copy; 2023 elsevier ltd. all rights reserved.",AB_0243
"fully convolutional neural (fcn) networks like u-net have been the state-of-the-art methods in colorectal polyp segmentation. however, u-net still has some limitations in modelling remote semantic information, especially since the semantic information at different levels can vary greatly, making it difficult to utilize this information fully. to address these issues, we propose a new network architecture called transefusionnet that utilizes the transformer's global modelling capability to better focus on global contextual semantic information. in addition, we added two feature fusion modules, a spatial feature fusion module (sfm) and an edge feature fusion module (efm), to the network. sef with a skip connection can improve the accuracy of passing deep features to shallow features. efm in the output part of each decoder layer improves the recognition of edge ambiguous features by refining the semantic information of the network. we validate the model's performance on five publicly available colorectal polyp datasets, and the experiments show that transefusionnet has higher segmentation accuracy. to measure the generalization ability of transefusionnet, we further applied the model to the cell nuclei dataset, which further verifies the performance of our model. code: https://github.com/linaaalin/transefusionnet.",AB_0243
"precise targeting of transcription factor binding sites (tfbss) is essential to comprehending transcriptional regulatory processes and investigating cellular function. although several deep learning algorithms have been created to predict tfbss, the models' intrinsic mechanisms and prediction results are difficult to explain. there is still room for improvement in prediction performance. we present deepstf, a unique deep-learning architecture for predicting tfbss by integrating dna sequence and shape profiles. we use the improved transformer encoder structure for the first time in the tfbss prediction approach. deepstf extracts dna higher order sequence features using stacked convolutional neural networks (cnns), whereas rich dna shape profiles are extracted by combining improved transformer encoder structure and bidirectional long short-term memory (bi-lstm), and, finally, the derived higher-order sequence features and representative shape profiles are integrated into the channel dimension to achieve accurate tfbss prediction. experiments on 165 encode chromatin immunoprecipitation sequencing (chip-seq) datasets show that deepstf considerably outperforms several state-of-the-art algorithms in predicting tfbss, and we explain the usefulness of the transformer encoder structure and the combined strategy using sequence features and shape profiles in capturing multiple dependencies and learning essential features. in addition, this paper examines the significance of dna shape features predicting tfbss. the source code of deepstf is available at https://github.com/yubinlab-qust/deepstf/.",AB_0243
"efficient and effective drug-target binding affinity (dtba) prediction is a challenging task due to the limited computational resources in practical applications and is a crucial basis for drug screening. inspired by the good representation ability of graph neural networks (gnns), we propose a simple-structured gnn model named ss-gnn to accurately predict dtba. by constructing a single undirected graph based on a distance threshold to represent protein-ligand interactions, the scale of the graph data is greatly reduced. moreover, ignoring covalent bonds in the protein further reduces the computational cost of the model. the graph neural network-multilayer perceptron (gnn-mlp) module takes the latent feature extraction of atoms and edges in the graph as two mutually independent processes. we also develop an edge-based atom-pair feature aggregation method to represent complex interactions and a graph pooling-based method to predict the binding affinity of the complex. we achieve state-of-the-art prediction performance using a simple model (with only 0.6 m parameters) without introducing complicated geometric feature descriptions. ss-gnn achieves pearson's r ( p ) = 0.853 on the pdbbind v2016 core set, outperforming state-of-the-art gnn-based methods by 5.2%. moreover, the simplified model structure and concise data processing procedure improve the prediction efficiency of the model. for a typical protein-ligand complex, affinity prediction takes only 0.2 ms. all codes are freely accessible at https://github.com/xianyuco/ss-gnn.",AB_0243
"recent studies reported that ion binding proteins (ibps) in phage play a key role in developing drugs to treat diseases caused by drug-resistant bacteria. therefore, correct recognition of ibps is an urgent task, which is beneficial for understanding their biological functions. to explore this issue, a new computational model was developed to identify ibps in this study. first, we used the physicochemical (pc) property and pearson's correlation coefficient (pcc) to denote protein sequences, and the temporal and spatial variabilities were employed to extract features. next, a similarity network fusion algorithm was employed to capture the correlation characteristics between these two different kinds of features. then, a feature selection method called f-score was utilized to remove the influence of redundant and irrelative information. finally, these reserved features were fed into support vector machine (svm) to discriminate ibps from non-ibps. experimental results showed that the proposed method has significant improvement in the classification performance, as compared with the state-of-the-art approach. the matlab codes and dataset used in this study are available at https://figshare.com/articles/ online resource/iibp-tsv/21779567 for academic use.",AB_0243
"motivation: transcriptional profiles of diverse tissues provide significant insights in both fundamental and translational researches, while transcriptome information is not always available for tissues that require invasive biopsies. alternatively, predicting tissue expression profiles from more accessible surrogate samples, especially blood transcriptome, has become a promising strategy when invasive procedures are not practical. however, existing approaches ignore tissue-shared intrinsic relevance, inevitably limiting predictive performance. results: we propose a unified deep learning-based multi-task learning framework, multi-tissue transcriptome mapping (mtm), enabling the prediction of individualized expression profiles from any available tissue of an individual. by jointly leveraging individualized cross-tissue information from reference samples through multi-task learning, mtm achieves superior sample-level and gene-level performance on unseen individuals. with the high prediction accuracy and the ability to preserve individualized biological variations, mtm could facilitate both fundamental and clinical biomedical research. availability and implementation: mtm's code and documentation are available upon publication on github (https://github.com/yangence/mtm).",AB_0243
"snow is a harsh natural phenomenon that greatly affects the performance of advanced computer vision tasks. recently, most image desnowing methods rely on complex model structures, leading to increased carbon emissions and impossibilities to deploy on lightweight computing devices. additionally, most deep learning-based methods do not effectively utilize the position information of snow particles, which will limit the performance of the model. to address these issues, considering that snow particles have block-shaped shape distribution and color uniformity similar to a mask, we propose a snowed autoencoder (sae) desnowing method. specifically, the sae is composed of four parts: snowed masking process, sae encoder, sae decoder, and prediction stage. first, a snowy image is passed through the snowed masking process to generate a mask that exactly covers the snow particles and output the coordinates of the four vertices of the mask and the index of each segmented image patch. the input image and each mask coordinate and index are passed through the sae encoder based on the snow particle attention module to generate an image representation for identifying typical features of snow particles. then, the generated typical features of snow particles are fed into the sae decoder to reconstruct image patches without snow. finally, all the patches will be passed through the prediction stage to reconstruct the original size of the snow-free image. a large number of experiments show that the proposed sae desnowing method achieves the state-of-the-art desnowing performance on three synthetic and one real-world desnowing datasets. the images after desnowing by sae have better color details and are more consistent with human visual habits. in addition, sae has faster desnowing speed and fewer parameters. the code is available at https://github.com/awhitewhale/sae. & copy; 2023 elsevier ltd. all rights reserved.",AB_0243
"single-cell rna sequencing (scrna-seq) measures transcriptome-wide gene expression at single-cell resolution. clustering analysis of scrna-seq data enables researchers to characterize cell types and states, shedding new light on cell-to-cell heterogeneity in complex tissues. recently, self-supervised contrastive learning has become a prominent technique for underlying feature representation learning. however, for the noisy, high-dimensional and sparse scrna-seq data, existing methods still encounter difficulties in capturing the intrinsic patterns and structures of cells, and seldom utilize prior knowledge, resulting in clusters that mismatch with the real situation. to this end, we propose scdecl, a novel deep enhanced constraint clustering algorithm for scrna-seq data analysis based on contrastive learning and pairwise constraints. specifically, based on interpolated contrastive learning, a pre-training model is trained to learn the feature embedding, and then perform clustering according to the constructed enhanced pairwise constraint. in the pre training stage, a mixup data augmentation strategy and interpolation loss is introduced to improve the diversity of the dataset and the robustness of the model. in the clustering stage, the prior information is converted into enhanced pairwise constraints to guide the clustering. to validate the performance of scdecl, we compare it with six state-of-the-art algorithms on six real scrna-seq datasets. the experimental results demonstrate the proposed algorithm outperforms the six competing methods. in addition, the ablation studies on each module of the algorithm indicate that these modules are complementary to each other and effective in improving the performance of the proposed algorithm. our method scdecl is implemented in python using the pytorch machine-learning library, and it is freely available at https://github.com/dblabdhu/scdecl.",AB_0243
"background: closing gaps in draft genomes leads to more complete and continuous genome assemblies. the ubiquitous genomic repeats are challenges to the existing gap-closing methods, based on either the k-mer representation by the de bruijn graph or the overlap-layout- consensus paradigm. besides, chimeric reads will cause erroneous k-mers in the former and false overlaps of reads in the latter. results: we propose a novel local assembly approach to gap closing, called regcloser. it represents read coordinates and their overlaps respectively by parameters and observations in a linear regression model. the optimal overlap is searched only in the restricted range consistent with insert sizes. under this linear regression framework, the local dna assembly becomes a robust parameter estimation problem. we solved the problem by a customized robust regression procedure that resists the influence of false overlaps by optimizing a convex global huber loss function. the global optimum is obtained by iteratively solving the sparse system of linear equations. on both simulated and real datasets, regcloser outperformed other popular methods in accurately resolving the copy number of tandem repeats, and achieved superior completeness and contiguity. applying regcloser to a plateau zokor draft genome that had been improved by long reads further increased contig n50 to 3-fold long. we also tested the robust regression approach on layout generation of long reads. conclusions: regcloser is a competitive gap- closing tool. the software is available at https://github.com/csh3/regcloser. the robust regression approach has a prospect to be incorporated into the layout module of long read assemblers.",AB_0243
"image fusion aims to integrate complementary characteristics of source images into a single fused image that better serves human visual observation and machine vision perception. however, most existing image fusion algorithms primarily focus on improving the visual appeal of fused images. although there are some semantic-driven methods that consider semantic requirements of downstream applications, none of them have demonstrated the potential of image-level fusion compared to feature-level fusion, which fulfills high-level vision tasks directly on multi-modal features rather than on a fused image. to overcome these limitations, this paper presents a practical infrared and visible image fusion network based on progressive semantic injection and scene fidelity constraints, termed psfusion. first of all, the sparse semantic perception branch extracts sufficient semantic features, which are then progressively integrated into the fusion network using the semantic injection module to fulfill the semantic requirements of high-level vision tasks. the scene fidelity path within the scene restoration branch is devised to ensure that the fusion features contain complete information for reconstructing the source images. additionally, the contrast mask and salient target mask are employed to construct the fusion loss to maintain impressive visual effects of fusion results. in particular, we provide quantitative and qualitative analyses to demonstrate the potential of image-level fusion compared to feature-level fusion for high-level vision tasks. with the rapid advancement of large-scale models, image-level fusion can expeditiously leverage the advantages of multi-modal data and state-of-the-art (sota) unimodal segmentation to achieve superior performance. furthermore, extensive comparative experiments demonstrate the superiority of our psfusion over sota image-level fusion alternatives in terms of visual appeal and high-level semantics. even under harsh circumstances, our method offers satisfactory fusion results to serve subsequent high-level vision applications. the source code is available at https://github.com/linfeng-tang/ psfusion.",AB_0243
