AB,NO
"cryo-electron microscopy (cryo-em) allows a macromolecular structure such as protein-dna/rna complexes to be reconstructed in a three-dimensional coulomb potential map. the structural information of these macromolecular complexes forms the foundation for understanding the molecular mechanism including many human diseases. however, the model building of large macromolecular complexes is often difficult and time-consuming. we recently developed deeptracer-2.0, an artificial-intelligence-based pipeline that can build amino acid and nucleic acid backbones from a single cryo-em map, and even predict the best-fitting residues according to the density of side chains. the experiments showed improved accuracy and efficiency when benchmarking the performance on independent experimental maps of protein-dna/rna complexes and demonstrated the promising future of macromolecular modeling from cryo-em maps. our method and pipeline could benefit researchers worldwide who work in molecular biomedicine and drug discovery, and substantially increase the throughput of the cryo-em model building. the pipeline has been integrated into the web portal https://deeptracer.uw.edu/.",AB_0615
"oncogenetic graphical models are crucial for understanding cancer progression by analyzing the accumulation of genetic events. these models are used to identify statistical dependencies and temporal order of genetic events, which helps design targeted therapies. however, existing algorithms do not account for temporal differences between samples in oncogenetic analysis. this paper introduces timed hazard networks (timedhn), a new statistical model that uses temporal differences to improve accuracy and reliability. timedhn models the accumulation process as a continuous-time markov chain and includes an efficient gradient computation algorithm for optimization. our simulation experiments demonstrate that timedhn outperforms current state-of-the-art graph reconstruction methods. we also compare timedhn with existing methods on a luminal breast cancer dataset, highlighting its potential utility. the matlab implementation and data are available at https://github.com/puar-playground/timedhn",AB_0615
"the ocean carbon and acidification data system (ocads) is a data management system at the national oceanic and atmospheric administration (noaa) national centers for environmental information (ncei). it manages a wide range of ocean carbon and acidification data, including chemical, physical, and biological observations collected from research vessels, ships of opportunity, and uncrewed platforms, as well as laboratory experiment results, and model outputs. additionally, ocads serves as a repository for related global ocean observing system (goos) biogeochemistry essential ocean variables (eovs), e.g., oxygen, nutrients, transient tracers, and stable isotopes. ocads endeavors to be one of the world's leading providers of ocean carbon and acidification data, information, products, and services. to provide the best data management services to the ocean carbon and acidification research community, ocads prioritizes adopting a customer-centric approach and gathering knowledge and expertise from the research community to improve its data management practices. ocads aims to make all ocean carbon and acidification data accessible via a single portal, and welcomes submissions from around the world: https://www.ncei.noaa.gov/products/ocean-carbon-acidification-data-system/.",AB_0615
"transmembrane proteins (tmps), with diverse cellular functions, are difficult targets for structural determination. predictions of tmps and the locations of transmembrane segments using computational methods could be unreliable due to the potential for false positives and false negatives and show inconsistencies across different programs. recent advances in protein structure prediction methods have made it possible to identify tmps and their membrane-spanning regions using high-quality structural models. we developed the alphafold transmembrane proteins (aftm) database of candidate human tmps by identifying transmembrane regions in alphafold structural models of human proteins and their domains using the positioning of proteins in membranes, version 3 program, followed by automatic corrections inspired by manual analysis of the results. we compared our results to annotations from the uniprot database and the human transmembrane proteome (htp) database. while aftm did not identify transmembrane regions in some single-pass tmps, it identified more transmembrane regions for multipass tmps than uniprot and htp. aftm also showed more consistent results with experimental structures, as benchmarked against the protein data bank transmembrane proteins (pdbtm) database. in addition, some proteins previously annotated as tmps were suggested to be non-tmps by aftm. we report the results of aftm together with those of uniprot, htp, tmalphafold, pdbtm and membranome in the online aftm database compiled as a comprehensive resource of candidate human tmps with structural models. database url: http://conglab.swmed.edu/aftm",AB_0615
"high-throughput screening (hts) methods enable the empirical evaluation of a large scale of compounds and can be augmented by virtual screening (vs) techniques to save time and money by using potential active compounds for experimental testing. structure-based and ligand-based virtual screening approaches have been extensively studied and applied in drug discovery practice with proven outcomes in advancing candidate molecules. however, the experimental data required for vs are expensive, and hit identification in an effective and efficient manner is particularly challenging during early-stage drug discovery for novel protein targets. herein, we present our target-driven machine learning-enabled vs (tame-vs) platform, which leverages existing chemical databases of bioactive molecules to modularly facilitate hit finding. our methodology enables bespoke hit identification campaigns through a userdefined protein target. the input target id is used to perform a homologybased target expansion, followed by compound retrieval from a large compilation of molecules with experimentally validated activity. compounds are subsequently vectorized and adopted for machine learning (ml) model training. these machine learning models are deployed to perform modelbased inferential virtual screening, and compounds are nominated based on predicted activity. our platform was retrospectively validated across ten diverse protein targets and demonstrated clear predictive power. the implemented methodology provides a flexible and efficient approach that is accessible to a wide range of users. the tame-vs platform is publicly available at https://github. com/bymgood/target-driven-ml-enabled-vs to facilitate early-stage hit identification.",AB_0615
"airborne transmission by droplets and aerosols is known to play a critical role in the spread of many viruses amongst which are the common flu and the more recent sars-cov-2 viruses. in the case of sars-cov-2, the nasal cavity not only constitutes an important viral entry point, but also a primary site of infection (sungnak w. et al. nat. med. 26:681-687. https:// doi. org/ 10. 1038/ s41591- 020- 08686, 2020).. although face masks are a well-established preventive measure, development of novel and easy-to-use prophylactic measures would be highly beneficial in fighting viral spread and the subsequent emergence of variants of concern (tao k. et al. nat rev genet 22:757-773. https:// doi. org/ 10. 1038/ s41576- 021-00408-x, 2021). our group has been working on optimizing a nasal spray delivery system that deposits particles inside the susceptible regions of the nasal cavity to act as a mechanical barrier to impede viral entry. here, we identify computationally the delivery parameters that maximize the protection offered by this barrier. we introduce the computational approach and quantify the protection rate obtained as a function of a broad range of delivery parameters. we also introduce a modified design and demonstrate that it significantly improves deposition, thus constituting a viable approach to protect against nasal infection of airborne viruses. we then discuss our findings and the implications of this novel system on the prevention of respiratory diseases and targeted drug delivery.",AB_0615
"the human cerebral cortex undergoes dramatic and critical development during early postnatal stages. benefiting from advances in neuroimaging, many infant brain magnetic resonance imaging (mri) datasets have been collected from multiple imaging sites with different scanners and imaging protocols for the investigation of normal and abnormal early brain development. however, it is extremely challenging to precisely process and quantify infant brain development with these multisite imaging data because infant brain mri scans exhibit (a) extremely low and dynamic tissue contrast caused by ongoing myelination and maturation and (b) inter-site data heterogeneity resulting from the use of diverse imaging protocols/scanners. consequently, existing computational tools and pipelines typically perform poorly on infant mri data. to address these challenges, we propose a robust, multisite-applicable, infant-tailored computational pipeline that leverages powerful deep learning techniques. the main functionality of the proposed pipeline includes preprocessing, brain skull stripping, tissue segmentation, topology correction, cortical surface reconstruction and measurement. our pipeline can handle both t1w and t2w structural infant brain mr images well in a wide age range (from birth to 6 years of age) and is effective for different imaging protocols/scanners, despite being trained only on the data from the baby connectome project. extensive comparisons with existing methods on multisite, multimodal and multi-age datasets demonstrate superior effectiveness, accuracy and robustness of our pipeline. we have maintained a website, ibeat cloud, for users to process their images with our pipeline (http://www.ibeat.cloud), which has successfully processed over 16,000 infant mri scans from more than 100 institutions with various imaging protocols/scanners.",AB_0615
"background an urgent need exists to rapidly screen potential therapeutics for severe covid-19 or other emerging pathogens associated with high morbidity and mortality. methods using an adaptive platform design created to rapidly evaluate investigational agents, hospitalised patients with severe covid-19 requiring >= 6 l/min oxygen were randomised to either a backbone regimen of dexamethasone and remdesivir alone (controls) or backbone plus one open-label investigational agent. patients were enrolled to the arms described between july 30, 2020 and june 11, 2021 in 20 medical centres in the united states. the platform contained up to four potentially available investigational agents and controls available for randomisation during a single time-period. the two primary endpoints were time-to-recovery (<6 l/min oxygen for two consecutive days) and mortality. data were evaluated biweekly in comparison to pre-specified criteria for graduation (i.e., likely efficacy), futility, and safety, with an adaptive sample size of 40-125 individuals per agent and a bayesian analytical approach. criteria were designed to achieve rapid screening of agents and to identify large benefit signals. concurrently enrolled controls were used for all analyses. https://clinicaltrials.gov/ct2/show/nct04488081. findings the first 7 agents evaluated were cenicriviroc (ccr2/5 antagonist; n = 92), icatibant (bradykinin antagonist; n = 96), apremilast (pde4 inhibitor; n = 67), celecoxib/famotidine (cox2/histamine blockade; n = 30), ic14 (anti-cd14; n = 67), dornase alfa (inhaled dnase; n = 39) and razuprotafib (tie2 agonist; n = 22). razuprotafib was dropped from the trial due to feasibility issues. in the modified intention-to-treat analyses, no agent met pre-specified efficacy/ graduation endpoints with posterior probabilities for the hazard ratios [hrs] for recovery <= 1.5 between 0.99 and 1.00. the data monitoring committee stopped celecoxib/famotidine for potential harm (median posterior hr for recovery 0.5, 95% credible interval [cri] 0.28-0.90; median posterior hr for death 1.67, 95% cri 0.79-3.58). interpretation none of the first 7 agents to enter the trial met the prespecified criteria for a large efficacy signal. celecoxib/famotidine was stopped early for potential harm. adaptive platform trials may provide a useful approach to rapidly screen multiple agents during a pandemic. funding quantum leap healthcare collaborative is the trial sponsor. funding for this trial has come from: the covid r&d consortium, allergan, amgen inc., takeda pharmaceutical company, implicit bioscience, johnson & johnson, pfizer inc., roche/genentech, apotex inc., fast grant from emergent venture george mason university, the dod defense threat reduction agency (dtra), the department of health and human services biomedical advanced research and development authority (barda), and the grove foundation. effort sponsored by the u.s. government under other transaction number w15qkn-16-9-1002 between the mcdc, and the government. copyright (c) 2023 the author. published by elsevier ltd. this is an open access article under the cc by-nc-nd license ().",AB_0615
"background: data archiving and distribution are essential to scientific rigor and reproducibility of research. the national center for biotechnology information's database of genotypes and phenotypes (dbgap) is a public repository for scientific data sharing. to support curation of thousands of complex data sets, dbgap has detailed submission instructions that investigators must follow when archiving their data.results: we developed dbgapcheckup, an r package which implements a series of check, awareness, reporting, and utility functions to support data integrity and proper formatting of the subject phenotype data set and data dictionary prior to dbgap submission. for example, as a tool, dbgapcheckup ensures that the data dictionary contains all fields required by dbgap, and additional fields required by dbgapcheckup; the number and names of variables match between the data set and data dictionary; there are no duplicated variable names or descriptions; observed data values are not more extreme than the logical minimum and maximum values stated in the data dictionary; and more. the package also includes functions that implement a series of minor/scalable fixes when errors are detected (e.g., a function to reorder the variables in the data dictionary to match the order listed in the data set). finally, we also include reporting functions that produce graphical and textual descriptives of the data to further reduce the likelihood of data integrity issues. the dbgapcheckup r package is available on cran (https://cran.r-project.org/package=dbgapcheckup) and developed on github (https://github.com/lwheinsberg/dbgapcheckup).conclusion: dbgapcheckup is an innovative assistive and timesaving tool that fills an important gap for researchers by making dbgap submission of large and complex data sets less error prone.",AB_0615
"accurate protein quantification is key to identifying protein markers, regulatory relationships between proteins, and pathophysiological mechanisms. realizing this potential requires sensitive and deep protein analysis of a large number of samples. toward this goal, proteomics throughput can be increased by parallelizing the analysis of both precursors and samples using multiplexed data independent acquisition (dia) implemented by the plexdia framework: https://plexdia.slavovlab.net. here we demonstrate the improved precisions of retention time estimates within plexdia and how this enables more accurate protein quantification. plexdia has demonstrated multiplicative gains in throughput, and these gains may be substantially amplified by improving the multiplexing reagents, data acquisition, and interpretation. we discuss future directions for advancing plexdia, which include engineering optimized mass-tags for high-plexdia, introducing isotopologous carriers, and developing algorithms that utilize the regular structures of plexdia data to improve sensitivity, proteome coverage, and quantitative accuracy. these advances in plexdia will increase the throughput of functional proteomic assays, including quantifying protein conformations, turnover dynamics, modifications states and activities. the sensitivity of these assays will extend to single-cell analysis, thus enabling functional",AB_0615
