AB,NO
"this work presents an innovative method for point set self-embedding, that encodes the structural information of a dense point set into its sparser version in a visual but imperceptible form. the self-embedded point set can function as the ordinary downsampled one and be visualized efficiently on mobile devices. particularly, we can leverage the self-embedded information to fully restore the original point set for detailed analysis on remote servers. this task is challenging, since both the self-embedded point set and the restored point set should resemble the original one. to achieve a learnable self-embedding scheme, we design a novel framework with two jointly-trained networks: one to encode the input point set into its self-embedded sparse point set and the other to leverage the embedded information for inverting the original point set back. further, we develop a pair of up-shuffle and down-shuffle units in the two networks, and formulate loss terms to encourage the shape similarity and point distribution in the results. extensive qualitative and quantitative results demonstrate the effectiveness of our method on both synthetic and real-scanned datasets. the source code and trained models will be publicly available at https://github.com/liruihui/self-embedding.",AB_0242
"rosa rugosa, a perennial shrub belonging to family rosaceae, is a well-known ornamental plant. its petals contain an abundance of essential oils and anthocyanins with enormous economic and health benefits when used as edible or cosmetic ingredients. the whole genome of r. rugosa was sequenced in 2021, which provided opportunities and challenges for gene regulation. however, many gene functions remain unknown. therefore, an analytical platform named roseap (http://www.gzybioinformatics.cn/roseap/ index.php) for the functional analysis of r. rugosa genes was constructed. it improved the gene annotation rate by integrating and analyzing genomic and transcriptomic datasets. first, 38,815 genes, covering 97.76% of the coding genes, were annotated functionally and structurally using a variety of algorithms and rules. second, a total of 33 transcriptome samples were integrated, including 23 samples from our lab and 10 samples from the sra database. a co-expression network containing approximately 29,657 positive or negative gene pairs, covering 74.7% of the coding genes, was constructed based on pcc and mr algorithms. network analysis revealed that the dfr function was closely related to anthocyanin metabolism. it demonstrated the reliability of the network. several saur genes of r. rugosa shared similar expression patterns. roseap was used to determine the sequence, structure, functional annotation, expression profile, regulatory network, and functional modules at the transcriptional and protein levels by inputting gene ids. in addition, auxiliary analytical tools, including blast, gene set enrichment, orthologue conversion, gene sequence extraction, gene expression value extraction, and jbrowse, were utilized. regular updates to roseap are expected to facilitate mining of gene function and promote genetic improvement in r. rugosa.",AB_0242
"g-quadruplex (g4), a non-canonical nucleic acid structure, has been suggested to play a key role in important cellular processes including transcription, replication and cancer development. recently, high-throughput sequencing approaches for g4 detection have provided a large amount of experimentally identified g4 data that reveal genome-wide g4 landscapes and enable the development of new methods for predicting potential g4s from sequences. although several existing databases provide g4 experimental data and relevant biological information from different perspectives, there is no dedicated database to collect and analyze dna g4 experimental data genome-widely. here, we constructed g4bank, a database of experimentally identified dna g-quadruplex sequences. a total of 6,915,983 dna g4s were collected from 13 organisms, and state-of-the-art prediction methods were performed to filter and analyze the g4 data. therefore, g4bank will facilitate users to access comprehensive g4 experimental data and enable sequence feature analysis of g4 for further investigation. the database of the experimentally identified dna g-quadruplex sequences can be accessed at http://tubic.tju.edu.cn/g4bank/. [graphics] .",AB_0242
"cd47/sirpa pathway is a new breakthrough in the field of tumor immunity after pd-1/pd-l1. while current monoclonal antibody therapies targeting cd47/sirpa have demonstrated some anti-tumor effectiveness, there are several inherent limitations associated with these formulations. in the paper, we developed a predictive model that combines next-generation phage display (ngpd) and traditional machine learning methods to distinguish cd47 binding peptides. first, we utilized ngpd biopanning technology to screen cd47 binding peptides. second, ten traditional machine learning methods based on multiple peptide descriptors and three deep learning methods were used to build computational models for identifying cd47 binding peptides. finally, we proposed an integrated model based on support vector machine. during the five-fold cross-validation, the integrated predictor demonstrated specificity, accuracy, and sensitivity of 0.755, 0.764, and 0.772, respectively. furthermore, an online bioinformatics tool called cd47binder has been developed for the integrated predictor. this tool is readily accessible on http://i.uestc.edu.cn/cd47binder/cgi-bin/cd47binder.pl.",AB_0242
"multi-exposure image fusion (mef) is an affordable and convenient option for high-dynamic-range imaging. current mef methods are prone to visually unrealistic results since they take no account of perceptual factors. to address this problem, a multi-exposure image fusion method is proposed based on perception enhanced structural patch decomposition, namely pespd-mef. an image patch is first decomposed into four components: perceptual gain, signal strength, signal structure, and mean intensity. then, the enhancement rule is designed for perceptual gain, and the latter three elements are fused independently in different ways. finally, the fused components are aggregated to generate informative and perception-realistic results. moreover, the multi-scale framework is adopted to boost the fused performance. the proposed method is also extended to address single low-light image enhancement issue. experimental results demonstrate that the proposed approach outperforms the state-of-the-art methods by a large margin in terms of perceptual realism. the source code is available at https://github.com/junchao2018/pespd-mef.",AB_0242
"existing lightweight networks perform inferior to large-scale models in human pose estimation because of shallow model depths and limited receptive fields. current approaches utilize large convolution kernels or attention mechanisms to encourage long-range receptive field learning at the expense of model redundancy. in this paper, we propose a novel multi-scale field lightweight high-resolution network (mfite-hrnet) for human pose estimation. specifically, our model mainly consists of two lightweight blocks, a multi-scale receptive field block (mrb) and a large receptive field block (lrb), to learn informative multi-scale and long-range spatial context information. the mrb utilizes group depthwise dilation convolutions with varied dilation rates to extract multi-scale spatial relationships from different feature maps. the lrb leverages large depthwise convolution kernels to model large-range spatial knowledge at the low-level features. we apply mfite-hrnet to single-person and multi-person pose estimation tasks. experiments on coco, mpii, and crowdpose datasets demonstrate that our network outperforms current state-of-the-art lightweight networks in either single-person or multi-person pose estimation tasks. the source code will be publicly available at https://github.com/lskdje/mfite-hrnet.git.",AB_0242
"motivation: large-scale prediction of drug-target affinity (dta) plays an important role in drug discovery. in recent years, machine learning algorithms have made great progress in dta prediction by utilizing sequence or structural information of both drugs and proteins. however, sequence-based algorithms ignore the structural information of molecules and proteins, while graph-based algorithms are insufficient in feature extraction and information interaction. results: in this article, we propose nhgnn-dta, a node-adaptive hybrid neural network for interpretable dta prediction. it can adaptively acquire feature representations of drugs and proteins and allow information to interact at the graph level, effectively combining the advantages of both sequence-based and graph-based approaches. experimental results have shown that nhgnn-dta achieved new state-of-the-art performance. it achieved the mean squared error (mse) of 0.196 on the davis dataset (below 0.2 for the first time) and 0.124 on the kiba dataset (3% improvement). meanwhile, in the case of cold start scenario, nhgnn-dta proved to be more robust and more effective with unseen inputs than baseline methods. furthermore, the multi-head self-attention mechanism endows the model with interpretability, providing new exploratory insights for drug discovery. the case study on omicron variants of sars-cov-2 illustrates the efficient utilization of drug repurposing in covid-19. availability and implementation: the source code and data are available at https://github.com/hehh77/nhgnn-dta.",AB_0242
"china has set a goal to achieve carbon neutrality by 2060, and satellite remote sensing allows for acquiring large-range and high-resolution carbon dioxide (co2) data, which can aid in achieving this goal. however, satellite-derived column-averaged dry-air mole fraction of co2 (xco2) products often suffer from substantial spatial gaps due to the im-pacts of narrow swath and clouds. here, this paper generates daily full-coverage xco2 data at a high spatial resolution of 0.1 & deg; over china during 2015-2020, by fusing satellite observations and reanalysis data in a deep neural network (dnn) framework. specifically, dnn constructs the relationships between orbiting carbon observatory-2 satellite xco2 retrievals, copernicus atmosphere monitoring service (cams) xco2 reanalysis data, and environmental factors. then, daily full-coverage xco2 data can be generated based on cams xco2 and environmental factors. results show that a satisfactory performance is reported in multiform validations, with rmse and r2 of 0.99 ppm and 0.963 in terms of the sample-based cross-validation, respectively. the independent in-situ validation also indicates high consistency (r2 = 0.866 and rmse = 1.71 ppm) between xco2 estimates and ground measurements. based on the generated dataset, spatial and seasonal distributions of xco2 across china are investigated, and a growth rate of 2.71 ppm/yr is found from 2015 to 2020. this paper generates long time series of full-coverage xco2 data, which helps promote our understanding of carbon cycling. the dataset is available from https://doi.org/10.5281/zenodo.7793917.",AB_0242
"high throughput nuclear segmentation and classification of whole slide images (wsis) is crucial to biological analysis, clinical diagnosis and precision medicine. with the advances of cnn algorithms and the continuously growing datasets, considerable progress has been made in nuclear segmentation and classification. however, few works consider how to reasonably deal with nuclear heterogeneity in the following two aspects: imbalanced data distribution and diversified morphology characteristics. the minority classes might be dominated by the majority classes due to the imbalanced data distribution and the diversified morphology characteristics may lead to fragile segmentation results. in this study, a cost -sensitive multi-task learning (smile) framework is conducted to tackle the data heterogeneity problem. based on the most popular multi-task learning backbone in nuclei segmentation and classification, we propose a multi-task correlation attention (mtca) to perform feature interaction of multiple high relevant tasks to learn better feature representation. a cost-sensitive learning strategy is proposed to solve the imbalanced data distribution by increasing the penalization for the error classification of the minority classes. furthermore, we propose a novel post-processing step based on the coarse-to-fine marker-controlled watershed scheme to alleviate fragile segmentation when nuclei are with large size and unclear contour. extensive experiments show that the proposed method achieves state-of-the-art performances on consep and monusac 2020 datasets. the code is available at: https://github.com/ panxipeng/nuclear_segandcls.",AB_0242
"assessing the quality of sequencing data plays a crucial role in downstream data analysis. however, existing tools often achieve sub-optimal efficiency, especially when dealing with compressed files or performing complicated quality control operations such as over-representation analysis and error correction. we present rabbitqcplus, an ultra-efficient quality control tool for modern multi-core systems. rabbitqcplus uses vectorization, memory copy reduction, parallel (de)compression, and optimized data structures to achieve substantial performance gains. it is 1.1 to 5.4 times faster when performing basic quality control operations compared to state-of-the-art applications yet requires fewer compute resources. moreover, rabbitqcplus is at least 4 times faster than other applications when processing gzip-compressed fastq files and 1.3 times faster with the error correction module turned on. furthermore, it takes less than 4 minutes to process 280 gb of plain fastq sequencing data, while other applications take at least 22 minutes on a 48-core server when enabling the per-read over-representation analysis. c++ sources are available at https://github .com /rabbitbio /rabbitqcplus.",AB_0242
