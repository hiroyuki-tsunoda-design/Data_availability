AB,NO
"multicast communication technology is widely applied in wireless environments with a high device density. traditional wireless network architectures have difficulty flexibly obtaining and maintaining global network state information and cannot quickly respond to network state changes, thus affecting the throughput, delay, and other qos requirements of existing multicasting solutions. therefore, this paper proposes a new multicast routing method based on multiagent deep reinforcement learning (madrl-mr) in a software-defined wireless networking (sdwn) environment. first, sdwn technology is adopted to flexibly configure the network and obtain network state information in the form of traffic matrices representing global network links information, such as link bandwidth, delay, and packet loss rate. second, the multicast routing problem is divided into multiple subproblems, which are solved through multiagent cooperation. to enable each agent to accurately understand the current network state and the status of multicast tree construction, the state space of each agent is designed based on the traffic and multicast tree status matrices, and the set of ap nodes in the network is used as the action space. a novel single-hop action strategy is designed, along with a reward function based on the four states that may occur during tree construction: progress, invalid, loop, and termination. finally, a decentralized training approach is combined with transfer learning to enable each agent to quickly adapt to the dynamic changes of network link information and accelerate convergence. simulation experiments show that madrl-mr outperforms existing algorithms in terms of throughput, delay, packet loss rate, etc., and can establish more intelligent multicast routes. code and model are available at https://github.com/guetye/madrl-mr code.",AB_0273
"geospatial object segmentation is a fundamental task in remote sensing image interpretation. although deep learning has shown great potential for this task, it often suffers from limited receptive fields, insufficient global feature extraction ability, and inaccurate edge positioning, resulting in low accuracy and errors in the results. in this letter, we propose a novel global context dependency aware network (gcdnet) to achieve high-accuracy segmentation results. to overcome the limited receptive fields and promote feature extraction ability, we propose a new dot-product attention (dpa) mechanism to establish long-distance dependencies between different receptive field feature maps. to achieve more accurate object edges, we design an edge-aware optimization (eao) module to guide the operation to directly optimize the edge details from the prediction result at the pixel level. extensive experiments on two well-known public high-resolution remote sensing image datasets have been conducted to verify the performance of the proposed method, and the results show that the proposed method has significant advantages and maintains its robustness in different cases. code will be available at: https://github.com/cuiadd/gcdnet.",AB_0273
"remote sensing image change captioning (rsicc) is a novel task that aims to describe the differences between bitemporal images by natural language. previous methods ignore a significant specificity of the task: the difficulty of rsicc is different for unchanged and changed image pairs. they process the unchanged and changed image pairs in a coupled way, which usually causes confusion for change captioning. in this article, we decouple the task into two issues to ease it: whether and what changes have occurred. an image-level classifier performs binary classification to address the first issue. a feature-level encoder contributes to extracting discriminative features to help the caption generation module address the second issue. besides, for caption generation, we utilize prompt learning to introduce pretrained large language models (llms) into the rsicc task. a multiprompt learning strategy is proposed to generate a set of unified prompts and a class-specific prompt conditioned on the image-level classifier's results. the strategy can prompt a pretrained llm to know whether changes exist and generate captions. finally, the multiple prompts and the visual features of the feature-level encoder are fed into a frozen llm for language generation. compared with previous methods, our method can leverage the powerful abilities of the pretrained llm in language to generate plausible captions, which is free of training. extensive experiments show that our method is effective and achieves the state-of-the-art performance. besides, an additional experiment demonstrates that our decoupling paradigm is more promising than the previous coupled paradigm for the rsicc task. we will make our codebase publicly available to facilitate future research at https://github.com/chen-yang-liu/promptcc.",AB_0273
"few-shot segmentation (fss) endeavors to utilize a minimal amount of annotated samples (support) to guide the segmentation of unseen objects (query). previous techniques primarily employ a support-to-query paradigm, neglecting to sufficiently leverage the mutual representation between query and support images, which leaves models suffering from intra-class variations and background interference in remote sensing images. this article proposes a holistic mutual representation enhancement (hmre) method to bridge these gaps. first, a dual activation (da) module is devised to establish information symmetry between the two branches and forms the foundation for mutual representation enhancement. subsequently, the holistic mutual enhancement is jointly constructed by the global semantic (gs) and spatial dense (sd) mutual enhancement modules. in the prediction stage for segmentation, we integrate the enhanced mutual representation into the mutual-fusion decoder to activate the homologous object regions bidirectionally. to expedite the replication of investigation in this task, we further create a corresponding benchmark flood-3i. the whole dataset is attainable at https://drive.google.com/drive/folders/ 1fmakf2sszofkjq0urumslnjdbwqspfxr. extensive experiments on two benchmarks isaid-5i and flood-3i demonstrate the superiority of our proposed method, which also sets a new state-of-the-art.",AB_0273
"although deep learning-based change detection (cd) methods achieve great success in remote sensing images, they still suffer from two main challenges. first, popular convolutional neural networks (cnns) are weak in extracting discriminated features focusing on changed regions, since most methods ignore the multifrequency components of bitemporal images. second, although existing cd methods employ the transformer structure to capture long-range dependency for global feature representation, it is difficult for them to simultaneously take into account the long-range dependency of changed objects at various scales. to address the above issues, we propose a triple cd network (tcd-net) via joint multifrequency and full-scale swin-transformer (fst). the proposed tcd-net has two main advantages. first, we propose a multifrequency channel attention (mfca) module to boost the ability of modeling the channel correlation, which can compensate for the problem of insufficient feature representation caused by only performing global average pooling (gap). furthermore, a joint multifrequency difference feature enhancement (jm-dfe) guiding block is proposed to improve the boundary quality and the position awareness of truly changed objects, which can effectively extract channel features of multifrequency information and thus improve the discriminative ability of features. second, unlike siamese-based structures, we propose an fst module as the third branch to model and aggregate the long-range dependency of multiscale changed objects, which can alleviate the missed detections of small objects and achieve more compact changed regions effectively. experiments on three public cd datasets exhibit that the proposed tcd-net achieves better cd accuracy with smaller model complexity than state-of-the-art (sota) methods. the code is publicly available at https://github.com/rscd-mz/tcd-net.",AB_0273
"the rapid progress in remote sensing technology has made it convenient for satellites to capture both multispectral (ms) and panchromatic (pan) images. ms has more spectral information, and pan has higher spatial resolution. how to exploit the complementarity between ms and pan images, and effectively combine their respective advantageous features while alleviating mode differences, has become a crucial research task. this article designs a style separation and mode unification network (ssmu-net) for ms and pan image classification from a novel and effective perspective. the network can be divided into two stages: style separation and mode unification (mu). in the style separation stage, we use wavelet decomposition and techniques similar to generative adversarial networks to preliminarily separate the information of ms and pan into different components. these components better preserve complete information from the original data and have their own advantages in style and content. then, we propose a symmetrical triplet traction module to perform style traction on different components, making style features more unique and content features more unified, achieving feature separation and purification. in the mu stage, we design an encoder-decoder model to reduce the impact of mode differences. the experimental results from multiple datasets validate the effectiveness of our proposed method. our overall accuracy (oa) improved by approximately 4% on the shanghai and beijing datasets, and it has exceeded 99.28% on the hohhot and vancouver datasets. our code is available at: https://github.com/proudpie/ssmu-net.",AB_0273
"the rapid development of the marine aquaculture industry has brought about a series of environmental problems that need to be monitored and planned. there is abundant marine aquaculture data obtained through synthetic aperture radar (sar) remote sensing over a long period. with a large amount of unlabeled data, self-supervised learning can describe the feature representation of targets. however, when self-supervised learning meets big data, it often leads to semantic information loss, such as interclass misjudgment and intraclass discontinuity. to address this issue, this article proposes a self-supervised transformer with feature fusion (stff) for the semantic segmentation of sar images in marine aquaculture monitoring. stff consists mainly of a self-attention encoding module with a hybrid loss function and a semantic segmentation decoding module with feature fusion. for encoding, the transformer is pretrained via self-supervised learning based on a hybrid loss function to enrich local, global, and edge information for dealing with semantic information loss and data imbalance in whole-scene sar images. for decoding, the features extracted from transformer blocks are fused to enhance semantic characteristics, improve the intraclass continuity of segmentation, and reduce the occurrence of interclass misjudgment. the superiority of the proposed method to state-of-the-art algorithms is demonstrated via experimentation on gaofen-3 and radarsat-2 sar datasets. the code has been available at https://github.com/fjc1575/marineaquaculture/tree/main/stff-code for the sake of reproducibility.",AB_0273
"cross-modal hashing aims to retrieve similar images from large-scale earth observation (eo) data archives, which typically contain multiple satellite sources of remote sensing (rs) images. however, existing cross-modal hashing methods primarily focus on dual-source rs images and often face two main limitations when retrieving multisource rs images. first, these methods exhibit significant redundancy as they require handling all possible dual-source combinations in multisource rs images. second, they often rely on pairwise or triplet image sources to construct objective functions, which are not significantly effective in reducing the discrepancies among multiple rs image sources. to address these limitations, we propose a novel consistency center-based deep cross-modal hashing method for multisource rs image retrieval called c(2)hash. our c(2)hash employs a multibranch hashing network to directly encode multisource rs images into unified hash codes, thereby offering higher processing efficiency. furthermore, c(2)hash introduces consistency centers to construct a novel objective function. the consistency center represents the shared semantic features among similar multisource rs images and is generated by a label hashing network. the objective function encourages similar multisource rs images to approach the same consistency center to align all image sources in a unified hamming space. our method can effectively reduce the discrepancies across multiple image sources and generate unified hash codes. to evaluate its effectiveness, we construct a new multisource rs image dataset called msrsi, comprising five different types of image sources. we conduct comprehensive experiments to demonstrate the superior performance of our method on the msrsi dataset (https://github.com/sunyuxi/c2hash).",AB_0273
"in real-world scenarios, the information quality provided by rgb and thermal (rgb-t) sensors often varies across samples. this variation will negatively impact the performance of semantic segmentation models in utilizing complementary information from rgb-t modalities, resulting in a decrease in accuracy and fusion credibility. dynamically estimating the uncertainty of each modality for different samples could help the model perceive such information quality variation and then provide guidance for a reliable fusion. with this in mind, we propose a novel uncertainty-guided trustworthy fusion network (utfnet) for rgb-t semantic segmentation. specifically, we design an uncertainty estimation and evidential fusion (ueef) module to quantify the uncertainty of each modality and then utilize the uncertainty to guide the information fusion. in the ueef module, we introduce the dirichlet distribution to model the distribution of the predicted probabilities, parameterized with evidence from each modality and then integrate them with the dempster-shafer theory (dst). moreover, illumination evidence gathering (ieg) and multiscale evidence gathering (meg) modules by considering illumination and target multiscale information, respectively, are designed to gather more reliable evidence. in the ieg module, we calculate the illumination probability and model it as the illumination evidence. the meg module can collect evidence for each modality across multiple scales. both qualitative and quantitative results demonstrate the effectiveness of our proposed model in accuracy, robustness, and trustworthiness. the code will be accessible at https://github.com/kustteamwqw/utfnet.",AB_0273
"for hyperspectral cross-domain recognition applications, the unseen target domain (td) is inevitable, and the model can only be trained on the source domain (sd) but directly applied to unknown domains. a major challenge of this domain generalization (dg) problem comes from the domain shift caused by differences in environments, devices, etc. one feasible strategy is performing domain expansion with latent variables and learning domain-invariant representation. inspired by this framework, the study proposes a generation network for extension, which consists of a symmetric encoder-decoder to implicitly build local joint feature under style randomization. moreover, supervised contrastive learning is employed to avoid duplicate augmentation. besides, considering the trade-off between domain-specific and domain-invariant, an adversarial penalty term is formed by inter-class and intra-class contrastive regularization in the discriminator. multiple evaluations on three public hsi datasets indicate that the proposed method outperforms state-of-the-art (sota) approaches. the codes is available from the website: https://github.com/huowumo/ieee_hsic_llurnet.",AB_0273
