AB,NO
"classical diffeomorphic image registration methods, while being accurate, face the challenges of high computational costs. deep learning based approaches provide a fast alternative to address these issues; however, most existing deep solutions either lose the good property of diffeomorphism or have limited flexibility to capture large deformations, under the assumption that deformations are driven by stationary velocity fields (svfs). also, the adopted squaring and scaling technique for integrating svfs is time- and memory-consuming, hindering deep methods from handling large image volumes. in this paper, we present an unsupervised diffeomorphic image registration framework, which uses deep residual networks (resnets) as numerical approximations of the underlying continuous diffeomorphic setting governed by ordinary differential equations, which is parameterized by either svfs or time-varying (non-stationary) velocity fields. this flexible parameterization in our residual registration network (r2net) not only provides the model's ability to capture large deformation but also reduces the time and memory cost when integrating velocity fields for deformation generation. also, we introduce a lipschitz continuity constraint into the resnet block to help achieve diffeomorphic deformations. to enhance the ability of our model for handling images with large volume sizes, we employ a hierarchical extension with a multi-phase learning strategy to solve the image registration task in a coarse-to-fine fashion. we demonstrate our models on four 3d image registration tasks with a wide range of anatomies, including brain mris, cine cardiac mris, and lung ct scans. compared to classical methods syn and diffeomorphic voxelmorph, our models achieve comparable or better registration accuracy with much smoother deformations. our source code is available online at https://github.com/ankitajoshi15/r2net.",AB_0233
"automated retinal blood vessel segmentation in fundus images provides important evidence to ophthalmologists in coping with prevalent ocular diseases in an efficient and non-invasive way. however, segmenting blood vessels in fundus images is a challenging task, due to the high variety in scale and appearance of blood vessels and the high similarity in visual features between the lesions and retinal vascular. inspired by the way that the visual cortex adaptively responds to the type of stimulus, we propose a stimulus-guided adaptive transformer network (sgat-net) for accurate retinal blood vessel segmentation. it entails a stimulus-guided adaptive module (sga-module) that can extract local-global compound features based on inductive bias and self-attention mechanism. alongside a light-weight residual encoder (resencoder) structure capturing the relevant details of appearance, a stimulus-guided adaptive pooling transformer (sgap-former) is introduced to reweight the maximum and average pooling to enrich the contextual embedding representation while suppressing the redundant information. moreover, a stimulus-guided adaptive feature fusion (sgaff) module is designed to adaptively emphasize the local details and global context and fuse them in the latent space to adjust the receptive field (rf) based on the task. the evaluation is implemented on the largest fundus image dataset (fives) and three popular retinal image datasets (drive, stare, chasedb1). experimental results show that the proposed method achieves a competitive performance over the other existing method, with a clear advantage in avoiding errors that commonly happen in areas with highly similar visual features. the sourcecode is publicly available at: https://github.com/gins-07/sgat.",AB_0233
"rapid developments in automatic driving technology have given rise to new experiences for passengers. safety is a main priority in automatic driving. a strong familiarity with road-surface conditions during the day and night is essential to ensuring driving safety. existing models used for recognizing road-surface conditions lack the required robustness and generalization abilities. most studies only validated the performance of these models on daylight images. to address this problem, we propose a novel multi-supervised bidirectional fusion network (mbfn) model to detect weather-induced road-surface conditions on the path of automatic vehicles at both daytime and nighttime. we employed convnext to extract the basic features, which were further processed using a new bidirectional fusion module to create a fused feature. then, the basic and fused features were concatenated to generate a refined feature with greater discriminative and generalization abilities. finally, we designed a multi-supervised loss function to train the mbfn model based on the extracted features. experiments were conducted using two public datasets. the results clearly demonstrated that the mbfn model could classify diverse road surface conditions, such as dry, wet, and snowy conditions, with a satisfactory accuracy and outperform state-of-the-art baseline models. notably, the proposed model has multiple variants that could also achieve competitive performances under different road conditions. the code for the mbfn model is shared at https://zenodo. org/badge/latestdoi/607014079.",AB_0233
"the damage caused by agricultural pests and diseases has brought huge losses to the economy. rapid recognition and timely treatment can minimize economic losses. most of the existing image databases are produced in laboratories, where the shooting costs are expensive, and the background of these images are very different from the real farmland environment. moreover, although the existing recognition systems can locate entities, they cannot provide discriminative evidence which is semantically interpretable, which makes it difficult for them to distinguish entities with very similar appearances. fortunately, there are text descriptions in professional agricultural control documents that can clearly distinguish similar entities. in this paper, a textual-visual database for agricultural pests and diseases named apd-229 is constructed. the goal of apd-229 is to learn prior knowledge that can distinguish similar entities from the control documents, and to guide the image recognition system to complete the task of fine-grained classification. the database contains two sub databases: pest set and disease set. a total of 121,213 images and 8,209 text descriptions belong to 229 categories. furthermore, extensive experiments were carried out on apd229, results show that in the single-modal image classification task, the accuracy of pest database is 75.15% and the accuracy of disease database is 61.23%. while in the multi modal image classification task, the accuracy is 78.74% and 71.67% respectively. compare with the single-model experiment, the accuracy of multi-model is improved by 4.78% and 17% respectively. apd-229 is publicly available at https://github.com/sdust-mmml/ apd-229.",AB_0233
"self-supervised representation learning (ssl) has achieved remarkable success in its application to natural images while falling behind in performance when applied to whole-slide pathological images (wsis). this is because the inherent characteristics of wsis in terms of gigapixel resolution and multiple objects in training patches are fundamentally different from natural images. directly transferring the state-of-the-art (sota) ssl methods designed for natural images to wsis will inevitably compromise their performance. we present a novel scheme sgcl: spatial guided contrastive learning, to fully explore the inherent properties of wsis, leveraging the spatial proximity and multi-object priors for stable self-supervision. beyond the self-invariance of instance discrimination, we expand and propagate the spatial proximity for the intra-invariance from the same wsi and inter-invariance from different wsis, as well as propose the spatial-guided multi-cropping for inner invariance within patches. to adaptively explore such spatial information without supervision, we propose a new loss function and conduct a theoretical analysis to validate it. this novel scheme of sgcl is able to achieve additional improvements over the sota pre-training methods on diverse downstream tasks across multiple datasets. extensive ablation studies have been carried out and visualizations of these results have been presented to aid understanding of the proposed sgcl scheme. as open science, all codes and pre-trained models are available at https://github.com/hhhedo/sgcl.",AB_0233
"prostaglandin e2 (pge2) is implicated in intestinal inflammation and intestinal blood flow regulation with a paradoxical effect on the pathogenesis of necrotizing enterocolitis (nec), which is not yet well understood. in the current study, we found that pge2, ep4, and cox-2 varied at different distances from the most damaged area in the terminal ileum obtained from human infants with nec. pge2 administration alleviated the phenotype of experimental nec and the intestinal microvascular features in experimental nec, but this phenomenon was inhibited by enos depletion, suggesting that pge2 promoted intestinal microcirculatory perfusion through enos. furthermore, pge2 administration increased the vegf content in mimecs under tnf alpha stress and promoted mimec proliferation. this response to pge2 was involved in enos phosphorylation and nitric oxide (no) production and was blocked by the ep4 antagonist in vitro, suggesting that targeting the pge2-ep4-enos axis might be a potential clinical and therapeutic strategy for nec treatment. the study is reported in accordance with arrive guidelines (https://arriveguidelines.org).",AB_0233
"using fiber optic raman spectroscopy and deep neural networks, we develop an intelligent system which will be used to assist surgeons accurately and efficiently to identify oral squamous cell carcinomas (oscc). this system is able to classify 6 types of oral tissues. to achieve this goal, a novel classification framework called deep multifeature fusion residual network (dmff-resnet) is proposed. this model is based on 16,200 raman spectral data, obtained from the normal oral tissues and the oscc of 90 patients through the surgical resection. firstly, the 1dimensional restnet50 is taken as its backbone network. then, the output spectral features of last three blocks are extracted from backbone network for feature fusion, which is expected to learn more spatial representations and have more discriminative power. lastly, the derived spectral features are sent into a fully-connected neural network for performing the multiclassification task. experimental results show that the proposed model achieves a competitive classification performance compared with state-of-the-art classifiers, and its accuracy, precision, and sensitivity reach 93.28%, 93.53%, and 93.13%, respectively. further, the proposed framework is deployed on an edge computing device to form a prototype intelligent system for oscc detection. to validate this system, we perform an offline test experiment in another 20 patients which demonstrates the developed intelligent system can successfully discriminate oscc and normal oral tissues, with accuracy, precision, and recall of 92.78%, 92.33%, and 92.57%, respectively. the code was available at https://github.com/isclab-bistu/retinan et-oscc.",AB_0233
"retinal vessel segmentation is a rapid method for the diagnosis of ocular diseases. by applying deep learning -based techniques to retinal images, more structural information about retinal vessels can be extracted to accurately assess the extent and classification of ocular diseases. however, current segmentation networks typically consist of a single network, making them vulnerable to noise, decreased image quality, and other interfering factors, resulting in erroneous segmentation outcomes. additionally, the traditional skip connection mechanism introduces noise from the encoder features into the decoder, which reduces the refinement of the final segmentation result. a three-stage fundus vessel segmentation model called ewsnet is proposed to address these issues. the ewsnet utilizes two different models to extract and reconstruct coarse and fine blood vessels, respectively. the reconstructed results are fed into the refinement network to rebuild the edge portion of the retinal vessels, achieving higher segmentation performance. within the framework of ewsnet, a wavelet -transformation-based sampling module is used to effectively suppress high-frequency noise in the features while using low-frequency features to reconstruct vascular information. besides, a new edge loss function (e -bce loss) is designed to encourage more precise predictions at the segmentation edges. experimental results on chase_db1, hrf, stare, and a newly collected ultra-wide-angle fundus dataset (uwf) demonstrate that ewsnet has more robust segmentation performance in the microvascular region compared to the current mainstream models. the code is available at: https://github.com/xuecheng990531/ewsnet.",AB_0233
"background. automatic cell type identification has been an urgent task for the rapid development of single-cell rna-seq techniques. generally, the current approach for cell type identification is to generate cell clusters by unsupervised clustering and later assign labels to each cell cluster with manual annotation. methods. here, we introduce lider (cell embedding based deep neural network classifier), a deep supervised learning method that combines cell embedding and deep neural network classifier for automatic cell type identification. based on a stacked denoising autoencoder with a tailored and reconstructed loss function, lider identifies cell embedding and predicts cell types with a deep neural network classifier. lider was developed upon a stacked denoising autoencoder to learn encoder-decoder structures for identifying cell embedding. results. lider accurately identifies cell types by using stacked denoising autoencoder. benchmarking against state-of-the-art methods across eight types of single-cell data, lider achieves comparable or even superior enhancement performance. moreover, lider suggests comparable robust to batch effects. our results show a potential in deep supervised learning for automatic cell type identification of single-cell rna-seq data. the lider codes are available at https://github.com/shimglab/lider.",AB_0233
"motivation: understanding drug-response differences in cancer treatments is one of the most challenging aspects of personalized medicine. recently, graph neural networks (gnns) have become state-of-the-art methods in many graph representation learning scenarios in bioinformatics. however, building an optimal handcrafted gnn model for a particular drug sensitivity dataset requires manual design and fine-tuning of the hyperparameters for the gnn model, which is time-consuming and requires expert knowledge. results: in this work, we propose autocdrp, a novel framework for automated cancer drug-response predictor using gnns. our approach leverages surrogate modeling to efficiently search for the most effective gnn architecture. autocdrp uses a surrogate model to predict the performance of gnn architectures sampled from a search space, allowing it to select the optimal architecture based on evaluation performance. hence, autocdrp can efficiently identify the optimal gnn architecture by exploring the performance of all gnn architectures in the search space. through comprehensive experiments on two benchmark datasets, we demonstrate that the gnn architecture generated by autocdrp surpasses state-of-the-art designs. notably, the optimal gnn architecture identified by autocdrp consistently outperforms the best baseline architecture from the first epoch, providing further evidence of its effectiveness. availability and implementation: https://github.com/beobm/autocdrp.",AB_0233
