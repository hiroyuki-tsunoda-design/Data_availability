AB,NO
"motivation: metabolic stability plays a crucial role in the early stages of drug discovery and development. accurately modeling and predicting molecular metabolic stability has great potential for the efficient screening of drug candidates as well as the optimization of lead compounds. considering wet-lab experiment is time-consuming, laborious, and expensive, in silico prediction of metabolic stability is an alternative choice. however, few computational methods have been developed to address this task. in addition, it remains a significant challenge to explain key functional groups determining metabolic stability. results: to address these issues, we develop a novel cross-modality graph contrastive learning model named cmms-gcl for predicting themetabolic stability of drug candidates. in our framework, we design deep learning methods to extract features for molecules from two modality data, i.e. smiles sequence and molecule graph. in particular, for the sequence data, we design a multihead attention bigru-based encoder to preserve the context of symbols to learn sequence representations of molecules. for the graph data, we propose a graph contrastive learning-based encoder to learn structure representations by effectively capturing the consistencies between local and global structures. we further exploit fully connected neural networks to combine the sequence and structure representations for model training. extensive experimental results on two datasets demonstrate that our cmms-gcl consistently outperforms seven state-of-the-art methods. furthermore, a collection of case studies on sequence data and statistical analyses of the graph structure module strengthens the validation of the interpretability of crucial functional groups recognized by cmms-gcl. overall, cmms-gcl can serve as an effective and interpretable tool for predicting metabolic stability, identifying critical functional groups, and thus facilitating the drug discovery process and lead compound optimization. availability and implementation: the code and data underlying this article are freely available at https://github.com/dubingxue/cmms-gcl.",AB_0232
"motivation: circrnas play a critical regulatory role in physiological processes, and the abnormal expression of circrnas can mediate the processes of diseases. therefore, exploring circrnas-disease associations is gradually becoming an important area of research. due to the high cost of validating circrna-disease associations using traditional wet-lab experiments, novel computational methods based on machine learning are gaining more and more attention in this field. however, current computational methods suffer to insufficient consideration of latent features in circrna-disease interactions. results: in this study, a multilayer attention neural graph-based collaborative filtering (mlngcf) is proposed. mlngcf first enhances multiple biological information with autoencoder as the initial features of circrnas and diseases. then, by constructing a central network of different diseases and circrnas, a multilayer cooperative attention-based message propagation is performed on the central network to obtain the high-order features of circrnas and diseases. a neural network-based collaborative filtering is constructed to predict the unknown circrna-disease associations and update the model parameters. experiments on the benchmark datasets demonstrate that mlngcf outperforms state-of-the-art methods, and the prediction results are supported by the literature in the case studies. availability and implementation: the source codes and benchmark datasets of mlngcf are available at https://github.com/abard0/ mlngcf.",AB_0232
"motivation: protein complexes are groups of polypeptide chains linked by non-covalent protein-protein interactions, which play important roles in biological systems and perform numerous functions, including dna transcription, mrna translation, and signal transduction. in the past decade, a number of computational methods have been developed to identify protein complexes from protein interaction networks by mining dense subnetworks or subgraphs. results: in this article, different from the existing works, we propose a novel approach for this task based on generative adversarial networks, which is called pcgan, meaning identifying protein complexes by gan. with the help of some real complexes as training samples, our method can learn a model to generate new complexes from a protein interaction network. to effectively support model training and testing, we construct two more comprehensive and reliable protein interaction networks and a larger gold standard complex set by merging existing ones of the same organism (including human and yeast). extensive comparison studies indicate that our method is superior to existing protein complex identification methods in terms of various performance metrics. furthermore, functional enrichment analysis shows that the identified complexes are of high biological significance, which indicates that these generated protein complexes are very possibly real complexes. availability and implementation: https://github.com/yul-pan/pcgan.",AB_0232
"accurate change detection of built-up areas (bas) fosters a comprehensive understanding of urban development. the post-classification comparison (pcc) is a widely-used change detection method by classification and temporal comparison. for classification, image-level labeling is an efficient alternative to pixel-level one for pixelwise weakly supervised segmentation, which frequently applies pixel-level pseudo labels generated from class activation map (cam) to train semantic segmentation networks. cam can be obtained from classification networks trained with image-level labels and can indicate the spatial location of objects. the existing studies are subject to the following issues: 1) they only rely on the single-scale and low-resolution cam, but ignore the multi-scale property of bas; 2) pixel-level pseudo labels usually contain noises (e.g., omissions and false alarms); 3) the temporal correlation between multi-temporal images is less considered in pcc. to address these limitations, this paper proposed a multi-scale weakly supervised learning method, which utilized a large number of single-temporal high-resolution images and image-level labels to detect ba changes. this method consisted of three modules: 1) multi-scale cam for ba pseudo label generation; 2) adaptive online noise correction for ba detection; and 3) generation of reliable pseudo labels for ba change detection. based on zy-3 images (2.5 m), we constructed the first multi-view datasets for both ba detection and change detection. each zy-3 image includes a multi-spectral image with red, green, blue, and near-infrared bands and a multi-view image with nadir-, forward, and backward-views. the ba detection dataset contained 86,166 image-level samples (256 x 256 pixels for each sample), covering 48 major cities in china, while the ba change detection dataset consisted of zy-3 bitemporal images at rapidly urbanizing areas (i.e., beijing and shanghai). experiments showed that the proposed method can detect ba changes and suppress pseudo changes effectively, yielding 88.2% f1-score in ba detection and 79.3% for shanghai and 78.5% for beijing in change detection. further analysis demonstrated the proposed method to be advantageous in the following two fronts: 1) the image-level weak labels can achieve pixel-wise ba change detection at low cost; and 2) the multi-scale cam and temporal correlation are effective in the scenarios with limited labels. datasets and codes will be accessed at https://github.com/lauraset/msws.",AB_0232
"we propose a new strategy to bridge point cloud denoising and surface reconstruction by alternately updating the denoised point clouds and the reconstructed surfaces. in poisson surface reconstruction, the implicit function is generated by a set of smooth basis functions centered at the octnodes. when the octree depth is properly selected, the reconstructed surface is a good smooth approximation of the noisy point set. our method projects the noisy points onto the surface and alternately reconstructs and projects the point set. we use the iterative poisson surface reconstruction (ipsr) to support unoriented surface reconstruction. our method iteratively performs ipsr and acts as an outer loop of ipsr. considering that the octree depth significantly affects the reconstruction results, we propose an adaptive depth selection strategy to ensure an appropriate depth choice. to manage the oversmoothing phenomenon near the sharp features, we propose a & lambda;-projection method, which means to project the noisy points onto the surface with an individual control coefficient & lambda;i for each point. the coefficients are determined through a voronoi-based feature detection method. experimental results show that our method achieves high performance in point cloud denoising and unoriented surface reconstruction within different noise scales, and exhibits well-rounded performance in various types of inputs. the source code is available at https://github.com/submanifold/alterupdate. & copy; 2023 elsevier ltd. all rights reserved.",AB_0232
"background: dietary soy protein (sp) is a potential intervention for protecting the kidneys and improving glucose and lipid metabolism. however, whether this effect is related to the percentage of sp intake remains unclear. objective: this study aims to review and analyze the results of randomized clinical trials (rcts) in patients with type 2 diabetic nephropathy (t2dn) who received diets with different percentages of sp. methods: the databases: pubmed, embase, cochrane central register of controlled trials (central), web of science, china national knowledge infrastructure (cnki), chinese biomedical literature database (cbm), wanfang, weipu (vip), and clinicaltrials.gov were searched until february 2023, for rcts on t2dn and sp. results: a total of six studies comprising 116 participants were included. the interventions were classified as 0% sp, 35% sp, and 100% sp. to improve serum creatinine (scr), blood urea nitrogen (bun), 24-h urine total protein (24hutp), and glomerular filtration rate (gfr), a 35% sp diet was the most effective, compared to a 0% sp diet, which showed a mean difference of 154.00 (95% confidence interval: 266.69, 41.31) for 24hutp. although it had significant benefits for 24hutp, great heterogeneity was observed. to improve the glycolipid metabolism-related markers such as cholesterol (cho), high-density lipoprotein cholesterol (hdl-c), low-density lipoprotein cholesterol (ldl-c), fasting blood glucose (fpg), and weight, the 35% sp diet demonstrated superior efficacy compared to the 0% sp diet. specifically, the mean difference for cho was 0.55 (95% confidence interval: 1.08, 0.03), and for ldl-c, it was 17.71 (95% confidence interval: 39.67, 4.24). the other indicators were not statistically significant. most studies had concerns regarding the risk of bias. conclusion: the findings of this study demonstrate that both 35% and 100% sp diets are more effective than a diet with no sp in improving renal function and glucolipid metabolism in patients with t2dn. as a result, a diet incorporating 35% sp may be the optimal choice for individuals with t2dn. systematic review registration: https://www.crd.york.ac.uk/prospero/display_ record.php?recordid=352638, identifier crd42022352638.",AB_0232
"generative artificial intelligence (ai), including large language models (llms), is poised to transform scientific research, enabling researchers to elevate their research productivity. this article presents a how-to guide for employing llms in academic settings, focusing on their unique strengths, constraints and implications through the lens of philosophy of science and epistemology. using chatgpt as a case study, i identify and elaborate on three attributes contributing to its effectiveness-intelligence, versatility and collaboration-accompanied by tips on crafting effective prompts, practical use cases and a living resource online (https://osf.io/8vpwu/). next, i evaluate the limitations of generative ai and its implications for ethical use, equality and education. regarding ethical and responsible use, i argue from technical and epistemic standpoints that there is no need to restrict the scope or nature of ai assistance, provided that its use is transparently disclosed. a pressing challenge, however, lies in detecting fake research, which can be mitigated by embracing open science practices, such as transparent peer review and sharing data, code and materials. addressing equality, i contend that while generative ai may promote equality for some, it may simultaneously exacerbate disparities for others-an issue with potentially significant yet unclear ramifications as it unfolds. lastly, i consider the implications for education, advocating for active engagement with llms and cultivating students' critical thinking and analytical skills. the how-to guide seeks to empower researchers with the knowledge and resources necessary to effectively harness generative ai while navigating the complex ethical dilemmas intrinsic to its application.",AB_0232
"motivation: few-shot learning that can effectively perform named entity recognition in low-resource scenarios has raised growing attention, but it has not been widely studied yet in the biomedical field. in contrast to high-resource domains, biomedical named entity recognition (bioner) often encounters limited human-labeled data in real-world scenarios, leading to poor generalization performance when training only a few labeled instances. recent approaches either leverage cross-domain high-resource data or fine-tune the pre-trained masked language model using limited labeled samples to generate new synthetic data, which is easily stuck in domain shift problems or yields low-quality synthetic data. therefore, in this article, we study a more realistic scenario, i.e. few-shot learning for bioner. results: leveraging the domain knowledge graph, we propose knowledge-guided instance generation for few-shot bioner, which generates diverse and novel entities based on similar semantic relations of neighbor nodes. in addition, by introducing question prompt, we cast bioner as question-answering task and propose prompt contrastive learning to improve the robustness of the model by measuring the mutual information between query-answer pairs. extensive experiments conducted on various few-shot settings show that the proposed framework achieves superior performance. particularly, in a low-resource scenario with only 20 samples, our approach substantially outperforms recent state-of-the-art models on four benchmark datasets, achieving an average improvement of up to 7.1% f1. availability and implementation: our source code and data are available at https://github.com/cpmss521/kgpc.",AB_0232
"3d scene stylization aims to generate impressive stylized images from arbitrary novel views based on the stylistic reference. existing image-driven 3d scene stylization methods require a specific style reference to be given, and lack the ability to produce diverse stylization results by combining style information from different aspects. in this paper, we propose a text-driven 3d scene stylization method based on semantic contrast learning, which takes neural radiance fields (nerf) as the 3d scene representation and generates diverse 3d stylized scenes by leveraging the semantic capabilities of the contrastive language-image pre-training (clip) model. for comprehensively exploiting the semantic knowledge to generate finely stylized results, we design a clip-based semantic contrast estimation loss, which can avoid the global stylistic inconsistency caused by the nerf ray sampling method and avoid the tendency to stylize neutral descriptions due to the semantic averaging of the clip space. in addition, to reduce the memory burden arising from nerf ray sampling, we propose a novel ray sampling method with gradient accumulation to optimize the nerf rendering process. the experimental results indicate that our method generates high-quality and plausible results with cross-view consistency. moreover, our method enables the creation of new styles that match the target text by combining multiple domains. the code will be available at http://cic.tju.edu.cn/faculty/likun/projects/tsnerf.& copy; 2023 elsevier ltd. all rights reserved.",AB_0232
"nuclei segmentation is a crucial task for whole slide image analysis in digital pathology. generally, the segmentation performance of fully-supervised learning heavily depends on the amount and quality of the annotated data. however, it is time-consuming and expensive for professional pathologists to provide accurate pixel-level ground truth, while it is much easier to get coarse labels such as point annotations. in this paper, we propose a weakly-supervised learning method for nuclei segmentation that only requires point annotations for training. first, coarse pixel-level labels are derived from the point annotations based on the voronoi diagram and the k-means clustering method to avoid overfitting. second, a co-training strategy with an exponential moving average method is designed to refine the incomplete supervision of the coarse labels. third, a self supervised visual representation learning method is tailored for nuclei segmentation of pathology images that transforms the hematoxylin component images into the h&e stained images to gain better understanding of the relationship between the nuclei and cytoplasm. we comprehensively evaluate the proposed method using two public datasets. both visual and quantitative results demonstrate the superiority of our method to the state-of-the-art methods, and its competitive performance compared to the fully-supervised methods. codes are available at https://github.com/hust-linyi/sc-net.",AB_0232
