AB,NO
"remote sensing object labels require high specialization, resulting in a limited number of labeled samples. without large labeled samples to support training, general remote sensing object recognition models have limited accuracy. addressing this issue, this article proposes a weakly correlated distillation learning framework for remote sensing object recognition with a small number of samples. benefitting from large-scale natural image datasets, many recognition models achieve superior feature extraction capabilities. thus, we use them as backbones to build teacher models, and then fine-tune the teacher models with a small-scale remote sensing dataset. however, due to the limited number of remote sensing samples, the teacher models may produce noisy features that reduce the performance of the student model. therefore, we propose a weakly correlated distillation method that selects the weakly correlated features from teacher models to distill the student. since the weakly correlated features contain different noise distributions that can be mutually suppressed, thereby improving the performance of the student. extensive experiments on three widely-used datasets of dota, hrrsd, and nwpu vhr-10 demonstrate the superior performance of our method compared with the state of the arts (sota). code is available at: https://github.com/wdzhao123/wcd.",AB_0267
"most existing adversarial attack methods against detectors involve adding adversarial perturbations to benign images to synthesize adversarial examples. however, directly applying these methods, originally designed for natural image detectors, to optical aerial image detectors can lead to perturbations that appear unnatural and suspicious to human eyes, owing to intrinsic dissimilarities between these two types of images. inspired by the fact that the captured optical aerial images are heavily affected by weather conditions, this article proposes a novel method for conducting adversarial attacks against optical aerial detectors by leveraging natural weather-style perturbations. compared to existing methods, our scheme produces more natural and stealthy adversarial examples. to enhance the practicality of the proposed method in real-world scenarios, we implement the attacks in black-box settings where only the model's predictions are accessible. specifically, we formulate the generation of adversarial weather perturbations in black-box as an optimization problem and effectively solve it using the differential evolution (de) algorithm. through extensive experiments, we verify the effectiveness of our method and investigate the transferability of generated adversarial examples across different models. in light of the significant generalization and effectiveness of our method, we generate and release the first dataset with adversarial weather-style perturbations based on the dota dataset, which we abbreviate as dota-w. this dataset serves as a valuable resource for evaluating and improving the robustness of optical aerial detectors. the code and dataset have been released at https://github.com/tang-agui/attads-awp.",AB_0267
"efficiently and accurately separating infrared (ir) small targets from complex backgrounds presents a significant challenge. numerous studies in the literature have proposed various feature fusion modules designed specifically to enhance the extraction of ir small target features. while these designs offer some incremental improvement to the accuracy of ir small target detection, they come at a steep cost of significantly increasing network parameters and floating-point operations per second (flops). striving for a balance between computational efficiency and model accuracy, we decided to forgo these complex feature fusion modules. instead, we developed a new lightweight encoding and decoding structure known as the lightweight ir small target segmentation network (lw-irstnet). this structure integrates regular convolutions, depthwise separable convolutions, atrous convolutions, and asymmetric convolutions modules. in addition, we devised postprocessing modules, including an eight-neighborhood clustering algorithm and an online target feature adjustment strategy. experimental results indicate that: 1) the segmentation accuracy metrics of lw-irstnet match the best results of 14 state-of-the-art comparative baselines; 2) the parameters and flops of lw-irstnet at only 0.16m and 303m, respectively, are significantly smaller in comparison to these baselines; and 3) the postprocessing modules enhance both user-friendliness and the robustness of algorithm deployment. moreover, lw-irstnet has been successfully implemented on both embedded platforms and websites, expanding its range of applications. utilizing the open neural network exchange (onnx) framework, neural network processing unit (npu) acceleration, and cpu multithreaded resource allocation, we have been able to achieve high-performance inference capabilities, as well as online dynamic threshold adjustment with the lw-irstnet. the source codes for this project can be accessed at https://github.com/kourenke/lw-irstnet.",AB_0267
"as a de facto solution, the vanilla vision transformers (vits) are encouraged to model long-range dependencies between arbitrary image patches while the global attended receptive field leads to quadratic computational cost. another branch of vision transformers exploits local attention inspired by cnns, which only models the interactions between patches in small neighborhoods. although such a solution reduces the computational cost, it naturally suffers from small attended receptive fields, which may limit the performance. in this work, we explore effective vision transformers to pursue a preferable trade-off between the computational complexity and size of the attended receptive field. by analyzing the patch interaction of global attention in vits, we observe two key properties in the shallow layers, namely locality and sparsity, indicating the redundancy of global dependency modeling in shallow layers of vits. accordingly, we propose multi-scale dilated attention (msda) to model local and sparse patch interaction within the sliding window. with a pyramid architecture, we construct a multi-scale dilated transformer (dilateformer) by stacking msda blocks at low-level stages and global multi-head self-attention blocks at high-level stages. our experiment results show that our dilateformer achieves state-of-the-art performance on various vision tasks. on imagenet-1 k classification task, dilateformer achieves comparable performance with 70% fewer flops compared with existing state-of-the-art models. our dilateformer-base achieves 85.6% top-1 accuracy on imagenet-1 k classification task, 53.5% box map/46.1% mask map on coco object detection/instance segmentation task and 51.1% ms miou on ade20 k semantic segmentation task. the code is available at https://isee-ai.cn/(similar to)jiaojiayu/dilteformer.html.",AB_0267
"deep learning has significantly advanced the field of hyperspectral remote sensing image classification. among various methods, the classification method based on spectral-spatial features for hyperspectral classification has attracted wide attention because of its exceptional classification performance. however, such methods encounter challenges in handling input sample and feature extraction. regarding the input sample, current hyperspectral image (hsi) classification methods based on spectral-spatial features treat each pixel of the sample equally, resulting in inadequate attention to valuable pixels within 3-d samples. regarding feature extraction, the classification methods struggle to effectively extract both local and global information from hsis. aiming at solving the above problems, we propose the local-global cross fusion network with gaussian-initialized positional prompting (lggnet). lggnet is designed with an end-to-end architecture, primarily comprising the gaussian-initialized learnable positional prompting and the local-global cross fusion network. the gaussian-initialized learnable positional prompting introduces prompting technique into hsi classification, utilizing trainable parameters with prior information to learn the spatial importance of different pixels within a sample for the first time. the local-global cross fusion network combines operations such as 3-d convolutional neural network (cnn) feature extraction, transformer feature extraction, and feature fusion, efficiently integrating local and global features. extensive experiments showcase that lggnet achieves state-of-the-art performance with limited training samples on four benchmark datasets, all within a lightweight framework. the relevant code is available at https://github.com/ibelieveican2018/lggnet.",AB_0267
"tracking arbitrary objects in aerial images presents formidable challenges to existing trackers. among these challenges, the large-scale variation and arbitrary geometry shape of visual targets are pronounced, resulting in twofold mismatch issues between the feature receptive field and the tracking target. for one, there is a mismatch between the prior receptive field center and arbitrary-shaped targets. for another, the single receptive field mismatches the significantly scale-varied targets in the aerial imagery. to handle these challenges, we propose to achieve precise aerial tracking with receptive field alignment (rfa), dubbed a3track. the proposed a3track is comprised of two modules: an rfa module and a pyramid receptive field (prf) module. first of all, we transform and update the receptive field center progressively, which drives the feature sampling location onto the targets' main body, thus gradually yielding precise feature representation for arbitrary-shaped targets. we term this progressively updating process as the rfa. moreover, the prf module constructs a set of pyramid features for the target, providing a multiscale receptive field to handle the large-scale variation of tracking objects. on four benchmarks, the new tracker a3track achieves leading performance compared with existing methods and shows consistent improvements over baselines. the project is available at: https://chnleixu.github.io/a3track-web/.",AB_0267
"convolutional neural network (cnn) and transformer-based self-attention models have their advantages in extracting local information and global semantic information, and it is a trend to design a model combining stacked residual convolution blocks (srcbs) and transformer. how to efficiently integrate the two mechanisms to improve the segmentation effect of remote sensing (rs) images is an urgent problem to be solved. an efficient fusion via srcb and transformer (srcbtfusion-net) is proposed as a new semantic segmentation architecture for rs images. the srcbtfusion-net adopts an encoder-decoder structure and the transformer is embedded into srcb to form a double coding structure, then the coding features are upsampled and fused with multiscale features of srcb to form a decoding structure. first, a semantic information enhancement module (siem) is proposed to get global clues for enhancing deep semantic information. subsequently, the relationship guidance module (rgm) is incorporated to reencode the decoder's upsampled feature maps, enhancing the edge segmentation performance. second, a multipath atrous self-attention module (masm) is developed to enhance the effective selection and weighting of low-level features, effectively reducing the potential confusion introduced by the skip connections between low- and high-level features. finally, a multiscale feature aggregation module (mfam) is developed to enhance the extraction of semantic and contextual information, thus alleviating the loss of image feature information and improving the ability to identify similar categories. the proposed srcbtfusion-net's performance on the vaihingen and potsdam datasets is superior to the state-of-the-art methods. the code will be freely available at https://github.com/js257/srcbtfusion-net.",AB_0267
"the vulnerability of deep neural networks (dnns) has garnered significant attention. various advanced adversarial attack methods have been proposed. however, these methods exhibit higher attack performance on three-band natural images while struggling to handle high-dimensional attacks in terms of attack transferability and robustness. hyperspectral images (hsis), unlike natural images, possess high-dimensional and redundant spectral information. on the one hand, different classification models focus on distinct discriminative spectral bands, leading to poor transferability. on the other hand, most existing attack methods are implemented at the pixel level, making them less resilient to image-processing-based defenses. in this article, we address the improvement of transferability and robustness in high-dimensional attacks and introduce a universal object-level adversarial attack method in hsi classification. we found that perturbations with higher similarity in a local region can decrease the sensitivity of adversarial attacks to various discriminative spectral patterns and enhance resistance to image-processing-based defenses. consequently, we construct spatial and spectral oversegmented templates by utilizing the local smooth properties of hsis, aiming to promote similarity among perturbations within a local region. extensive experiments conducted on two real hsi datasets validate that our method enhances the attack transferability and robustness of several existing attack methods. by incorporating the object-level adversarial attack with the baseline fast gradient sign method (fgsm), momentum iterative fgsm (mi-fgsm), and variance tuning mi-fgsm (vmi-fgsm), the average transferability success rate of the proposed method has increased by 7.38% on the paviau dataset and 9.30% on the houstonu 2018 dataset than the baselines. meanwhile, the proposed method outperforms the baselines by an average of 6.19% on the paviau dataset and 10.05% on the houstonu 2018 dataset in attacking image-processing-based defense models. the code is available at https://github.com/ aaaa-cs/ss_fgsm_hyperspectraladversarialattack.",AB_0267
"data augmentation has become one of the keys to alleviating the over-fitting of models on training data and improving the generalization capabilities on testing data. most existing data augmentation methods only focus on one modality, which is incapable when facing multiple data modalities. some prior works try to interpolate with random coefficients in the latent space to generate new samples, which can generically work for any data modality. however, these works ignore the extra information conveyed by multimodality data. in fact, the extra information in one modality can provide semantic directions to generate more meaningful samples in another modality. this paper proposes cross-modal data augmentation (cmda), a simple yet effective data augmentation method to alleviate the over-fitting issue and improve the generalization performance. we evaluate cmda on unsupervised and supervised tasks of different modalities, on which cmda consistently and significantly outperforms baselines. for instance, cmda improves the unsupervised anomaly detection baseline in vision modality from the auroc 76.46%, 73.07% and 64.36% to 83.25%, 76.22% and 70.57% on three different datasets, respectively. besides, extensive experiments demonstrate that cmda is applicable to various neural network architectures. furthermore, prior methods that interpolate in the latent space need to work with downstream tasks to construct the latent space. in contrast, cmda can work with or without downstream tasks, which makes the applicability of cmda more extensive. the source code is publicly available for non-commercial or research use at https://github.com/anfeather/cmda",AB_0267
"in recent years, deep-learning-based methods have been extensively utilized in remote sensing image scene classification and have achieved remarkable performance. the wide geographical coverage and resolution differences of scene images result in significant within-class diversity and between-class similarity, hindering the further improvement of classification accuracy. attention-based methods automatically estimate the importance of local regions by learning weight assignments, which effectively enhance the feature extraction capability of the network. however, methods that solely rely on the network to automatically learn weight assignments may introduce biases in the attention calculations. by analyzing the specific contribution of local features to the key components of global semantics, we propose a collaborative dictionary learning network (cdlnet). cdlnet utilizes the collaborative representation method to decompose global features into a set of key semantic vectors to guide the attention learning process of the network. specifically, we design a semantic summarization module (ssm) that reconstructs global semantic features by optimizing a low-redundancy dictionary. next, we propose a global semantic attention module (gsam) that calculates the contribution of local features to the global feature key information based on their correlation with the reconstructed key semantic set. finally, an attention transfer loss is introduced to further enhance the attention of low-level feature maps. the experimental results on three publicly available datasets demonstrate that cdlnet can effectively improve within-class diversity and between-class similarity by optimizing the attention learning of the network, thereby achieving great promotion in comparison with state-of-the-art methods. the implementation is publicly available at https://github.com/liuofficial/cdlnet.",AB_0267
