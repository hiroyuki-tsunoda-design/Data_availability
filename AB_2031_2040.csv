AB,NO
"in this article, we introduce a new building dataset and propose a novel domain generalization method to facilitate the development of building extraction from high-resolution remote sensing images. the problem with the current building datasets involves that they lack diversity to train a practical learning model with good generalization ability, and the quality of the labels is unsatisfactory. to address these issues, we built a diverse, large-scale, and high-quality building dataset named the whu-mix building dataset, which is more practice-oriented. the whu-mix building dataset consists of a training/validation set containing 43 727 diverse images collected from all over the world, and a test set containing 8402 images from five other cities on five continents. in addition, to further improve the generalization ability of a building extraction model, we propose a domain generalization method named batch style mixing (bsm), which can be embedded as an efficient plug-and-play module in the front-end of a building extraction model, providing the model with a progressively larger data distribution to learn data-invariant knowledge. the experiments conducted in this article confirmed the potential of the whu-mix building dataset to improve the performance of a building extraction model, resulting in a 6-36% improvement in mean intersection over union (miou), compared to the other existing datasets. the adverse impact of the inaccurate labels in the other datasets can cause about 20% intersection over union decrease. the experiments also confirmed the high performance of the proposed bsm module in enhancing the generalization ability and robustness of a model, exceeding the baseline model without domain generalization by 13% and the recent domain generalization methods by 4-15% in miou. the whu-mix dataset is available at http://gpcv.whu.edu.cn/data.",AB_0204
"the fusion of red, green, blue (rgb) and thermal images has profound implications for the semantic segmentation of challenging urban scenes, such as those with poor illumination. nevertheless, existing rgb-thermal (rgb-t) fusion networks pay less attention to modality differences, i.e., rgb and thermal images are commonly fused with fixed weights. in addition, spatial context details are lost during regular extraction operations, inevitably leading to imprecise object segmentation. to improve the segmentation accuracy, a novel network named spatial feature aggregation and fusion with modality adaptation (sfaf-ma) is proposed in this article. the modality difference adaptive fusion (mdaf) module is introduced to adaptively fuse rgb and thermal images with corresponding weights generated from an attention mechanism. in addition, the spatial semantic fusion (ssf) module is designed to tap into more information by capturing multiscale perceptive fields with dilated convolutions of different rates, and aggregate shallower-level features with rich visual information and deeper-level features with strong semantics. compared with existing methods on the public mfnet dataset and pst900 dataset, the proposed network significantly improves the segmentation effectiveness. the code is available at https://github.com/hexunjie/sfaf-ma.",AB_0204
"despite the advancements in the technologies of autonomous driving, it is still challenging to study the safety of a self-driving vehicle. trajectory prediction is one core function of an autonomous vehicle. this study proposes an attention-based interaction-aware trajectory prediction (ai-tp) for traffic agents around the autonomous vehicle. with an encoder-decoder architecture, the ai-tp model uses graph attention networks (gat) to describe the interactions of traffic agents and convolutional gated recurrent units (convgru) to carry out predictions. based on the attention mechanism, the ai-tp model constructs graphs from various traffic scenes to predict trajectories of different types of traffic agents. traffic data from both the high-way (i.e., ngsim) and urban road areas (i.e., apolloscape and argoverse) are used to evaluate the performance of the ai-tp model. numerical results demonstrate that the ai-tp model requires less inference time and achieves better prediction accuracy than state-of-the-art methods. specifically, the ai-tp model improves the performance with much less inference time on the ngsim dataset, which shows the promise of predicting trajectories under various scenarios. the code of the ai-tp model will be available at https://github.com/kp-zhang/ai-tp.",AB_0204
"electromagnetic tomography (emt) is a research hotspot in electrical tomography, which has wide application prospect for multiphase flow measurement. the existing emt usually visualizes the distributions of conductivity or permeability separately. in order to realize the simultaneous imaging of different electromagnetic characteristics in the measurement area and improve the quality of the reconstructed images, a deep learning-based multiparameter emt method is proposed in this article. firstly, the information from the mutual inductance and magnetic induction intensity of the imaging area is measured. then, the landweber algorithm is used to reconstruct the initial conductivity and permeability images using the above measurements. finally, the initial images are input into the improved deeplabv3 network for image segmentation and the images of conductivity and permeability distributions with clear boundary and accurate size and position are output. the images reconstructed by the improved deeplabv3 network are compared with those from traditional methods, unet++, linknet, and pyramid attention networks (pans) through the simulation and experiment. the experimental results show that our method achieves root-mean-square error (rmse) of 0.1667, correlation coefficient (cc) of 0.6984 and structural similarity index measurement (ssim) of 0.6542 on average for permeability distribution reconstruction, and rmse of 0.1907, cc of 0.7791, and ssim of 0.7538 on average for conductivity distribution reconstruction. these results prove that the proposed method can simultaneously obtain the conductivity and permeability distributions with high-quality reconstructed images. our code is publicly available at https://github.com/tougerr/landweber-dlv3.",AB_0204
"images captured under low-illumination conditions usually suffer from severe degradations, such as fading and low contrast, drastically affecting the performance of systems relying on images under low-illumination conditions. to address such problems, this study proposes a linear contrast enhancement network (lcenet) for low-illumination image enhancement. it consists of three subnets: two encoder-decoder-based subnets for gradient map restoration and brightness enhancement, respectively, and a backbone network for adaptive brightness and contrast adjustment. in addition, a linear contrast enhancement adaptive instance normalization (lceain) module with linear contrast enhancement ability is proposed in the backbone network, which can avoid the problem of ignoring contrast enhancement when enhancing image brightness. considerable evaluations on both synthetic and real low-illumination images show that the proposed method performs favorably against other existing similar methods. moreover, our method can handle complex low-illuminance conditions and has good generalization for low-illuminance scenes with backlighting, night scenes with light sources, as well as underwater scenes with low illuminance. code: https://github.com/zhouzhaorun/lcenet.",AB_0204
"the fusion of multispectral (ms) and panchromatic (pan) images is of great significance for the construction of high-resolution remote sensing images. because of differences in sensors, no single ms or pan image can express the complete information of a scene. therefore, it is a key issue to fuse ms images containing rich spectral content and pan images with spatial information to construct a high-resolution ms image. in this work, an adaptive shuffle attention (asa) module and an optimized unet++ are combined in a fusion-unet++ (f-unet++) framework for the problem of ms and pan image fusion. this asa module can focus on important information in the mixed domain and adjust the dimensions of tensors. f-unet++ includes a multiscale feature extraction module, multiscale feature fusion module, and image reconstruction module. the multiscale feature extraction module obtains spectral and spatial information, the multiscale feature fusion module fuses spectral and spatial information, and a composite multi-input image reconstruction module (cmi-unet++) reconstructs the final image. by combining the asa attention module, the loss of feature information can be reduced to enhance the fidelity of the spectral and spatial information of the fused image. experiments show that f-unet++ is qualitatively and quantitatively superior to current image fusion methods. (the code is available at https://github.com/josephing/f-unet).",AB_0204
"lidar localization is of great importance to autonomous vehicles and robotics. absolute pose regression, directly estimating the mapping from a scene to a 6-dof pose, has achieved impressive results in learning-based localization. different from traditional map-based methods, it does not need a pre-built 3d map during inference. however, current regression networks typically suffer from scene ambiguities, especially in challenging traffic environments, leading to large wrong predictions (e.g., outliers) and limited applications. to address this problem, a novel lidar localization framework with spatio-temporal constraints is proposed, termed stcloc, to reduce scene ambiguities and achieve more accurate localization. first, we propose to regularize regression in the spatial dimension with a novel classification task to reduce outliers. specifically, the classification task categorizes the point cloud in terms of position and orientation and then couples it with the regression task to conduct multi-task learning. second, to learn discriminative features to reduce scene ambiguities, we propose using attention-based feature aggregation to capture the correlation in lidar sequences. we conduct extensive experiments on two benchmark datasets, where the localization takes 97ms on each dataset. results show that our model outperforms state-of-the-art methods by 43.33%/36.76% (position/orientation) on the oxford radar robotcar dataset, verifying the effectiveness of our method. the source code is available on the project website at https://github.com/psyz1234/stcloc.",AB_0204
"defect detection is a task to locate and classify the possible defects in an image. however, unlike common object detection tasks, defect detection often needs to deal with images with relatively complex backgrounds, for example, in industrial product quality inspection scenario. the complex background can greatly interfere with the feature of the target objects in the multiscale feature fusion process and therefore puts great challenge on the defect detector. in this work, a channel-space adaptive enhancement feature pyramid network (ca-fpn) is proposed to eliminate this interference from the complex background. by extracting the inner relationship of different scale features, ca-fpn realizes adaptive fusion of multiscale features to enhance the semantic information of the defect while avoiding background interference as much as possible. in particular, ca-fpn is very lightweight. moreover, considering that defects are often of varying sizes and can be extremely tiny or slender, a flexible anchor-free detector ca-autoassign is proposed by combining ca-fpn and an anchor-free detection strategy autoassign. based on the alibaba cloud tianchi fabric dataset and neu-det, ca-autoassign is compared with the state-of-the-art (sota) detectors. the experimental results show that ca-autoassign has the best detection performance with ap50 [mean average precision (map) with the intersection over union (iou) threshold of 50%] reaching 89.1 and 82.7, respectively. despite the improvement in accuracy, the processing time has barely increased. furthermore, ca-fpn is applied to other classical detectors, and the experimental results demonstrate the competitiveness and generalization ability of ca-fpn. the code is available at https://github.com/easonluht/ca-autoassign.git.",AB_0204
"retargeting aims at displaying a photo with an arbitrary aspect ratio, wherein the visually/semantically prominent objects are appropriately preserved and visual distortions can be well alleviated. conventional retargeting models are built upon the visual perception of photos from a family of prespecified communities (e.g., portrait), wherein the underlying community-specific features are not learned explicitly. thus, they cannot appropriately retarget aerial photos, which contains a rich variety of objects with different scales. in this article, a novel aerial photo retargeting framework is designed by encoding the deep features from automatically detected google maps (https://www.google.com/maps) communities into a regularized probabilistic model. specifically, we first propose an enhanced matrix factorization (mf) algorithm to calculate communities based on million-scale google maps pictures, for each of which deep feature is learned simultaneously. the enhanced mf incorporates label denoising, between-communities correlation, and deep feature encoding collaboratively. subsequently, a probabilistic model called latent topic model (ltm) is designed that quantifies the spatial layouts of multiple google maps communities in the underlying hidden space. to alleviate the overfitting from google maps communities with imbalanced numbers of aerial photos, a regularizer is added into the ltm. finally, by leveraging the regularized ltm, we shrink the test photo horizontally/vertically to maximize the posterior probability of the retargted photo. comprehensive subjective evaluations and visualizations have demonstrated the advantages of our method. besides, our calculate google maps communities are competitively consistent with the ground truth, according to the quantitative comparisons on the 2 m google maps photos.",AB_0204
"the emerging embodied artificial intelligence (ai) paradigm enables intelligent robots to learn like humans from interaction, and is thus considered an effective way approach to general ai. unfortunately, even the best-performing agents still overfit and generalize poorly to unseen scenes, due to the limited scenes provided by embodied ai simulators. to alleviate this issue, we propose a scene augmentation (sa) strategy to scale up the scene diversity for interactive tasks and make the interactions more like the real world. compared to existing methods focusing on improving diversity in observation space, our approach aims to automatically derive a new distribution of scene layout or object states to provide sufficient conditional transfer models for the agent to learn environmental invariant and irrelevant features through interaction. specifically, we provide four representative and systematical sa methods that can derive scene variants for entities from different levels of a scene graph. we demonstrate the efficiency of our methods in the popular embodied ai simulator igibson. to verify the effectiveness of interactive agents, we also conduct two representative interactive tasks with a proposed continuous action parameterized method. the evaluation results show that our sa strategy can boost the performance of interactive agents and generalize well to unseen scenes. finally, we present a systematic generalization analysis using the proposed methods to explicitly estimate the ability of agents to generalize to new layouts, new objects, and new object states. we claim that the proposed methods are not limited to igibson and can be extended to other interactive simulators. the code and additional information are available at: https://github.com/sanghongrui/sceneaug.",AB_0204
