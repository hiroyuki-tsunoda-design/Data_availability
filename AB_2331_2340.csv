AB,NO
"automatic selfie facial acne grading plays a crucial role in treatment for facial care customers, and it attracts increasing attention with the revolution of telemedicine and virtual beauty. unfortunately, the limited quantity and quality of the selfie facial dataset greatly challenge the learning of acne grading models. in this paper, we propose a gated tonesensitive augmented domain transfer (gtadt) model to address the selfie facial acne grading problem. a high-quality clinical source domain and associated cross-domain data augmentation are introduced to generate sufficient data. also, an aligned tone-sensitive model with multiple tone subnetworks is devised to bridge the domain gaps. in addition, two gate networks are devised to capture the correlation between different tone subnetworks in both the label and feature spaces. we establish three selfie facial acne datasets which consist of people across different skin tones, ages, poses, etc. the experimental results on the newly established datasets demonstrate that: 1) both the cross-domain data augmentation, tone-sensitive module, and gate networks can enhance the performance; and 2) the proposed model performs favorably against state-of-the-art methods. we make both the code and datasets publicly available at https://github.com/ wrlh/gtadt.git.",AB_0234
"unsupervised domain adaptation (uda) methods have made remarkable progress in histopathological image analysis and various cancer diagnosis domains. however, most cur-rent research focuses on transfer between single-source domains. the distribution of features between different cancer types is far away, and a well-trained model in one field may not be able to generalize well to data in other fields. to address the domain shift problem, this paper proposes a multi-source unsupervised domain adaptation method with domain mixing bridging. using multiple source and target domains, feature representations of all domains are extracted, and latent relationships are captured. afterward, the complementary information of the hybrid bridging intermediate domain is integrated to align the feature distribution. addi-tionally, we introduce a domain adversarial adaptation module to generate domain invariant features. we experimented on three different cancer pathology image datasets and achieved an average accuracy of 92.94% classification performance. it is proved that compared with the existing deep transfer learning technology, the method in this paper has a better effect. code will be available at: https://github.com/ww994/mhdan.",AB_0234
"microbiome research is now moving beyond the compositional analysis of microbial taxa in a sample. increasing evidence from large human microbiome studies suggests that functional consequences of changes in the intestinal microbiome may provide more power for studying their impact on inflammation and immune responses. although 16s rrna analysis is one of the most popular and a cost-effective method to profile the microbial compositions, marker-gene sequencing cannot provide direct information about the functional genes that are present in the genomes of community members. bioinformatic tools have been developed to predict microbiome function with 16s rrna gene data. among them, picrust2 (phylogenetic investigation of communities by reconstruction of unobserved states) has become one of the most popular functional profile prediction tools, which generates community-wide pathway abundances. however, no state-of-art inference tools are available to test the differences in pathway abundances between comparison groups. we have developed ggpicrust2, an r package, for analyzing functional profiles derived from 16s rrna sequencing. this powerful tool enables researchers to conduct extensive differential abundance analyses and generate visually appealing visualizations that effectively highlight functional signals. with ggpicrust2, users can obtain publishable results and gain deeper insights into the functional composition of their microbial communities. availability and implementation: the package is open-source under the mit and file license and is available at cran and https://github.com/cafferychen777/ggpicrust2. its shiny web is available at https://a95dps-caffery-chen.shinyapps.io/ggpicrust2_shiny/.",AB_0234
"recently, many methods have utilized haar wavelet to extract frequency-domain information for image deraining. however, haar wavelet and other tensor product wavelets only capture high frequencies in horizontal, vertical, and diagonal directions, leading to the loss of high-frequency details in deraining methods. to address this issue, we propose a multi aggregation network (magnet) based on non-separable lifting wavelet transform (nlwt), where nlwt is employed to capture high-frequency rain streaks in various directions. magnet aggregates neighboring features and incorporates outer skip connections based on the u-net architecture, effectively utilizing the complementary information of rain patterns in different features. additionally, a gated fusion module is used to fuse the aggregated features, capturing important rain pattern information and reducing feature redundancy. moreover, magnet employs a scale-guide progressive fusion module to exploit the similarity between rain patterns at adjacent scales for deraining. experiments on rainy datasets and a joint rain removal and object detection task demonstrate that our magnet outperforms advanced methods. the code is available at https://github.com/fashyon/magnet.",AB_0234
"motivation: the interactions between t-cell receptors (tcr) and peptide-major histocompatibility complex (pmhc) are essential for the adaptive immune system. however, identifying these interactions can be challenging due to the limited availability of experimental data, sequence data heterogeneity, and high experimental validation costs. results: to address this issue, we develop a novel computational framework, named mix-tpi, to predict tcr-pmhc interactions using amino acid sequences and physicochemical properties. based on convolutional neural networks, mix-tpi incorporates sequence-based and physicochemical-based extractors to refine the representations of tcr-pmhc interactions. each modality is projected into modality-invariant and modality-specific representations to capture the uniformity and diversities between different features. a self-attention fusion layer is then adopted to form the classification module. experimental results demonstrate the effectiveness of mix-tpi in comparison with other state-of-theart methods. mix-tpi also shows good generalization capability on mutual exclusive evaluation datasets and a paired tcr dataset. availability and implementation: the source code of mix-tpi and the test data are available at: https://github.com/wolverinerine/mix-tpi.",AB_0234
"vision transformer (vit) has emerged as a potential alternative to convolutional neural networks for large datasets. however, applying vit directly to medical image segmentation is challenging due to its lack of induction bias, which requires a large number of high-quality annotated medical images for effective model training. recent studies have discovered that, in addition to the increased model capacity and generalization resulting from the lack of induction bias, the excellent performance of transformer can also be attributed to its large receptive field. in this paper, we propose a u-shaped medical image segmentation model that combines large kernel convolutions with transformers. specifically, we construct a basic transformer unit using pyramidal convolution modules with multi-scale kernels and multi-layer perceptron. in the pyramid convolution module, we employ grouped convolution to reduce parameter and computational complexity while utilizing multi-scale large kernel attention as a foundation for more efficient feature extraction. for different types of grouping, different sizes of convolutions are used to enhance the extraction of features with multiple receptive fields. to optimize the extracted features from the encoder, the u-shaped model integrates a variant of the pyramidal convolutional module into the skip connections. this variant utilizes multi-scale large kernel convolutional attention based on channel splitting. the incorporation of this variant enables efficient refinement of the feature representations within the skip connections. through extensive comparisons on multi-modal medical image datasets, our model outperforms state-of-the-art methods across various evaluation metrics, with notable superiority observed on small-scale medical datasets. our research findings suggest that the combination of large kernel convolutions and transformer models introduces an advantageous inductive bias, resulting in enhanced performance specifically for small-scale medical image datasets. to facilitate accessibility, we have made our code openly accessible on our github repository, which can be found at https://github.com/medical-images-process/cnn-transformer.",AB_0234
"cracking the entangling code of proteinligand interaction (pli) is of great importance to structure-based drug design and discovery. different physical and biochemical representations can be used to describe pli such as energy terms and interaction fingerprints, which can be analyzed by machine learning (ml) algorithms to create ml-based scoring functions (mlsfs). here, we propose the ml-based pli capturer (ml-plic), a web platform that automatically characterizes pli and generates mlsfs to identify the potential binders of a specific protein target through virtual screening (vs). ml-plic comprises five modules, including docking for ligand docking, descriptors for pli generation, modeling for mlsf training, screening for vs and pipeline for the integration of the aforementioned functions. we validated the mlsfs constructed by ml-plic in three benchmark datasets (directory of useful decoys-enhanced, active as decoys and tocodecoy), demonstrating accuracy outperforming traditional docking tools and competitive performance to the deep learning-based sf, and provided a case study of the serine/threonine-protein kinase wee1 in which mlsfs were developed by using the ml-based vs pipeline in ml-plic. underpinning the latest version of ml-plic is a powerful platform that incorporates physical and biological knowledge about pli, leveraging pli characterization and mlsf generation into the design of structure-based vs pipeline. the ml-plic web platform is now freely available at http://cadd.zju.edu.cn/plic/.",AB_0234
"the chest x-ray is commonly employed in the diagnosis of thoracic diseases. over the years, numerous approaches have been proposed to address the issue of automatic diagnosis based on chest x-rays. however, the limited availability of labeled data for related diseases remains a significant challenge in achieving accurate diagnoses. this paper focuses on the diagnostic problem of thorax diseases and presents a novel deep reinforcement learning framework. this framework incorporates prior knowledge to guide the learning process of diagnostic agents, and the model parameters can be continually updated as more data becomes available, mimicking a person's learning process. specifically, our approach offers two key contributions: (1) prior knowledge can be acquired from pre-trained models using old data or similar data from other domains, effectively reducing the dependence on target domain data; and (2) the reinforcement learning framework enables the diagnostic agent to be as exploratory as a human, leading to improved diagnostic accuracy through continuous exploration. moreover, this method effectively addresses the challenge of learning models with limited data, enhancing the model's generalization capability. we evaluate the performance of our approach using the well-known nih chestx-ray 14 and chexpert datasets, and achieve competitive results. more importantly, in clinical application, we make considerable progress. the source code for our approach can be accessed at the following url: https://github.com/neasez/marl.",AB_0234
"cutaneous melanoma represents one of the most life-threatening malignancies. histopathological image analysis serves as a vital tool for early melanoma detection. deep neural network (dnn) models are frequently employed to aid pathologists in enhancing the efficiency , accuracy of diagnoses. however, due to the paucity of well-annotated, high-resolution, whole-slide histopathology image (wsi) datasets, wsis are typically fragmented into numerous patches during the model training and testing stages. this process disregards the inherent interconnectedness among patches, potentially impeding the models' performance. additionally, the presence of excess, non-contributing patches extends processing times and introduces substantial computational burdens. to mitigate these issues, we draw inspiration from the clinical decision -making processes of dermatopathologists to propose an innovative, weakly supervised deep reinforcement learning framework, titled fast medical decision-making in melanoma histopathology images (fastmdp-rl). this framework expedites model inference by reducing the number of irrelevant patches identified within wsis. fastmdp-rl integrates two dnn-based agents: the search agent (seagent) and the decision agent (deagent). the seagent initiates actions, steered by the image features observed in the current viewing field at various magnifications. simultaneously, the deagent provides labeling probabilities for each patch. we utilize multi -instance learning (mil) to construct a teacher-guided model (miltg), serving a dual purpose: rewarding the seagent and guiding the deagent. our evaluations were conducted using two melanoma datasets: the publicly accessible tcia-cm dataset and the proprietary melsc dataset. our experimental findings affirm fastmdp-rl's ability to expedite inference and accurately predict wsis, even in the absence of pixel-level annotations. moreover, our research investigates the wsi-based interactive environment, encompassing the design of agents, state and reward functions , feature extractors suitable for melanoma tissue images. this investigation offers valuable insights and references for researchers engaged in related studies. the code is available at: https://github.com/titizheng/fastmdp-rl.",AB_0234
"background: visualizing genome coverage is of vital importance to inspect and interpret various next-generation sequencing (ngs) data. besides genome coverage, genome annotations are also crucial in the visualization. while different ngs data require different annotations, how to visualize genome coverage and add the annotations appropriately and conveniently is challenging. many tools have been developed to address this issue. however, existing tools are often inflexible, complicated, lack necessary preprocessing steps and annotations, and the figures generated support limited customization. results: here, we introduce ggcoverage, an r package to visualize and annotate genome coverage of multi-groups and multi-omics. the input files for ggcoverage can be in bam, bigwig, bedgraph and tsv formats. for better usability, ggcoverage provides reliable and efficient ways to perform read normalization, consensus peaks generation and track data loading with state-of-the-art tools. ggcoverage provides various available annotations to adapt to different ngs data (e.g. wgs/wes, rna-seq, chip-seq) and all the available annotations can be easily superimposed with ' + '. ggcoverage can generate publication-quality plots and users can customize the plots with ggplot2. in addition, ggcoverage supports the visualization and annotation of protein coverage. conclusions: ggcoverage provides a flexible, programmable, efficient and user-friendly way to visualize and annotate genome coverage of multi-groups and multi-omics. the ggcoverage package is available at https://github.com/showteeth/ggcoverage under the mit license, and the vignettes are available at https://showteeth.github.io/ ggcoverage/.",AB_0234
