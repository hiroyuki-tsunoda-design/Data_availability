AB,NO
"with the increasing application of deep learning (dl) in various domains, salient object detection in optical remote sensing images (orsis-sod) has attracted significant attention. however, most existing orsi-sod methods predominantly rely on local information from low-level features to infer salient boundary cues and supervise them using boundary ground truth (gt) but fail to sufficiently optimize and protect the local information, and almost all approaches ignore the potential advantages offered by the last layer of the decoder to maintain the integrity of saliency maps. to address these issues, we propose a novel method named boundary-semantic collaborative guidance network (bscgnet) with dual-stream feedback mechanism. first, we propose a boundary protection calibration (bpc) module, which effectively reduces the loss of edge position information during forward propagation and suppresses noise in low-level features without relying on boundary gt. second, based on the bpc module, a dual feature feedback complementary (dffc) module is proposed, which aggregates boundary-semantic dual features and provides effective feedback to coordinate features across different layers, thereby enhancing cross-scale knowledge communication. finally, to obtain more complete saliency maps, we consider the uniqueness of the last layer of the decoder for the first time and propose the adaptive feedback refinement (afr) module, which further refines feature representation and eliminates differences between features through a unique feedback mechanism. extensive experiments on three benchmark datasets demonstrate that bscgnet exhibits distinct advantages in challenging scenarios and outperforms the 17 state-of-the-art (sota) approaches proposed in recent years. codes and results have been released on github: https://github.com/yuhsss/bscgnet.",AB_0269
"remote sensing image (rsi) captioning aims to generate meaningful and grammatically accurate sentences for rsis. however, in comparison to natural image captioning, rsi captioning encounters additional challenges due to the unique characteristics of rsis. the first challenge arises from the abundance of objects present in these images. as the number of objects increases, it becomes increasingly difficult to determine the main focus of the description. moreover, the objects in rsis often share similar appearances, which further complicates the generation of accurate descriptions. to overcome these challenges, we propose a prior knowledge-guided transformer (pkg-transformer) for rsi captioning. first, scene-level and object-level features are extracted in a multilevel feature extraction (mfe) module. to further refine and enhance the extracted multilevel features, we introduce a feature enhancement (fe) module. this module utilizes a combination of graph neural networks and attention mechanisms to capture the correlation and difference between different objects or scene regions. moreover, we propose a prior knowledge augmented attention (pka) mechanism to select the objects that are more relevant to the scene regions by establishing the relationships between them. this attention mechanism is seamlessly integrated into the transformer structure, providing valuable prior knowledge that promotes the caption generation process. extensive experiments on three rsi captioning datasets verify the superiority of the proposed method. compared with the baseline methods, the proposed method achieves more impressive performance. the code will be publicly available at https://github.com/one-paper-luck/pkg-transformer",AB_0269
"masked image modeling (mim) is a highly popular and effective self-supervised learning method for image understanding. the existing mim-based methods mostly focus on spatial feature modeling, neglecting spectral feature modeling. meanwhile, the existing mim-based methods use transformer for feature extraction, and some local or high-frequency information may get lost. to this end, we propose a spatial-spectral masked autoencoder (ss-mae) for hyperspectral image (hsi) and light detection and ranging (lidar)/synthetic aperture radar (sar) data joint classification. specifically, ss-mae consists of a spatialwise branch and a spectralwise branch. the spatialwise branch masks random patches and reconstructs missing pixels, while the spectralwise branch masks random spectral channels and reconstructs missing channels. our ss-mae fully exploits the spatial and spectral representations of the input data. furthermore, to complement local features in the training stage, we add two lightweight convolutional nerual networks (cnns) for feature extraction. both global and local features are taken into account for feature modeling. to demonstrate the effectiveness of the proposed ss-mae, we conduct extensive experiments on three publicly available datasets. extensive experiments on three multisource datasets verify the superiority of our ss-mae compared with several state-of-the-art baselines. the source codes are available at https://github.com/summitgao/ss-mae.",AB_0269
"remote sensing image super-resolution (rsisr) plays a vital role in enhancing spatial details and improving the quality of satellite imagery. recently, transformer-based models have shown competitive performance in rsisr. to mitigate the quadratic computational complexity resulting from global self-attention, various methods constrain attention to a local window, enhancing its efficiency. consequently, the receptive fields in a single attention layer are inadequate, leading to insufficient context modeling. furthermore, while most transformer-based approaches reuse shallow features through skip connections, relying solely on these connections treats shallow and deep features equally, impeding the model's ability to characterize them. to address these issues, we propose a novel transformer architecture called cross-spatial pixel integration and cross-stage feature fusion-based transformer network (spiffnet) for rsisr. our proposed model effectively enhances context cognition and understanding of the entire image, facilitating efficient integration of cross-stage features. the model incorporates cross-spatial pixel integration attention (cspia) to introduce contextual information into a local window, while cross-stage feature fusion attention (csffa) adaptively fuses features from the previous stage to improve feature expression in line with the requirements of the current stage. we conducted comprehensive experiments on multiple benchmark datasets, demonstrating the superior performance of our proposed spiffnet in terms of both quantitative metrics and visual quality when compared to state-of-the-art methods. our code is available at https://github.com/dr-lyt/spiffnet.",AB_0269
"contrastive learning (cl) with learnable examples performs outstandingly in data representation. however, when dealing with hard samples, instance-level alignment with excessive uniformity may descend into trivial clusters, especially when confronted with interclass similarity and intraclass diversity in hyperspectral images (hsis). to solve this problem, we regard prototypical cl as tracing the potential probability density distribution. then, a novel pretraining method, learnable sparse contrastive sampling (lscosa), is proposed for discriminative representation learning, containing sparse positive sampling and multiple positives learning. specifically, on the basis of cooperative-adversarial cl, we first exert a kullback-leibler (kl) divergence regularizer on the average activation probability of the prototypes, suppressing fake density prototypes for sparse positive sampling. furthermore, we propose multiple positives learning, in which the top -k potential positives are retrieved and dynamically weighted for contrastive supervision, to avoid trivial clusters and cover satisfying semantic variations. comprehensive experiments on three hsi benchmark datasets demonstrate that lscosa achieves significant advantages over other hsi classification (hsic) methods. the code is available at https://github.com/sakurashine/lscosa.",AB_0269
"pansharpening refers to the fusion of a panchromatic image with high spatial resolution (pan) a multispectral image with low spatial resolution (lrms) image with low spatial resolution to obtain a high spatial resolution multispectral (hrms) image, which is beneficial to visual display and geographic research. recently, many deep learning (dl) methods have been proposed to address the pansharpening problem, but still a few examples of dl-based techniques are designed from the perspective of a better receptive field while the scale of features greatly varies among different ground objects. in this article, we mainly focus on designing a cascadic multireceptive learning resblock (cml-resblock) relying on the residual network (resnet) block, which can efficiently extract multiscale features from both the pan and lrms images. moreover, we propose a novel multiplication network preserving a physical significance, which uses deep neural networks (dnns) to learn the coefficients of the pixelwise restoration mapping and multiplies the upsampled lrms image with the learned coefficients to get the hrms image. the two parts mentioned above constitute our cascadic multireceptive learning network (cmlnet). extensive experiments on both reduced-resolution and full-resolution images acquired by the worldview-3 (wv-3), gaofen-2 (gf-2), and quickbird (qb) satellites show that the proposed approach outperforms state-of-the-art methods. furthermore, additional experiments have been conducted to prove the generality of the cml-resblock and multiplication network. the code is available at: https://github.com/wajuda/cml.",AB_0269
"limited labeled training samples constitute a challenge in hyperspectral image classification, with much research devoted to cross-domain adaptation, where the classes of the source and target domains are different. current cross-domain few-shot learning (fsl) methods only use a small number of sample pairs to learn the discriminant features, which limits their performance. to address this problem, we propose a new framework for cross-domain fsl, considering all possible positive and negative pairs in a training batch and not just pairs between the support and query sets. furthermore, we propose a new kernel triplet loss to characterize complex nonlinear relationships between samples and design appropriate feature extraction and discriminant networks. specifically, the source and target data are simultaneously fed into the same feature extraction network, and then, the proposed kernel triplet loss on the embedding feature and the cross-entropy loss on the softmax output are used to learn discriminant features for both source and target data. finally, an iterative adversarial strategy is employed to mitigate domain shifts between source and target data. the proposed method significantly outperforms state-of-the-art methods in experiments on four target datasets and one source dataset. the code is available at https://github.com/kkcocoon/cfsl-kt.",AB_0269
"dimensionality reduction (dr) is important for feature extraction and classification of hyperspectral images (hsis). recently proposed superpixel-based dr models have shown promising performance, where superpixel segmentation techniques were applied to segment an hsi and then dr models like principal component analysis (pca) or linear discriminant analysis (lda) were employed to extract the local and/or global features. however, superpixelwise pca (superpca)-based local features are unsatisfactory because pca aims to extract features with high variance, which could be inefficient in superpixels with mixed objects or strong noise/outliers. in addition, superpixelwise unsupervised lda (superulda) based global features may neglect local (spatial-contextual) information. to address these issues, we propose a new spectral-spatial and superpixelwise unsupervised lda (s3-ulda) model for unsupervised feature extraction from hsis. specifically, the hsi is first segmented into various superpixels with pseudo labels. then, superpixel-based local reconstruction for hsi denoising is conducted. next, superulda is performed on both the original hsi and locally reconstructed data to extract global features. then, superpixelwise unsupervised local fisher discriminant analysis (superulfda) is developed for local feature extraction, where each superpixel and its adjacent superpixels (along with their pseudo-labels) are fed into local fisher discriminant analysis (lfda) to extract local features. the superpixel-level local manifold structures can be effectively modeled by the proposed superulfda. finally, by fusing the extracted global and local features, novel global-local and spectral-spatial features can be obtained. our experimental results on several benchmark hsis demonstrate the superiority of the proposed method over state-of-the-art methods. the code of the proposed model is available at https://github.com/xinweijiang/s3-ulda.",AB_0269
"existing few-shot segmentation approaches basically adopt the idea of comparing the semantic prototype vector of the query image and support images, and then obtaining the segmentation result. however, recent studies have shown that a single feature vector in feature map cannot accurately represent pixel-level categories, thus leading to poor segmentation of object boundary and semantic ambiguity. to address this common problem, we propose a novel contour-aware network (ctanet) for few-shot segmentation in this paper. unlike the usual practice of classifying each pixel separately, ctanet regards all pixels within the same contour as a whole, which can take advantage of the internal consistency of objects to obtain a more accurate representation of category information. to obtain the accurate object contour, our network consists of a contour generation module and a contour refinement module, where the former exploits multiple levels of features to generate a primary contour map and the latter learns to refine the primary contour map. furthermore, a novel contour-aware mixed loss is proposed to fuse the common bce loss and our contour-aware loss to supervise the training process on two levels, pixel-level and contour-level. extensive experiments demonstrate that our ctanet achieves a new state-of-the-art performance on pascal-5(i) and coco-20(i) . hopefully, our new perspective could provide more clues for future research on few-shot segmentation. our code is freely available at: https://github.com/hardtogeta/ctanet",AB_0269
"how to estimate the quality of the network output is an important issue, and currently there is no effective solution in the field of human parsing. to solve this problem, this work proposes a statistical method based on the output probability map to calculate the pixel classification quality, which is called pixel score. in addition, the quality-aware module (qam) is proposed to fuse the different quality information, the purpose of which is to estimate the quality of human parsing results. we combine qam with a concise and effective network design to propose quality-aware network (qanet) for human parsing. benefiting from the superiority of qam and qanet, we achieve the best performance on three multiple and one single human parsing benchmarks, including cihp, mhp-v2, pascal-person-part, atr and lip. without increasing the training and inference time, qam improves the ap$<^>\text{r}$ criterion by more than 10 points in the multiple human parsing task. qam can be extended to other tasks with good quality estimation, e.g instance segmentation. specifically, qam improves mask r-cnn by similar to% map on coco and lvisv1.0 datasets. based on the proposed qam and qanet, our overall system wins 1st place in cvpr2021 l2id high-resolution human parsing (hrhp) challenge, and 2nd in cvpr2021 pic short-video face parsing (sfp) challenge. code and models are available at https://github.com/soeaver/qanet.",AB_0269
