AB,NO
"motivation: while many pipelines have been developed for calling genotypes using rna-sequencing (rna-seq) data, they all have adapted dna genotype callers that do not model biases specific to rna-seq such as allele-specific expression (ase). results: here, we present bayesian beta-binomial mixture model (bbmix), a bayesian beta-binomial mixture model that first learns the expected distribution of read counts for each genotype, and then deploys those learned parameters to call genotypes probabilistically. we benchmarked our model on a wide variety of datasets and showed that our method generally performed better than competitors, mainly due to an increase of up to 1.4% in the accuracy of heterozygous calls, which may have a big impact in reducing false positive rate in applications sensitive to genotyping error such as ase. moreover, bbmix can be easily incorporated into standard pipelines for calling genotypes. we further show that parameters are generally transferable within datasets, such that a single learning run of less than 1 h is sufficient to call genotypes in a large number of samples. availability and implementation: we implemented bbmix as an r package that is available for free under a gpl-2 licence at https://gitlab. com/evigorito/bbmix and https://cran.r-project.org/package=bbmix with accompanying pipeline at https://gitlab.com/evigorito/bbmix_pipeline.",AB_0443
"background: polymerized polyphenols (pp) found in oolong tea can inhibit pancreatic lipase activity in vitro, and pilot work indicates that this may reduce postprandial lipemia. since tea contains caffeine and catechins, the interactions between these ingredients and pp warrant investigation. objectives: to assess whether pp ingested alone or with caffeine and catechins lowers postprandial lipemia. methods: fifty healthy adults [mean (sd) age: 26 (7) y; bmi (in kg/m2): 24.0 (2.7); female: n = 16] completed 4 oral lipid tolerance tests in a placebo controlled randomized, crossover design. participants ingested 40 g of fat with either 1) placebo, 2) 100 mg pp, 3) 150 mg pp, or 4) 100 mg pp plus 50 mg caffeine and 63 mg catechins (pp + cc). blood was sampled for 3 h postprandially to assess concentrations of serum and plasma triacylglycerol and plasma markers of lipid (nefa; glycerol; ldl and hdl cholesterol; and apoa-i, a-ii, b, c-ii, c-iii, and e) and glucose metabolism (glucose, insulin, and c-peptide). results: serum and plasma triacylglycerol concentrations and lipid metabolism variables generally increased following any test drink ingestion (main effect of time, p < 0.001). nevertheless, for the lipid metabolism responses, there were no statistically significant condition-time interactions and no statistically significant differences in incremental or total area under the curve between conditions, apart from hdl cholesterol (p = 0.021). ingesting 100 mg pp + cc lowered peak plasma glucose, insulin, and c-peptide concentrations compared with all other conditions 30 min postingestion (p < 0.001), with persistent alterations in glucose concentrations observed for 90 min compared with placebo and 100 mg pp conditions. conclusions: pp ingested at doses <150 mg does not clearly alter early-phase postprandial triacylglycerol concentrations in healthy adults, irrespective of the presence or absence of caffeine and catechins. nevertheless, caffeine and catechins added to pp lowered postprandial glucose and insulin concentrations. this trial was registered in clinicaltrials.gov as nct03324191 (https://clinicaltrials.gov/ct2/show/nct03324191).",AB_0443
"continent-scale observations of seismic phenomena have provided multi-scale constraints of the earth's interior. of those analyzed, array-based observations of slowness vector properties (backazimuth and horizontal slowness) and multipathing have yet to be made on a continental scale. slowness vector measurements give inferences on mantle heterogeneity properties such as velocity perturbation and velocity gradient strength and quantify their effect on the wavefield. multipathing is a consequence of waves interacting with strong velocity gradients resulting in two arrivals with different slowness vector properties and times. the mantle structure beneath the contiguous unites states has been thoroughly analyzed by previous seismic studies and is data-rich, making it an excellent testing ground to both analyze mantle structure with our approach and compare with other imaging techniques. we apply an automated array-analysis technique to an sks data set to create the first continent-scale data set of multipathing and slowness vector measurements. we analyze the divergence of the slowness vector deviation field to highlight seismically slow and fast regions. our results resolve several slow mantle anomalies beneath yellowstone, the appalachian mountains and fast anomalies throughout the mantle. many of the anomalies cause multipathing in frequency bands 0.15-0.30 and 0.20-0.40 hz which suggests velocity transitions over at most 500 km exist. comparing our observations to synthetics created from tomography models, we find model na13 (bedle et al., 2021, https:// doi.org/10.1029/2021gc009674) fits our data best but differences still remain. we therefore suggest slowness vector measurements should be used as an additional constraint in tomographic inversions and will lead to better resolved models of the mantle.",AB_0443
"background: in the scope i trial (safety and efficacy of the symetis acurate neo/tf compared to the edwards sapien 3 bioprosthesis), transcatheter aortic valve implantation with the self-expanding acurate neo (neo) did not meet noninferiority compared with the balloon-expandable sapien 3 (s3) device regarding a composite end point at 30 days due to higher rates of prosthetic valve regurgitation and acute kidney injury. data on long-term durability of neo are scarce. here, we report whether early differences between neo and s3 translate into differences in clinical outcomes or bioprosthetic valve failure 3 years after transcatheter aortic valve implantation. methods: patients with severe aortic stenosis were randomized to transfemoral transcatheter aortic valve implantation with neo or s3 at 20 european centers. clinical outcomes at 3 years are compared using cox proportional or fine-gray subdistribution hazard models by intention-to-treat. bioprosthetic valve failure is reported for the valve-implant cohort. results: among 739 patients, 84 of 372 patients (24.3%) had died in the neo and 85 of 367 ( 25%) in the s3 group at 3 years. comparing neo with s3, the 3- year rates of all-cause death (hazard ratio, 0.98 [95% ci, 0.73- 1.33]), stroke (subhazard ratio, 1.04 [95% ci, 0.56- 1.92]), and hospitalization for congestive heart failure ( subhazard ratio, 0.74 [95% ci, 0.51-1.07]) were similar between the groups. aortic valve reinterventions were required in 4 neo and 3 s3 patients (subhazard ratio, 1.32 [ 95% ci, 0.30- 5.85]). new york heart association functional class <= ii was observed in 84% (neo) and 85% (s3), respectively. mean gradients remained lower after neo at 3 years (8 versus 12 mm hg; p<0.001).' conclusions: early differences between neo and s3 did not translate into significant differences in clinical outcomes or bioprosthetic valve failure throughout 3 years. registration: url: https://clinicaltrials.gov, unique identifier: nct03011346.",AB_0443
"foreseeing the evolution of brain connectivity between anatomical regions from a baseline observation can propel early disease diagnosis and clinical decision making. such task becomes challenging when learning from multiple decentralized datasets with missing timepoints (e.g., datasets collected from different hospitals with a varying sequence of acquisitions). federated learning (fl) is an emerging paradigm that enables collaborative learning among multiple clients (i.e., hospitals) in a fully privacy-preserving fashion. however, to the best of our knowledge, there is no fl work that foresees the time-dependent brain connectivity evolution from a single timepoint-let alone learning from non-iid decentralized longitudinal datasets with varying acquisition timepoints. in this paper, we propose the first fl framework to significantly boost the predictive performance of local hospitals with missing acquisition timepoints while benefiting from other hospitals with available data at those timepoints without sharing data. specifically, we introduce 4d-fed-gnn+, a novel longitudinal federated gnn framework that works in (i) a uni-mode, where it acts as a graph self-encoder if the next timepoint is locally missing or (ii) in a dual-mode, where it concurrently acts as a graph generator and a self-encoder if the local follow-up data is available. further, we propose a dual federation strategy, where (i) gnn layer-wise weight aggregation and (ii) pairwise gnn weight exchange between hospitals in a random order. to improve the performance of the poorly-conditioned hospitals (e.g., consecutive missing timepoints, intermediate missing timepoint), we further propose a second variant, namely 4d-fed-gnn++, which federates based on an ordering of the local hospitals computed using their incomplete sequential patterns. our comprehensive experiments on real longitudinal datasets show that overall 4d-fed-gnn+ and 4d-fed-gnn++ significantly outperform benchmark methods. our source code is available at https://github.com/basiralab/4d-fedgnn-plus.",AB_0443
"network embedding - finding a low dimensional representation of the nodes with attributes in a hierarchical, directed network remains a challenging problem in the machine learning community. an emerging approach is to embed complex networks - networks of real-world systems - into hyperbolic space due to the fact that hyperbolic space can better naturally represent such a network's hierarchical structure. existing hyperbolic embedding approaches, however, cannot handle the embedding of attributed directed networks to an arbitrary embedding dimension. to fill this gap, we introduce headnet, for hyperbolic embedding of attributed directed networks, an algorithm based on extending previous works for embedding directed attributed networks to gaussian distributions in hyperbolic space of arbitrary dimension. through experimentation on a variety of both synthetic and real-world networks, we show that headnet can achieve competitive performance on common downstream machine learning tasks, including predicting directed links for previously unseen nodes. headnet provides an inductive hyperbolic embedding method for directed attributed networks, which opens the door to hyperbolic manifold learning on a wider range of real-world networks. the source code is freely available at https://github.com/davidmcdonald1993/headnet.",AB_0443
"this paper proposes a novel paradigm for the unsupervised learning of object landmark detectors. contrary to existing methods that build on auxiliary tasks such as image generation or equivariance, we propose a self-training approach where, departing from generic keypoints, a landmark detector and descriptor is trained to improve itself, tuning the keypoints into distinctive landmarks. to this end, we propose an iterative algorithm that alternates between producing new pseudo-labels through feature clustering and learning distinctive features for each pseudo-class through contrastive learning. with a shared backbone for the landmark detector and descriptor, the keypoint locations progressively converge to stable landmarks, filtering those less stable. compared to previous works, our approach can learn points that are more flexible in terms of capturing large viewpoint changes. we validate our method on a variety of difficult datasets, including ls3d, bbcpose, human3.6m and pennaction, achieving new state of the art results. code and models can be found at https://github.com/dimitrismallis/keypointstolandmarks/.",AB_0443
"ulysses is a python package that calculates the baryon asymmetry produced from leptogenesis in the context of a type-i seesaw mechanism. in this release, the new features include code which solves the boltzmann equations for low-scale leptogenesis; the complete boltzmann equations for thermal leptogenesis applying proper quantum statistics without assuming kinetic equilibrium of the righthanded neutrinos; and, primordial black hole-induced leptogenesis. ulysses version 2 has the added functionality of a pre-provided script for a two-dimensional grid scan of the parameter space. as before, the emphasis of the code is on user flexibility, rapid evaluation and is publicly available at https:// github .com /earlyuniverse /ulysses.program summary program title: ulysses cpc library link to program files: https://doi .org /10 .17632 /rzd24f34h2 .2 developer's repository link: https://github .com /earlyuniverse /ulysses /tree /master /ulysses licensing provisions: mit programming language: python3 journal reference of previous version: comput. phys. commun. 262 (2021) 107813 does the new version supersede the previous version?: yes reasons for the new version: inclusion of additional effects in boltzmann equations. summary of revisions: low scale leptogenesis, pbh-induced leptogenesis, and complete leptogenesis equations (non-momentum integrated equations) are provided. nature of problem: solve the boltzmann equations for thermal, resonant and low-scale leptogenesis through a set of coupled ordinary differential equations. solution method: boltzmann equations are solved using a combination of odeint and solve_ivp in python3.& copy; 2023 published by elsevier b.v.",AB_0443
"the planned and potential introduction in global satellite observing systems of conically scanning ka- and w-band atmospheric radars (e.g., the radars in the tomorrow.io constellation, https://www.tomorrow.io/space/, last access: 1 june 2022, and the wivern (wind velocity radar nephoscope) radar, https://www.wivern.polito.it, last access: 1 july 2022) calls for the development of methodologies for calibrating and cross-calibrating these systems. traditional calibration techniques pointing at the sea surface at about 11 & lcirc; incidence angle are in fact unfeasible for such fast rotating systems.this study proposes a cross-calibration method for conically scanning spaceborne radars based on ice cloud reflectivity probability distribution functions (pdfs) provided by reference radars like the global precipitation measurement (gpm) ka-band radar or the w-band radars planned for the esa-jaxa earthcare or for the nasa atmosphere observing system missions. in order to establish the accuracy of the methodology, radar antenna boresight positions are propagated based on four configurations of expected satellite orbits so that the ground-track intersections can be calculated for different intersection criteria, defined by cross-over instrument footprints within a certain time and a given distance. the climatology of the calibrating clouds, derived from the w-band cloudsat and ka-band gpm reflectivity records, can be used to compute the number and the spatial distribution of calibration points. finally, the mean number of days required to achieve a given calibration accuracy is computed based on the number of calibration points needed to distinguish a biased reflectivity pdf from the sampling-induced noisiness of the reflectivity pdf itself.findings demonstrate that it will be possible to cross-calibrate, within 1 db, a ka-band (w-band) conically scanning radar like that envisaged for the tomorrow.io constellation (wivern mission) every few days (a week). such uncertainties are generally meeting the mission requirements and the standards currently achieved with absolute calibration accuracies.",AB_0443
"an increasingly common output arising from the analysis of shotgun metagenomic datasets is the generation of metagenome-assembled genomes (mags), with tens of thousands of mags now described in the literature. however, the discovery and comparison of these mag collections is hampered by the lack of uniformity in their generation, annotation and storage. to address this, we have developed mgnify genomes, a growing collection of biome-specific non-redundant microbial genome catalogues generated using mags and publicly available isolate genomes. genomes within a biome-specific catalogue are organised into species clusters. for species that contain multiple conspecific genomes, the highest quality genome is selected as the representative, always prioritising an isolate genome over a mag. the species representative sequences and annotations can be visualised on the mgnify website and the full catalogue and associated analysis outputs can be downloaded from mgnify servers. a suite of online search tools is provided allowing users to compare their own sequences, ranging from a gene to sets of genomes, against the catalogues. seven biomes are available currently, comprising over 300,000 genomes that represent 11,048 non-redundant species, and include 36 taxonomic classes not currently represented by cultured genomes. mgnify genomes is available at https://www.ebi.ac.uk/metagenomics/browse/genomes/.& copy; 2023 the authors. published by elsevier ltd. this is an open access article under the cc by license ().",AB_0443
