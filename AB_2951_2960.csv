AB,NO
"the low-rank matrix completion has gained rapidly increasing attention from researchers in recent years for its efficient recovery of the matrix in various fields. numerous studies have exploited the popular neural networks to yield low-rank outputs under the framework of low-rank matrix factorization. however, due to the discontinuity and nonconvexity of rank function, it is difficult to directly optimize the rank function via back propagation. although a large number of studies have attempted to find relaxations of the rank function, e.g., schatten-p norm, they still face the following issues when updating parameters via back propagation: 1) these methods or surrogate functions are still non-differentiable, bringing obstacles to deriving the gradients of trainable variables. 2) most of these surrogate functions perform singular value decomposition upon the original matrix at each iteration, which is time-consuming and blocks the propagation of gradients. to address these problems, in this paper, we develop an efficient block-wise model dubbed differentiable low-rank learning (dlrl) framework that adopts back propagation to optimize the multi-schatten-p norm surrogate (mss) function. distinct from the original optimization of this surrogate function, the proposed framework avoids singular value decomposition to admit the gradient propagation and builds a block-wise learning scheme to minimize values of schatten-p norms. accordingly, it speeds up the computation and makes all parameters in the proposed framework learnable according to a predefined loss function. finally, we conduct substantial experiments in terms of image recovery and collaborative filtering. the experimental results verify the superiority of the proposed framework in terms of both runtimes and learning performance compared with other state-of-the-art low-rank optimization methods. our codes are available at https://github.com/chenzl23/dlrl.",AB_0296
"we study the task of single person dense pose estimation. specifically, given a human-centric image, we learn to map all human pixels onto a 3d, surface-based human body model. existing methods approach this problem by fitting deep convolutional networks on sparse annotated points where the regression on both surface coordinate components for each body part is uncorrelated and optimized separately. in this work, we devise a novel, unified loss function that explicitly characterizes the correlation for surface coordinates regression, achieving significant improvements in both accuracy and efficiency. furthermore, based on an observation that the image-to-surface correspondence is intrinsically invariant to geometric transformations from input images, we propose to enforce a geometric equivariance consistency on the target mapping, thereby allowing us to enable reliable supervision on large amounts of unlabeled pixels. we conduct comprehensive studies on the effectiveness of our approach using a quite simple network. extensive experiments on the densepose-coco dataset show that our model achieves superior performance against previous state-of-the-art methods with much less computation complexity. we hope that our work would serve as a solid baseline for future study in the field. the code will be available at https://github.com/johnqczhang/densepose.pytorch.",AB_0296
"remote sensing images (rsis) in real scenes may be disturbed by multiple factors, such as optical blur, undersampling, and additional noise, resulting in complex and diverse degradation models. at present, mainstream super-resolution (sr) algorithms only consider a single and fixed degradation (such as bicubic interpolation) and cannot flexibly handle complex degradations in real scenes. therefore, designing an sr model that can deal with various degradations has gradually attracted researchers' attention. some early studies estimate degradation kernels and then perform degradation-adaptive sr but face the problems of estimation error amplification and insufficient high-frequency details in the results. although blind sr algorithms based on generative adversarial networks (gans) have greatly improved visual quality, they still suffer from pseudotexture, mode collapse, and poor training stability. this article proposes a novel blind sr framework based on the stochastic normalizing flow (blindsrsnf) to address the above problems. blindsrsnf learns the conditional probability distribution over the high-resolution image space given a low-resolution (lr) image by explicitly optimizing the variational bound on the likelihood. blindsrsnf is easy to train and can generate photorealistic sr results that outperform gan-based models. in addition, we introduce a degradation representation strategy based on contrastive learning to avoid the error amplification problem caused by explicit degradation estimation. comprehensive experiments show that the proposed algorithm can obtain sr results with excellent visual perception quality on both simulated lr and real-world rsis. the code is available at https://github.com/hanlinwu/blindsrsnf.",AB_0296
"the mainstream remote sensing image (rsi) super-resolution (sr) algorithms treat tasks with different scale factors independently, and a single model can only process a fixed integer scale factor. however, in practical applications, it is important to continuously super-resolve rsis to multiple resolutions, as different resolutions present various levels of detail. retraining the model for each scale factor consumes huge computational resources and storage space. existing continuous-scale sr models employ static convolutions, and most are designed for natural scenes, ignoring dynamic feature extraction needs for different scale factors and the inherent properties of rsis. in addition, efficiently obtaining the continuous representation of rsis and avoiding the artifacts of rsi sr results is still a challenging problem. to address the above problems, we propose a scale-aware dynamic network (sadn) for rsi continuous-scale sr. first, we devise a scale-aware dynamic convolutional (sad-conv) layer to handle the strong randomness of the rsi textural distribution and achieve dynamic feature extraction according to scale factors. second, we devise a continuous-scale upsampling module (csum) with the multi-bilinear global implicit function (mbgif) for any-scale upsampling. the csum constructs multiple feature spaces with asymptotic resolutions to approximate the continuous representation of an image, and then, the mbgif makes full use of multiresolution features to map arbitrary coordinates to spectral values. we evaluate our sadn using various benchmarks, and the experimental results show that the csum can efficiently achieve continuous-scale upsampling while maintaining excellent objective and visual performance. moreover, our sadn uses fewer parameters and even outperforms the state-of-the-art fixed-scale sr methods. the source code is available at https://github.com/hanlinwu/sadn",AB_0296
"semantic segmentation of remote sensing images (rsis) is effective for large-scale land cover mapping, which heavily relies on a large amount of training data with laborious pixel-level labeling. due to the easy availability of image-level labels, weakly supervised semantic segmentation (wsss) based on them has attracted intensive attention. however, existing image-level wsss methods for rsis mainly focus on binary segmentation, which are difficult to apply to multiclass scenarios. this study proposes a comprehensive framework for image-level multiclass wsss of rsis, consisting of appropriate image-level label generation, high-quality pixel-level pseudo mask generation, and segmentation network iterative training. specifically, a training sample filtering method, as well as a dataset co-occurrence evaluation metric, is proposed to demonstrate proper image-level training samples. leveraging multiclass class activation maps (cams), an uncertainty-driven pixel-level weighted mask is proposed to relieve the overfitting of labeling noise in pseudo masks when training the segmentation network. extensive experiments demonstrate that the proposed framework can achieve high-quality multiclass wsss performance with image-level labels, which can attain 94.23% and 90.77% of the mean intersection over union (miou) from pixel-level labels for the isprs potsdam and vaihingen datasets, respectively. beyond that, for the deepglobe dataset with more complex landscapes, the wsss framework can achieve an accuracy close to 99% of the fully supervised case. in addition, we further demonstrate that compared to adopting multiple binary wsss models, directly training a multiclass wsss model can achieve better results, which can provide new thoughts to achieve wsss of rsis for multiclass application scenarios. our code is publically available at https://github.com/nju-lhrs/ome.",AB_0296
"recently, relying on convolutional neural networks (cnns), many methods for salient object detection in optical remote-sensing images (orsi-sod) are proposed. however, most methods ignore the number of parameters and computational cost brought by cnns, and only a few pay attention to portability and mobility. to facilitate practical applications, in this article, we propose a novel lightweight network for orsi-sod based on semantic matching and edge alignment, termed seanet. specifically, seanet includes a lightweight mobilenet-v2 for feature extraction, a dynamic semantic matching module (dsmm) for high-level features, an edge self-alignment module (esam) for low-level features, and a portable decoder for inference. first, the high-level features are compressed into semantic kernels. then, semantic kernels are used to activate salient object locations in two groups of high-level features through dynamic convolution operations in dsmm. meanwhile, in esam, cross-scale edge information extracted from two groups of low-level features is self-aligned through l-2 loss and used for detail enhancement. finally, starting from the highest level features, the decoder infers salient objects based on the accurate locations and fine details contained in the outputs of the two modules. extensive experiments on two public datasets demonstrate that our lightweight seanet not only outperforms most state-of-the-art lightweight methods, but also yields comparable accuracy with state-of-the-art conventional methods, while having only 2.76 m parameters and running with 1.7 g floating point operations (flops) for 288 x 288 inputs. our code and results are available at https://github.com/mathlee/seanet.",AB_0296
"self-supervised contrastive learning (sscl) has achieved significant milestones in remote sensing image (rsi) understanding. its essence lies in designing an unsupervised instance discrimination pretext task to extract image features from a large number of unlabeled images that are beneficial for downstream tasks. however, existing instance discrimination-based sscl suffers from two limitations when applied to the rsi semantic segmentation task: 1) positive sample confounding issue (sci), sscl treats different augmentations of the same rsi as positive samples, but the richness, complexity, and imbalance of rsi ground objects lead to the model actually pulling a variety of different ground objects closer while pulling positive samples closer, which confuse the feature of different ground objects, and 2) feature adaptation bias, sscl treats rsi patches containing various ground objects as individual instances for discrimination and obtains instance-level features, which are not fully adapted to pixel-level or object-level semantic segmentation tasks. to address the above limitations, we consider constructing samples containing single ground objects to alleviate positive sci and make the model obtain object-level features from the contrastive between single ground objects. meanwhile, we observed that the discrimination information can be mapped to specific regions in rsi through the gradient of unsupervised contrastive loss, and these specific regions tend to contain single ground objects. based on this, we propose contrastive learning with gradient-guided sampling strategy (grass) for rsi semantic segmentation. grass consists of two stages: 1) the instance discrimination warm-up stage to provide initial discrimination information to the contrastive loss gradients and 2) the gradient-guided sampling contrastive training stage to adaptively construct samples containing more singular ground objects using the discrimination information. experimental results on three open datasets demonstrate that grass effectively enhances the performance of sscl in high-resolution rsi semantic segmentation. compared with eight baseline methods from six different types of sscl, grass achieves an average improvement of 1.57% and a maximum improvement of 3.58% in terms of mean intersection over the union (miou). in addition, we discovered that the unsupervised contrastive loss gradients contain rich feature information, which inspires us to use gradient information more extensively during model training to attain additional model capacity. the source code is available at https://github.com/geox-lab/grass.",AB_0296
"infrared small target detection plays an important role in military and civilian fields while it is difficult to be solved by deep learning (dl) technologies due to scarcity of data and strong interclass imbalance. to relieve scarcity of data, we build a massive dataset irdst, which contains 142 727 frames. also, we propose a receptive-field and direction-induced attention network (rdian), which is designed using the characteristics of target size and grayscale to solve the interclass imbalance between targets and background. using convolutional layers with different receptive fields in feature extraction, target features in different local regions are captured, which enhances the diversity of target features. using multidirection guided attention mechanism, targets are enhanced in low-level feature maps. experimental results with comparison methods and ablation study demonstrate effective detection performance of our model. dataset and code will be available at https://xzbai.buaa.edu.cn/datasets.html.",AB_0296
"change detection (cd) has received raising attention for its broad application value. however, traditional fully supervised cd methods have a huge demand for pixel-level annotations, which are laborious and even impossible in some few-shot scenarios. recently, several semisupervised cd (sscd) methods have been proposed to utilize numerous unlabeled remote sensing image (rsi) pairs, which can largely reduce the annotation dependence. these methods are mainly based on: 1) adversarial learning, whose optimization direction is difficult to control as a black-box method, or 2) feature-consistency learning, which has no explicit physical meaning. to deal with these difficulties, we propose a novel progressive sscd framework in this article, termed feature-prediction alignment (fpa). fpa can efficiently utilize unlabeled rsi pairs for training by two alignment strategies. first, a class-aware feature alignment (fa) strategy is designed to align the area-level change/no-change feature extracted from different unlabeled rsi pairs (i.e., across regions) with the awareness of their locations, in order to reduce the feature difference within the same classes. second, a pixelwise prediction alignment (pa) is devised to align the pixel-level change prediction of strongly augmented unlabeled rsi pairs to the pseudo-labels calculated from the corresponding weakly augmented counterparts, in order to reduce the prediction uncertainty of various rsi transformations with physical meaning. experiments are carried out on four widely used cd benchmarks, including learning, vision and remote sensing laboratory (levir-cd), wuhan university building cd (whu-cd), cdd, and gz-cd, and our fpa achieves the state-of-the-art performance. the experimental results demonstrate the superiority of our method in both effectiveness and generalization. our code is available at https://github.com/zxt9/fpa-sscd.",AB_0296
"the fusion between the low-resolution hyperspectral image (lrhsi) and the panchromatic (pan) image could obtain the high-resolution hyperspectral image (hrhsi). recently, deep learning (dl)-based fusion methods have been explored widely due to their powerful feature learning ability. however, most dl-based methods that use the one-step fusion manner can suffer from the blurring effect. in addition, they have not fully utilized the spatial and spectral feature information of two input images, which hinders the improvement of the resulting image quality. therefore, to fully mitigate the blurring effect and utilize two input images, we propose a dual conditional diffusion models-based fusion network (dcdmf) to obtain the fused hrhsi. the conditional diffusion model (cdm) can generate the high-quality image with realistic details in an iterative denoising manner (in the inference sampling stage) other than the one-step fusion manner, which could mitigate the blurring effect greatly. to improve the spatial and spectral fidelity of the fused hrhsi, we propose the dual spatial and spectral cdm (dcdm) (two noise prediction networks with different conditional input) to, respectively, extract the image feature from the lrhsi and pan images with different image characteristics and reconstruct the corresponding hrhsi feature and fuse them. in addition, considering the high-dimensional property of the hyperspectral image (hsi), we pretrain an auto-encoder to encode the hsi into the low-dimensional latent space with more discriminate features to reduce the computational cost. based on the autoencoder, we also perform the image generation process in the residual latent space to focus on learning the residual latent spatial details. extensive experimental results on three datasets show the superiority of our method over several state-of-the-art (sota) methods. (the ziyuan (zy) dataset and codes could be available at https://github.com/rs-lsl/dcdmf)",AB_0296
