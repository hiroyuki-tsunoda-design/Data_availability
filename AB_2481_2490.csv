AB,NO
"single-view three-dimensional (3d) object reconstruction has always been a long-term challenging task. objects with complex topologies are hard to accurately reconstruct, which makes existing methods suffer from blurring of shape boundaries between multiple components in the object. moreover, most of them cannot balance learning between global geometric structure information and local detail information. in this article, we propose a multi-scale edge-guided learning network (megln) to utilize the global edge information guiding the network to better capture and recover local details. the goal is to exploit the multi-scale learning strategy to learn global edge information and local details, thus achieving robust 3d object reconstruction. we first design a multi-scale gaussian difference block (mgdb) to extract global edge geometry features for input images of different scales and adopt the attention mechanism to aggregate the extracted global edge geometry features of different scales. second, we design a multi-scale feature interaction block (mfib) to learn local details, which utilizes the multi-scale feature interaction to capture the features of multiple objects or components at multiple scales. the mfib can learn and capture better as much local detail information as possible under the guidance of global edge information. finally, we dynamically fuse the predicted probabilities of the mgdb and mfib to obtain the final predicted result, which makes our megln able to recover 3d shapes with global complex topological structures and rich local details via the multi-scale learning strategy. extensive qualitative and quantitative experimental results on the shapenet dataset demonstrate that our approach achieves competitive performance compared with state-of-the-art methods. code is available at https://github.com/ray-tju/megln.",AB_0249
"deep convolutional neural networks have been demonstrated to be effective for single-image super-resolution in recent years. on the one hand, residual connections and dense connections have been used widely to ease forward information and backward gradient flows to boost performance. however, current methods use residual connections and dense connections separately in most network layers in a sub-optimal way. on the other hand, although various networks and methods have been designed to improve computation efficiency, save parameters, or utilize training data of multiple scale factors for each other to boost performance, they either do super-resolution in high-resolution space to have a high computation cost or cannot share parameters between models of different scale factors to save parameters and inference time. to tackle these challenges, we propose an efficient single-image super-resolution network using dual path connections with multiple scale learning (emsrdpn). by introducing dual path connections inspired by dual path networks into emsrdpn, it uses residual connections and dense connections in an integrated way in most network layers. dual path connections have the benefits of both reusing common features of residual connections and exploring new features of dense connections to learn a good representation for single-image super-resolution. to utilize the feature correlation of multiple scale factors, emsrdpn shares all network units in low-resolution space between different scale factors to learn shared features and only uses a separate reconstruction unit for each scale factor, which can utilize training data of multiple scale factors to help each other to boost performance, meanwhile, which can save parameters and support shared inference for multiple scale factors to improve efficiency. experiments show emsrdpn achieves better performance and comparable or even better parameter and inference efficiency over state-of-the-art methods. code will be available at https://github.com/yangbincheng/emsrdpn.",AB_0249
"well-annotatedmedical datasets enable deep neural networks (dnns) to gain strong power in extracting lesion-related features. building such large and well-designed medical datasets is costly due to the need for high-level expertise. model pre-training based on image-net is a common practice to gain better generalization when the data amount is limited. however, it suffers from the domain gap between natural and medical images. in this work, we pre-train dnns on ultrasound (us) domains instead of imagenet to reduce the domain gap in medical us applications. to learn us image representations based on unlabeled us videos, we propose a novel meta-learning-based contrastive learning method, namely meta ultrasound contrastive learning (meta-uscl). to tackle the key challenge of obtaining semantically consistent sample pairs for contrastive learning, we present a positive pair generation module along with an automatic sampleweightingmodule based on meta-learning. experimental results on multiple computer-aided diagnosis (cad) problems, including pneumonia detection, breast cancer classification, and breast tumor segmentation, show that the proposed self-supervised method reaches state-of-the-art (sota). the codes are available at https://github.com/schuture/meta-uscl.",AB_0249
"the secondary vascular tissue emanating from meristems is central to understanding how vascular plants such as forest trees evolve, grow, and regulate secondary radial growth. however, the overall molecular characterization of meristem origins and developmental trajectories from primary to secondary vascular tissues in woody tree stems is technically challenging. in this study, we combined high-resolution anatomic analysis with a spatial transcriptome (st) technique to define features of meristematic cells in a developmental gradient from primary to secondary vascular tissues in poplar stems. the tissue-specific gene expression of meristems and derived vascular tissue types were accordingly mapped to specific anatomical domains. pseudotime analyses were used to track the origins and changes of meristems throughout the development from primary to secondary vascular tissues. surprisingly, two types of meristematiclike cell pools within secondary vascular tissues were inferred based on high-resolution microscopy combined with st, and the results were confirmed by in situ hybridization of , transgenic trees, and single-cell sequencing. the rectangle shape procambium-like (pcl) cells develop from procambium meristematic cells and are located within the phloem domain to produce phloem cells, whereas fusiform shape cambium zone (cz) meristematic cells develop from fusiform metacambium meristematic cells and are located inside the cz to produce xylem cells. the gene expression atlas and transcriptional networks spanning the primary transition to secondary vascular tissues generated in this work provide new resources for studying the regulation of meristem activities and the evolution of vascular plants. a web server (https:// pgx.zju.edu.cn/strnapal/) was also established to facilitate the use of st rna-seq data.",AB_0249
"various deep learning methods have been proposed to segment breast lesions from ultrasound images. however, similar intensity distributions, variable tumor morphologies and blurred boundaries present challenges for breast lesions segmentation, especially for malignant tumors with irregular shapes. considering the complexity of ultrasound images, we develop an adaptive attention u-net (aau-net) to segment breast lesions automatically and stably from ultrasound images. specifically, we introduce a hybrid adaptive attention module (haam), which mainly consists of a channel self-attention block and a spatial self-attention block, to replace the traditional convolution operation. compared with the conventional convolution operation, the design of the hybrid adaptive attention module can help us capture more features under different receptive fields. different from existing attention mechanisms, the haam module can guide the network to adaptively select more robust representation in channel and space dimensions to cope with more complex breast lesions segmentation. extensive experiments with several state-of-the-art deep learning segmentation methods on three public breast ultrasound datasets show that our method has better performance on breast lesions segmentation. furthermore, robustness analysis and external experiments demonstrate that our proposed aau-net has better generalization performance in the breast lesion segmentation. moreover, the haam module can be flexibly applied to existing network frameworks. the source code is available on https:// github.com/cgpxy/aau-net.",AB_0249
"collecting sufficient high-quality training data for deep neural networks is often expensive or even unaffordable in medical image segmentation tasks. we thus propose to train the network by using external data that can be collected in a cheaper way, e.g., crowd-sourcing. we show that by data discernment, the network is able to mine valuable knowledge from external data, even though the data distribution is very different from that of the original (internal) data. we discern the external data by learning an importance weight for each of them, with the goal to enhance the contribution of informative external data to network updating, while suppressing the data that are 'useless' or even 'harmful'. an iterative algorithm that alternatively estimates the importance weight and updates the network is developed by formulating the data discernment as a constrained nonlinear programming problem. it estimates the importance weight according to the distribution discrepancy between the external data and the internal dataset, and imposes a constraint to drive the network to learn more effectively, compared with the network without using the external data. we evaluate the proposed algorithm on two tasks: abdominal ct image and cervical smear image segmentation, using totally 6 publicly available datasets. the effectiveness of the algorithm is demonstrated by extensive experiments. source codes are available at: https://github.com/youyisong/data-discernment.",AB_0249
"accurate citywide traffic inference is critical for improving intelligent transportation systems with smart city applications. however, this task is very challenging given the limited training data, due to the high cost of sensor installment and maintenance across the entire urban space. a more practical scenario to study the citywide traffic inference is effectively modeling the spatial and temporal traffic patterns with limited historical traffic observations. in this work, we propose a dynamic multi-view graph neural network for citywide traffic inference with the method ctvi+. specifically, for the temporal dimension, we propose a temporal self-attention mechanism that is capable of learning the dynamics of traffic data with the time-evolving traffic volume variations. for spatial dimension, we build a multi-view graph neural network, employing the road-wise message passing scheme to capture the region dependencies. with the designed spatial-temporal learning paradigms, we enable our traffic inference model to encode the dynamism from both spatial and temporal traffic patterns, which is reflective of intra- and inter-road traffic correlations. in our evaluation, ctvi+ achieves consistent better performance compared with different baselines on real-world traffic volume datasets. further ablation study validates the effectiveness of key components in ctvi+. we release the model implementation at https://github.com/dsj96/tkdd.",AB_0249
"medical visual question answering (med-vqa) aims to accurately answer a clinical question presented with a medical image. despite its enormous potential in healthcare services, the development of this technology is still in the initial stage. on the one hand, med-vqa tasks are highly challenging due to the massive diversity of clinical questions that require different visual reasoning skills for different types of questions. on the other hand, medical images are complex in nature and very different from natural images, while current med-vqa datasets are small-scale with a few hundred radiology images, making it difficult to train a well-performing visual feature extractor. this paper addresses above two critical issues. we propose a novel conditional reasoning mechanism with a question-conditioned reasoning component and a type-conditioned reasoning strategy to learn effective reasoning skills for different med-vqa tasks adaptively. further, we propose to pre-train a visual feature extractor for med-vqa via contrastive learning on large amounts of unlabeled radiology images. the effectiveness of our proposals is validated by extensive experiments on existing med-vqa benchmarks, which show significant improvement of our model in prediction accuracy over state-of-the-artmethods. the source code and pre-training dataset are provided at https://github.com/awenbocc/cpcr.",AB_0249
"transformer-based methods are recently popular in vision tasks because of their capability to model global dependencies alone. however, it limits the performance of networks due to the lack of modeling local context and global-local correlations of multi-scale features. in this paper, we present missformer, a medical image segmentation transformer. missformer is a hierarchical encoder-decoder network with two appealing designs: 1) a feed-forward network in transformer block of u-shaped encoder-decoder structure is redesigned, remix-ffn, which explore global dependencies and local context for better feature discrimination by re-integrating the local context and global dependencies; 2) a remixed transformer context bridge is proposed to extract the correlations of global dependencies and local context in multi-scale features generated by our hierarchical transformer encoder. the missformer shows a solid capacity to capture more discriminative dependencies and context in medical image segmentation. the experiments on multi-organ, cardiac segmentation and retinal vessel segmentation tasks demonstrate the superiority, effectiveness and robustness of our missformer. specifically, the experimental results of missformer trained from scratch even outperform state-of-the-art methods pre-trained on imagenet, and the core designs can be generalized to other visual segmentation tasks. the code has been released on github: https://github.com/zhifangdeng/missformer.",AB_0249
"motivation: single-cell rna sequencing (scrna-seq) technology attracts extensive attention in the biomedical field. it can be used to measure gene expression and analyze the transcriptome at the single-cell level, enabling the identification of cell types based on unsupervised clustering. data imputation and dimension reduction are conducted before clustering because scrna-seq has a high dropout rate, noise and linear inseparability. however, independence of dimension reduction, imputation and clustering cannot fully characterize the pattern of the scrna-seq data, resulting in poor clustering performance. herein, we propose a novel and accurate algorithm, ssnmdi, that utilizes a joint learning approach to simultaneously perform imputation, dimensionality reduction and cell clustering in a non-negative matrix factorization (nmf) framework. in addition, we integrate the cell annotation as prior information, then transform the joint learning into a semi-supervised nmf model. through experiments on 14 datasets, we demonstrate that ssnmdi has a faster convergence speed, better dimensionality reduction performance and a more accurate cell clustering performance than previous methods, providing an accurate and robust strategy for analyzing scrna-seq data. biological analysis are also conducted to validate the biological significance of our method, including pseudotime analysis, gene ontology and survival analysis.we believe that we are among the first to introduce imputation, partial label information, dimension reduction and clustering to the single-cell field. availability and implementation: the source code for ssnmdi is available at https://github.com/yushanqiu/ssnmdi.",AB_0249
