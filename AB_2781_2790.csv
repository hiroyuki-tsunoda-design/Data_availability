AB,NO
"few-shot segmentation (fss), which aims to determine specific objects in the query image given only a handful of densely labeled samples, has received extensive academic attention in recent years. however, most existing fss methods are designed for natural images, and few works have been done to investigate more realistic and challenging applications, e.g., remote sensing image understanding. in such a setup, the complex nature of the raw images would undoubtedly further increase the difficulty of the segmentation task. to couple with potential inference failures, we propose a novel and powerful remote sensing fss framework with global rectification (gr) and decoupled registration (dr), termed r(2)net. specifically, a series of dynamically updated global prototypes are utilized to provide auxiliary nontarget segmentation cues and to prevent inaccurate prototype activation resulting from the variability between query-support image pairs. the foreground (fg) and background information flows are then decoupled for more targeted and tailored object localization, avoiding unnecessary confusion from information redundancy. furthermore, we impose additional constraints to promote interclass separability and intraclass compactness. extensive experiments on the standard benchmark isaid-5(i) demonstrate the superiority of the proposed r(2)net over state-of-the-art fss models. the code is available at https://github.com/chunbolang/r2net.",AB_0279
"recently, fully convolutional networks (fcns) have contributed significantly to salient object detection (sod) in optical remote sensing images (rsis). however, owing to the limited receptive fields of fcns, accurate and integral detection of salient objects in rsis with complex edges and irregular topology is still challenging. moreover, suffering from the low contrast and complicated background of rsis, existing models often occur ambiguous or uncertain recognition. to remedy the above problems, we propose a novel hybrid modeling approach, i.e., uncertainty-aware graph reasoning with global collaborative learning (ug2l) framework. specifically, we propose a graph reasoning pipeline to model the intricate relations among rsi patches instead of pixels and introduce an efficient graph reasoning block (grb) to build graph representations. on top of it, a global context block (gcb) with a linear attention mechanism is proposed to explore the multiscale and global context collaboratively. finally, we design a simple yet effective uncertainty-aware loss (ual) to enhance the model's reliability for better prediction of saliency or nonsaliency. experimental and visual results on three datasets show the superiority of the proposed ug2l. code is available at https://github.com/lyf0801/ug2l.",AB_0279
"the normalized difference index (ndi) originates from ndvi, which has been widely used in remote sensing applications and to guide the development of ndi in other fields due to its excellent performance; however, injective mapping from the original bands to ndi leads to information loss in land cover classification. when ndi is represented as a simple form, it is, furthermore, prone to premature saturation in specific change detection and variable inversion tasks. in this study, we first propose the radius index (ri), a new index to represent illumination variations by using the missing band information from ndi. based on ri, we develop a generalized ndi (gnd) by adding four positive scaling coefficients to ndi foundation, and the value range and sensitivity of gnd are adjusted by these four coefficients, which are derived from the statistical information of the study area. the derivation of these four coefficients is, moreover, reversible, making it possible to interpret the applicable range of the derived set of coefficients. our experiments demonstrate that: 1) gnd is more effective in terms of improving saturation than the traditional indices and 2) mapping the original bands to gnd-ri (gnd combined with ri) can guide classifiers to learn more generalized features based on spectral information and thus achieve higher classification accuracy both in machine learning and the latest deep learning semantic segmentation models. the data and code for the article can be found at https://github.com/zoulinx/gnd.",AB_0279
"in recent years, the task of salient object detection in optical remote sensing images (rsi-sod) has received extensive attention. benefiting from the development of deep learning, much progress has been made in rsi-sod field. however, existing methods still face challenges in addressing various issues present in optical rsi, including uncertain numbers of salient objects, cluttered backgrounds, and interference from shadows. to address these challenges, we propose a novel approach, adaptive edge-aware semantic interaction network (aesinet) for efficient salient object detection (sod). specifically, to improve the extraction of complex edge information, we design a local detail aggregation module (ldam). this module can adaptively enhance the edge information of salient objects by leveraging our proposed difference perception mechanism. notably, our difference perception mechanism is a novel edge enhancement method without the supervision of edge ground truth. additionally, to accurately locate salient objects of varying numbers and scales, we design a multiscale feature extraction module (mfem), which effectively captures and utilizes multiscale information. moreover, we design the deep semantic interaction module (dsim) to identify salient objects amidst cluttered backgrounds and effectively mitigate the interference of shadows. we conduct extensive experiments on three well-established optical rsi datasets and the results demonstrate that our proposed model outperforms 14 state-of-the-art methods. all codes and detection results are available at https://github.com/xumingzhu989/aesinet-tgrs.",AB_0279
"deep learning (dl) has recently achieved outstanding performance in change detection of multitemporal images. however, most existing dl-based change detection methods still suffer from the problem of insufficient labeled training samples. to overcome this limitation, an unsupervised superpixel-guided self-supervised learning network (s3net) is proposed for detecting changes occurred on the land surface. by performing principal component analysis on two input images, a triple-channel pseudocolor image containing the main information of both the images is first generated, which is used for superpixel segmentation to produce homogeneous image objects. then, a siamese network composing of two identical subnetworks with shared weight based on transfer learning is trained for pretext task in a self-supervised learning way, aiming to obtain multiscale object-level spatial feature difference images. on this basis, a high-quality difference image is generated by incorporating the pixel-level and object-level difference information using a simple weighted fusion strategy, which can be analyzed by thresholding to produce the final binary change map. the experimental results on four real-world datasets from different sensors show that the proposed approach can obtain superior performance in comparison to several state-of-the-art change detection methods, which further demonstrates its effectiveness and practicability. we make our data and code publicly available (https://github.com/omega-rs/s3net_cd).",AB_0279
"hyperspectral (hs) image change detection (cd) is an integral component of multitemporal remote-sensing (rs) earth observation research. however, the existing hs image cd technology still has some problems, such as insufficient effective information extraction and weak correlation between shallow information and deep information, and so on. this letter proposes a new approach called attention-guided feature fusion network for multitemporal hs image cd (agf(2)net). this method is capable of efficiently retrieving and combining spatial-spectral (ss) features extracted from both shallow and deep layers of hs images, thereby enhancing the network's capability to capture features from multitemporal hs images. the attention-guided enhanced joint feature extraction strategy is used to obtain a better change discriminative feature representation, and the multilevel features extracted from the backbone network are combined between spatial information and spectral information. the integrated channel feature fusion module (icffm) not only solves the problem of insufficient feature fusion at different levels, but also strengthens effective semantic information and forms features with more discriminative ability, while also realizing the advantages of multiple features and enhancing the network's robustness and the accuracy of cd results. according to experimental results obtained from three publicly available datasets for detecting changes in hs images, the findings indicate that the proposed agf(2)net outperforms most advanced state-of-the-art (sota) methods. the source code of the agf(2)net will be public on https://github.com/nwh/agf(2)net.",AB_0279
"transformer has been widely applied in image processing tasks as a substitute for convolutional neural networks (cnns) for feature extraction due to its superiority in global context modeling and flexibility in model generalization. however, the existing transformer-based methods for semantic segmentation of remote sensing (rs) images are still with several limitations, which can be summarized into two main aspects: 1) the transformer encoder is generally combined with cnn-based decoder, leading to inconsistency in feature representations; and 2) the strategies for global and local context information utilization are not sufficiently effective. therefore, in this article, a global-local transformer segmentor (glots) framework is proposed for the semantic segmentation of rs images to acquire consistent feature representations by adopting transformers for both encoding and decoding, in which a masked image modeling (mim) pretrained transformer encoder is adopted to learn semantic-rich representations of input images and a multiscale global-local transformer decoder is designed to fully exploit the global and local features. specifically, the transformer decoder uses a feature separation-aggregation module (fsam) to utilize the feature adequately at different scales and adopts a global-local attention module (glam) containing global attention block (gab) and local attention block (lab) to capture the global and local context information, respectively. furthermore, a learnable progressive upsampling strategy (lpus) is proposed to restore the resolution progressively, which can flexibly recover the fine-grained details in the upsampling process. the experiment results on the three benchmark rs datasets demonstrate that the proposed glots is capable of achieving better performance with some state-of-the-art methods, and the superiority of the proposed framework is also verified by ablation studies. the code will be available at https://github.com/lyhnsn/glots.",AB_0279
"deep learning methods have been proved outperforming the traditional methods in the field of hyperspectral image classification (hsic). however, in pursuit of higher accuracy, hsic networks have become deeper and more complex, resulting in excessive parameters and computational cost. to deploy neural networks on small platforms such as mobile or embedded devices, many studies have focused on lightweight hsic networks. currently, these works are dominated by patch-based networks, which suffer from the low accuracy caused by lightweight scale and slow inference speed derived from structural deficiencies of such networks. it is worth noting that full convolutional networks (fcns) are able to achieve fast inference, but they tend to consume massive memory. to this end, this article proposes a novel lightweight hsic method, which consists of a successive spatial rectified network (ssrnet) and a noncentral positional sampling (ncps) strategy. ssrnet is composed of a local channel attention-based spectral fcn and several separable atrous spatial pyramid modules (saspms). these shallow subnetworks are concatenated together to progressively optimize their outputs by successive spatial rectified learning. for decreasing memory access cost (mac), ssrnet makes little patches as input to perform patch-wise pixel-to-pixel learning. after training, ssrnet is able to adapt to any size of hyperspectral images and complete fast inference of the full image directly. in particular, the ncps sampling strategy enables all labeled pixels to equally traverse all spatial positions of each training patch through the positional shift sampling, which effectively alleviates the sparse problem of hyperspectral semantic labels. experiments upon three public benchmark datasets indicate that ssrnet is comparable to the state-of-the-art methods in classification accuracy with less than 0.15 m parameters and only occupies less than 10-mb memory for single forward computation. moreover, ssrnet behaves significantly superior to traditional patch-based networks in terms of the inference speed. the source codes can be available from the website of https://github.com/pancakerr/hsic-platform.",AB_0279
"semantic segmentation of mars scenes has a crucial role in mars rovers science missions. current convolutional neural network (cnn)-based composition of u-net has powerful information extraction capabilities; however, convolutional localization suffers from the limited global context modeling capability. although transformer global modeling has performed well, it still encounters obstacles in the extraction and retention of low-level features. this issue is particularly relevant for martian rocks with their varying shapes, textures, and sizes in mars scenes. in this article, we propose a novel transformer semantic segmentation framework for martian rock images, called marsformer, that consists of an encoder-decoder structure connected through a feature enhancement module (fem) and a window transformer block (wtb). specifically, multiscale hierarchical features are generated by the mix transformer (mit) encoders, upgraded-ffn decoder (ufd) fuse and filter features at different scales, preserving the rich local and global contextual information. fem enhances the inter-multiscale feature correlation from both spatial and channel perspectives. wtb captures the long-range contexts and preserves the local features. we built two datasets of synthetic and real martian rocks. the synthetic dataset is synmars, referencing data from the zhurong rover taken from its virtual terrain engine. the other dataset is marsdata-v2, from real mars scenes, and published recently in our previous study. extensive experiments conducted on both datasets showed that marsformer achieves superiority in martian rock segmentation, obtaining state-of-the-art performance with favorable computational simplicity. the data are available at: https://github.com/cvir-lab/synmars.",AB_0279
"various deep neural networks (dnns) have been constructed to inject the spatial information of the panchromatic (pan) image into the low spatial resolution multispectral (lr ms) image. however, most of them ignore the local dissimilarity (ld) prior between ms and pan images, which has a negative influence on the fused image. considering the above-mentioned issues, we propose a deep multiscale ld network (dmld-net) to learn the ld prior at different scales and enhance the spatial and spectral information in the fused image better. specifically, we first synthesize a downsampled pan image from the original pan image to match the scale of the lr ms image. then, an ld metric is designed to calculate the dissimilarity map between the two images in feature space. according to the learned dissimilarity map, we use an ld-guided attention block (ldgab) to suppress the impact of ld, which filters out the dissimilar information in the features of the pan image. to learn the ld prior between ms and pan images sufficiently, the multiscale architecture is considered and we infer the dissimilar maps hierarchically and inject filtered features into the lr ms image progressively. finally, the fused image is generated by a reconstruction block. through the ld learning at different scales, reasonable spatial information is extracted from the pan image, by which the distortions in the fused image caused by ld can be reduced efficiently. extensive experiments are conducted on the geoeye-1 and worldview-2 datasets, and the results demonstrate the effectiveness of the proposed dmld-net in terms of spatial and spectral preservation. the code is available at https://github.com/rsmagneto/dmld-net.",AB_0279
