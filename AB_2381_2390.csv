AB,NO
"the estimation of spacecraft pose is crucial in numerous space missions, including rendezvous and docking, debris removal, and on-orbit maintenance. estimating the pose of space objects is significantly more challenging than that of objects on earth, primarily due to the widely varying lighting conditions, low resolution, and limited amount of data available in space images. our main proposal is a new deep learning neural network architecture, which can effectively extract orbiting spacecraft features from images captured by inverse synthetic aperture radar (isar) for pose estimation of non-cooperative on orbit spacecraft. specifically, our model enhances spacecraft imaging by improving image contrast, reducing noise, and using transfer learning to mitigate data sparsity issues via a pre-trained model. to address sparse features in spacecraft imaging, we propose a dense residual u-net network that employs dense residual block to reduce feature loss during downsampling. additionally, we introduce a multi-head self-attention block to capture more global information and improve the model's accuracy. the resulting tightly interlinked architecture, named as su-net, delivers strong performance gains on pose estimation by spacecraft isar imaging. experimental results show that we achieve the state of the art results, and the absolute error of our model is 0.128 degrees to 0.4491 degrees, the mean error is about 0.282 degrees, and the standard deviation is about 0.065 degrees. the code are released at https://github.com/tombs98/su-net.",AB_0239
"objective. due to the blurry edges and uneven shape of breast tumors, breast tumor segmentation can be a challenging task. recently, deep convolution networks based approaches achieve satisfying segmentation results. however, the learned shape information of breast tumors might be lost owing to the successive convolution and down-sampling operations, resulting in limited performance.approach. to this end, we propose a novel shape-guided segmentation (sgs) framework that guides the segmentation networks to be shape-sensitive to breast tumors by prior shape information. different from usual segmentation networks, we guide the networks to model shape-shared representation with the assumption that shape information of breast tumors can be shared among samples. specifically, on the one hand, we propose a shape guiding block (sgb) to provide shape guidance through a superpixel pooling-unpooling operation and attention mechanism. on the other hand, we further introduce a shared classification layer (scl) to avoid feature inconsistency and additional computational costs. as a result, the proposed sgb and scl can be effortlessly incorporated into mainstream segmentation networks (e.g. unet) to compose the sgs, facilitating compact shape friendly representation learning.main results. experiments conducted on a private dataset and a public dataset demonstrate the effectiveness of the sgs compared to other advanced methods.significance. we propose a united framework to encourage existing segmentation networks to improve breast tumor segmentation by prior shape information. the source code will be made available at https://github.com/txlin7/shape-seg.",AB_0239
"exposome is the future of next-generation environmentalhealthto establish the association between environmental exposure and diseases.however, due to low concentrations of exposure chemicals, exposomehas been hampered by lacking an effective analytical platform to characterizeits composition. in this study, by combining the benefit of chemicalisotope labeling and pseudo-multiple reaction monitoring (cil-pseudo-mrm),we have developed one highly sensitive and high-throughput platform(cil-expmrm) by isotope labeling urinary exposure biomarkers. dansylchloride (dnscl), n-methylphenylethylamine (mpea),and their isotope-labeled forms were used to derivatize polar hydroxyland carboxyl compounds, respectively. we have programmed a seriesof scripts to optimize mrm transition parameters, curate the mrm database(>70,000 compounds), predict accurate retention time (rt), andautomizedynamic mrms. this was followed by an automated mrm peak assignment,peak alignment, and statistical analysis. a computational pipelinewas eventually incorporated into a user-friendly website interface,named cil-expmrm (http://www.exposomemrm.com/). the performance of this platform has been validated with a relativelylow false positive rate (10.7%) across instrumental platforms. cil-expmrmhas systematically overcome key bottlenecks of exposome studies tosome extent and outperforms previous methods due to its independenceof ms/ms availability, accurate rt prediction, and collision energyoptimization, as well as the ultrasensitivity and automated robustintensity-based quantification. overall, cil-expmrm has great potentialto advance the exposomic studies based on urinary biomarkers. this study achieved ultrasensitive andhigh-throughput analysisof polar compounds, revealing great potential to advance the screeningof urinary biomarkers.",AB_0239
"unsupervised domain adaptation (uda) is an important solution for the cross-domain problem in semantic segmentation. existing segmentation uda methods mainly consider the domain shift as the major challenge. this paper, from a novel viewpoint, disentangles the cross-domain problem into two negative factors beyond the domain shift. specifically, we find that apart from the domain shift factor, the dispersed within-class distribution on the target domain is another factor that compromises cross-domain segmentation. this paper finds that the neglected target domain distribution dispersion is a challenge as crucial as the domain shift. in response to the joint of these two negative factors, we propose a pull-and-concentrate (puco) method comprised of two consistencies: (1) a cross-domain consistency pulls the source and target domain distribution (of the same class) close to each other based on a novel statistical style transfer. (2) an intra-domain consistency concentrates the within-class distribution on the target domain in a new unsupervised teacher-student method. both consistencies have the advantage of being robust (or insulated) from pseudo-label noises. this advantage allows puco to bring consistent improvement over a battery of pseudo-label-based uda methods. for example, on gta5 to cityscapes and synthia to cityscapes, puco achieves 60.3% and 57.2% mean iou, respectively. code is available at https://github.com/jarvis73/puco.",AB_0239
"background: cancer subtype classification is helpful for personalized cancer treat-ment. although, some approaches have been developed to classifying caner subtype based on high dimensional gene expression data, it is difficult to obtain satisfactory classification results. meanwhile, some cancers have been well studied and classified to some subtypes, which are adopt by most researchers. hence, this priori knowledge is significant for further identifying new meaningful subtypes.results: in this paper, we present a combined parallel random forest and autoencoder approach for cancer subtype identification based on high dimensional gene expression data, forestsubtype. forestsubtype first adopts the parallel rf and the priori knowledge of cancer subtype to train a module and extract significant candidate features. second, forestsubtype uses a random forest as the base module and ten parallel random forests to compute each feature weight and rank them separately. then, the intersection of the features with the larger weights output by the ten parallel random forests is taken as our subsequent candidate features. third, forestsubtype uses an autoencoder to condenses the selected features into a two-dimensional data. fourth, forestsubtype utilizes k-means++ to obtain new cancer subtype identifica-tion results. in this paper, the breast cancer gene expression data obtained from the cancer genome atlas are used for training and validation, and an independent breast cancer dataset from the molecular taxonomy of breast cancer international consor-tium is used for testing. additionally, we use two other cancer datasets for validat-ing the generalizability of forestsubtype. forestsubtype outperforms the other two methods in terms of the distribution of clusters, internal and external metric results. the open-source code is available at https://github.com/lffyd/forestsubtype.conclusions: our work shows that the combination of high-dimensional gene expression data and parallel random forests and autoencoder, guided by a priori knowledge, can identify new subtypes more effectively than existing methods of can-cer subtype classification.",AB_0239
"recently, convolutional neural networks (cnns) directly using whole slide images (wsis) for tumor diagnosis and analysis have attracted considerable attention, because they only utilize the slide-level label for model training without any additional annotations. however, it is still a challenging task to directly handle gigapixel wsis, due to the billions of pixels and intra-variations in each wsi. to overcome this problem, in this paper, we propose a novel end-to-end interpretable deep mil framework for wsi analysis, by using a two-branch deep neural network and a multi-scale representation attention mechanism to directly extract features from all patches of each wsi. specifically, we first divide each wsi into bag-, patch-and cell-level images, and then assign the slide-level label to its corresponding bag-level images, so that wsi classification becomes a mil problem. additionally, we design a novel multi-scale representation attention mechanism, and embed it into a two-branch deep network to simultaneously mine the bag with a correct label, the significant patches and their cell-level information. extensive experiments demonstrate the superior performance of the proposed framework over recent state-of-the-art methods, in term of classification accuracy and model interpretability. all source codes are released at : https://github.com/xhangchen/mran/.",AB_0239
"due to the cross-domain distribution shift aroused from diverse medical imaging systems, many deep learning segmentation methods fail to perform well on unseen data, which limits their real-world applicability. recent works have shown the benefits of extracting domain-invariant representations on domain generalization. however, the interpretability of domain-invariant features remains a great challenge. to address this problem, we propose an interpretable bayesian framework (bayeseg) through bayesian modeling of image and label statistics to enhance model generalizability for medical image segmentation. specifically, we first decompose an image into a spatial-correlated variable and a spatial-variant variable, assigning hierarchical bayesian priors to explicitly force them to model the domain-stable shape and domain-specific appearance information respectively. then, we model the segmentation as a locally smooth variable only related to the shape. finally, we develop a variational bayesian framework to infer the posterior distributions of these explainable variables. the framework is implemented with neural networks, and thus is referred to as deep bayesian segmentation. quantitative and qualitative experimental results on prostate segmentation and cardiac segmentation tasks have shown the effectiveness of our proposed method. moreover, we investigated the interpretability of bayeseg by explaining the posteriors and analyzed certain factors that affect the generalization ability through further ablation studies. our code is released via https://zmiclab.github.io/projects.html.",AB_0239
"in video captioning, many pioneering approaches have been developed to generate higher-quality captions by exploring and adding new video feature modalities. however, as the number of modalities increases, the negative interaction between them gradually reduces the gain of caption generation. to address this problem, we propose a three-layer hierarchical attention network based on a bidirectional decoding transformer that enhances multimodal features. in the first layer, we execute different encoders according to the characteristics of each modality to enhance the vector representation of each modality. then, in the second layer, we select keyframes from all sampled frames of the modality by calculating the attention value between the generated words and each frame of the modality. finally, in the third layer, we allocate weights to different modalities to reduce redundancy between them before generating the current word. additionally, we use a bidirectional decoder to consider the context of the ground-truth caption when generating captions. experiments on two mainstream benchmark datasets, msvd and msr-vtt, demonstrate the effectiveness of our proposed model. the model achieves state-of-the-art performance in significant metrics, and the generated sentences are more in line with human language habits. overall, our three-layer hierarchical attention network based on a bidirectional decoding transformer effectively enhances multimodal features and generates high-quality video captions. codes are available on https://github.com/nickchen121/mhan.",AB_0239
"the self-distillation methods can transfer the knowledge within the network itself to enhance the generalization ability of the network. however, due to the lack of spatially refined knowledge representations, current self-distillation methods can hardly be directly applied to object segmentation tasks. in this paper, we propose a novel self-distillation framework via pyramid knowledge representation and transfer for the object segmentation task. firstly, a lightweight inference network is built to perform pixel-wise prediction rapidly. secondly, a novel self-distillation method is proposed. to derive refined pixel-wise knowledge representations, the auxiliary self-distillation network via multi-level pyramid representation branches is built and appended to the inference network. a synergy distillation loss, which utilizes the top-down and consistency knowledge transfer paths, is presented to force more discriminative knowledge to be distilled into the inference network. consequently, the performance of the inference network is improved. experimental results on five datasets of object segmentation demon-strate that the proposed self-distillation method helps our inference network perform better segmentation effectiveness and efficiency than nine recent object segmentation network. furthermore, the proposed self-distillation method outperforms typical self-distillation methods. the source code is publicly available at https://github.com/xfflyer/skdforsegmentation.",AB_0239
"self-supervised learning (ssl) has achieved remarkable performance in various medical imaging tasks by dint of priors from massive unlabeled data. however, regarding a specific downstream task, there is still a lack of an instruction book on how to select suitable pretext tasks and implementation details throughout the standard pretrain-then-finetuneworkflow. in this work, we focus on exploiting the capacity of ssl in terms of four realistic and significant issues: (1) the impact of ssl on imbalanced datasets, (2) the network architecture, (3) the applicability of upstream tasks to downstream tasks and (4) the stacking effect of ssl and common policies for deep learning. we provide a large-scale, in-depth and fine-grained study through extensive experiments on predictive, contrastive, generative and multi-ssl algorithms. based on the results, we have uncovered several insights. positively, ssl advances class-imbalanced learning mainly by boosting the performance of the rare class, which is of interest to clinical diagnosis. unfortunately, ssl offers marginal or even negative returns in some cases, including severely imbalanced and relatively balanced data regimes, as well as combinations with common training policies. our intriguing findings provide practical guidelines for the usage of ssl in the medical context and highlight the need for developing universal pretext tasks to accommodate diverse application scenarios. the code of this paper can be found at https://github.com/endoluminalsurgicalvision-imr/medical-ssl.",AB_0239
