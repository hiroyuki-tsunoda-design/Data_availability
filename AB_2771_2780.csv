AB,NO
"hyperspectral image change detection (hsi-cd) aims to detect subtle changes on the earth's surface through approximately continuous spectral information, which has gradually become a very important research hotspot in the field of remote sensing (rs). in recent years, convolutional neural networks (cnns)-based hsi-cd methods have shown strong feature extraction capabilities. however, due to the simple fusion of spectral information in the channel dimension by cnn, the medium-and long-term sequence properties of spectral features cannot be well mined and represented. most previous studies mainly extract semantic features from images at different times, ignoring the temporal correlation between features, which cannot fully extract and effectively utilize temporal-spatial- spectral features. to this end, this article proposes a cascade spectral-aware transformer (cast) for hsi-cd. first, we pro-pose a temporal-spatial transformer (ts-former) to enhance the temporal correlation and spatial global relationship of extracted features, thereby addressing the insufficient consideration of temporal correlation. second, a spectral-awareness transformer (sa-former) is designed to better mine and represent the sequence properties of spectral features, especially the medium -and long-term dependencies. finally, we observe a spectral distortion in the process of extracting temporal-spatial features and, based on this, present a spectral constraint module (scm) to preserve the sequence properties of spectral features and reduce the distortion of the spectrum. extensive experiments on three challenging hyperspectral datasets demonstrate that our method achieves state-of-the-art results. the code is available at https://github.com/tianshunli/cast.",AB_0278
"the classification of ground objects from hyperspectral images (hsis) is of great importance for human perception of information about the terrain and landscape. hsis have numerous dimensions, and obtaining the data is difficult. the issue of slow convergence of neural network training is brought on by high-dimensional data, and the neural network's performance is impacted by the challenging data acquisition process. in order to achieve the effects of low data dependence and rapid convergence, we propose a redundancy elimination network architecture with a decoupled-gaze attention mechanism and phantom fractal modules (dgpf-renet) for hsi classification. first, we propose the decoupled-gaze attention (dga) mechanism to make full use of the correlation between adjacent bands and the continuity of neighboring pixels in hsis. then, a redundancy elimination module (rem) is proposed to reduce the number of feature points and eliminate redundant information while preserving the contextual information and relationships between pixels. finally, the phantom fractal module (pfm) is proposed, which improves the scale of feature learning by fractalizing convolutions at multiple scales. four publicly available hsis datasets, including indian pines, salinas, dfc2018, and whuhi-honghu, were used in our experiments. according to experimental findings, compared to other state-of-the-art methods, our method performs best with a small number of training samples and few iterations. we have released our code and models at https://github.com/yuhua666/dgpf-renet.",AB_0278
"multimedia-based recommendation is a challenging task that requires not only learning collaborative signals from user-item interaction, but also capturing modality-specific user interest clues from complex multimedia content. though significant progress on this challenge has been made, we argue that current solutions remain limited by multimodal noise contamination. specifically, a considerable proportion of multimedia content is irrelevant to the user preference, such as the background, overall layout, and brightness of images; the word order and semantic-free words in titles; etc. we take this irrelevant information as noise contamination to discover user preferences. moreover, most recent research has been conducted by graph learning. this means that noise is diffused into the user and item representations with the message propagation; the contamination influence is further amplified. to tackle this problem, we develop a novel framework named multimodal graph contrastive learning (mgcl), which captures collaborative signals from interactions and uses visual and textual modalities to respectively extract modality-specific user preference clues. the key idea of mgcl involves two aspects: first, to alleviate noise contamination during graph learning, we construct three parallel graph convolution networks to independently generate three types of user and item representations, containing collaborative signals, visual preference clues, and textual preference clues. second, to eliminate as much preference-independent noisy information as possible from the generated representations, we incorporate sufficient self-supervised signals into the model optimization with the help of contrastive learning, thus enhancing the expressiveness of the user and item representations. extensive experiments validate the effectiveness and scalability of mgcl at https://github.com/hfutmars/mgcl.",AB_0278
"pansharpening (which stands for panchromatic (pan) sharpening) involves the fusion between a multispectral (ms) image with a higher spectral content than a fine spatial resolution pan image to generate a high spatial resolution ms (hrms) image. a widely used concept is the construction of the relationship between pan and hrms images by designing pixel-based coefficients. previous pixel-based methods compute the coefficients pixel-by-pixel while suffering from inaccuracies in some areas leading to spatial distortion. however, we found that the coefficients inherit the spatial properties of the hrms image, e.g., the local smoothness and nonlocal self-similarity, and the spatial correlation between the coefficients and the hrms image can increase the accuracy of the estimation process. in this article, we propose a novel spatial fidelity with nonlocal regression (sfnlr) to describe the relationship between pan and hrms images. unlike from the pixel-based perspective, the sfnlr can jointly use the local smoothness and nonlocal self-similarity of the coefficients for preserving spatial information. besides, the sfnlr is integrated with a widely used spectral fidelity to formulate a new variational model for the pansharpening problem. an effective algorithm based on the alternating direction method of multiplier (admm) framework is designed to solve the proposed model. qualitative and quantitative assessments on reduced and full resolution datasets from different satellites demonstrate that the proposed approach outperforms several state-of-the-art methods. the code is available at: https://github.com/jin-liangxiao/sfnlr.",AB_0278
"deep learning-based (dl-based) change detection (cd) using remote sensing (rs) images has received increasing attention in recent years. however, how to effectively extract and fuse the deep features of bi-temporal images for improving the accuracy of cd is still a challenge. to address that, a novel adjacent-level feature fusion network with 3-d convolution (named afcf3d-net) is proposed in this article. first, through the inner fusion property of 3-d convolution, we design a new feature fusion way that can simultaneously extract and fuse the feature information from bi-temporal images. then, to alleviate the semantic gap between low-level features and high-level features, we propose an adjacent-level feature cross-fusion (afcf) module to aggregate complementary feature information between the adjacent levels. furthermore, the full-scale skip connection strategy is introduced to improve the capability of pixel-wise prediction and the compactness of changed objects in the results. finally, the proposed afcf3d-net has been validated on the three challenging rs cd datasets: the wuhan building dataset (whu-cd), the levir building dataset (levir-cd), and the sun yat-sen university dataset (sysu-cd). the results of quantitative analysis and qualitative comparison demonstrate that the proposed afcf3d-net achieves better performance compared to other state-of-the-art (sota) methods. the code for this work is available at https://github.com/wm-githuber/afcf3d-net.",AB_0278
"change detection (cd) plays an important role in earth surface analysis. current cd methods have achieved good performance in large flat areas, but cd of detailed parts is still a great challenge, and the loss of detail causes many faults around the change boundaries and on small objects. by analyzing the feature map of the widely used u-net architecture in existing methods, we ascribe the detail loss to the depletion of detailed features during the top-to-down delivery in the u-net architecture. the feature refine cd (frcd) model is proposed in which the detection results are predicted directly from the multiscale features instead of the u-net architecture. by direct prediction, the representation ability of details is enhanced, and thus the detection accuracy (acc) of boundaries and small objects improves. moreover, the normal upsampling in direct prediction is replaced with the deformable upsampling, which delivers detailed information from the low-level to the high-level via the deformable convolution, allowing the results to further fit boundaries in the frcd model. experimental results on two datasets confirm the effectiveness of frcd compared to the state-of-the-art methods, and the cd results of boundaries and small objects are improved significantly by the proposed method. code will be available after the acceptance of the letter in https://github.com/ijnokml/cdfr.",AB_0278
"understanding subglacial bed topography is essential for learning about antarctica in the geologic and glaciological fields. the primary method of investigating the antarctic bed involves measuring the bed elevation by radio-echo sounding (res) deployed on aircraft. digital elevation models (dems) of the antarctic bed generated by traditional interpolation methods usually lack resolution, precision, and roughness. to generate antarctic bed dems by interpolating sparse res bed elevation data, we use a two-stage coarse-to-fine fully convolutional neural network (cnn), which presents a deep generative elevation inpainting method that can extract, use in-depth features, and reconstruct the bed elevation conforming to the textural character of deglacial landscapes. our method can generate a detailed and reasonable bed dem with the full calculation of cnn and the training strategy of a generative adversarial network (gan). the quantitative evaluation results show that a 250-m resolution elevation grid map with a 77-m mean absolute error (mae) can be generated through elevation inpainting by sparse data with 4-km res survey spacing in the arctic test area. our study also generates two realistic bed dems with a 250-m spatial resolution in the gamburtsev subglacial mountains and amundsen sea embayment. compared with the existing antarctic bed dem products, bedmachine_antarctica, deepbedmap_dem, and mb_deepbedmap_dem, our generated bed dems show more realistic terrain and elevation with low maes in test regions, which could better suit follow-up glaciological research. the code of this work will be available at https://github.com/hecian/gei_2022 for the sake of reproducibility.",AB_0278
"both local and global context dependencies are essential for building extraction from remote sensing (rs) images. convolutional neural network (cnn) can extract local spatial details well but lacks the ability to model long-range dependency. in recent years, vision transformer (vit) has shown great potential in modeling global context dependency. however, it usually brings huge computational cost, and spatial details cannot be fully retained in the process of feature extraction. to maximize the advantages of cnns and vits, we propose dual spatial attention transformer net (dsat-net), which combines them in one model. in dsat-net, we design an efficient dual spatial attention transformer (dsaformer) to solve the defects of standard vit. it has a dual attention structure to complement each other. specifically, the global attention path (gap) conducts large-scale downsampling of the feature maps before the global self-attention (sa) computing, to reduce the computational cost. the local attention path (lap) uses efficient stripe convolution to generate local attention, which can alleviate the loss of information caused by downsampling operation in the gap and supplement the spatial details. in addition, we design a feature refining module called channel mixing feature refine module (cm-frm) to fuse low- and high-level features. our model achieved competitive results on three public building extraction datasets. the code will be available at https://github.com/stdcoutzrh/buildingextraction.",AB_0278
"hyperspectral target detection (htd) is an important issue in earth observation, with applications in both military and civilian domains. however, conventional representation-based detectors are hindered by the reliance on the unknown background dictionary, the limited ability to capture nonlinear representations using the linear mixture model (lmm), and the insufficient background-target recognition based on handcrafted priors. to address these problems, this article proposes an interpretable representation network that intuitively realizes lmm for htd, making nonlinear feature expression and physical interpretability compatible. specifically, a subspace representation network is designed to separate the background and target components, where the background subspace can be adaptively learned. in addition, to further enhance the nonlinear representation and more accurately learn the coefficients, a lightweight multiscale transformer is proposed by modeling long-distance feature dependencies between channels. furthermore, to supplement the depiction for target-background discrimination, a constrained energy minimization (cem) loss is tailored by minimizing the output background energy and maximizing the target response. the effectiveness of the proposed method is demonstrated on four benchmark datasets, showing its superiority over state-of-the-art methods. the code for this work is available at https://github.com/shendb2022/htd-irn for reproducibility purposes.",AB_0278
"real-time eyeblink detection in the wild is a recently emerged challenging task that suffers from dramatic variations in face attribute, pose, illumination, camera view and distance, etc. one key issue is to well characterize eyelid's intrinsic motion (i.e., approaching and departure between upper and lower eyelid) robustly, under unconstrained conditions. towards this, a novel eyelid's intrinsic motion-aware feature learning approach is proposed. our proposition lies in 3 folds. first, the feature extractor is led to focus on informative eye region adaptively via introducing visual attention in a coarse-to-fine way, to guarantee robustness and fine-grained descriptive ability jointly. then, 2 constraints are proposed to make feature learning be aware of eyelid's intrinsic motion. particularly, one concerns the fact that the inter-frame feature divergence within eyeblink processes should be greater than non-eyeblink ones to better reveal eyelid's intrinsic motion. the other constraint minimizes the inter-frame feature divergence of non-eyeblink samples, to suppress motion clues due to head or camera movement, illumination change, etc. meanwhile, concerning the high ambiguity between eyeblink and non-eyeblink samples, soft sample labels are acquired via self-knowledge distillation to conduct feature learning with finer supervision than the hard ones. the experiments verify that, our proposition is significantly superior to the state-of-the-art ones (i.e., advantage on f1-score over 7%) and with real-time running efficiency. it is also of strong generalization capacity towards constrained conditions. the source code is available at https://github.com/wenzhengzeng/blink_eyelid.",AB_0278
