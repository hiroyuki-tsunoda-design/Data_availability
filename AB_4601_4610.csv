AB,NO
"high-throughput chemical analysis of natural productmixtures lagsbehind developments in genome sequencing technologies and laboratoryautomation, leading to a disconnect between library-scale chemicaland biological profiling that limits new molecule discovery. here,we report a new orthogonal sample multiplexing strategy that can increasemass spectrometry-based profiling up to 30-fold over traditional methods.profiled pooled samples undergo subsequent computational deconvolutionto reconstruct peak lists for each sample in the set. we validatedthis approach using in silico experiments and demonstrateda high assignment precision (>97%) for large, pooled samples (r = 30), particularly for infrequently occurring metabolitesof relevance in drug discovery applications. requiring only 5% ofthe previously required ms acquisition time, this approach was repeatedin a recent biological activity profiling study on 925 natural productextracts, leading to the rediscovery of all previously reported bioactivemetabolites. this new method is compatible with ms data from any instrumentvendor and is supported by an open-source software package: https://github.com/liningtonlab/multiplexms.",AB_0461
"we introduce 3dshape2vecset, a novel shape representation for neural fields designed for generative diffusion models. our shape representation can encode 3d shapes given as surface models or point clouds, and represents them as neural fields. the concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. our new representation encodes neural fields on top of a set of vectors. we draw from multiple concepts, such as the radial basis function representation, and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. our results show improved performance in 3d shape encodof generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation. code: https://1zb.github.io/3dshape2vecset/.",AB_0461
"the radiation characteristics of rotor-beam interaction noise are studied experimentally for low reynolds number smallscale rotors in interaction with beams of different shapes, sizes, and downstream positions. the number of blades ranges from two to four. for the two-bladed rotor, the presence of the beam has no effect on the mean aerodynamic performance. moreover, the blade passing frequency (bpf) and the high frequency broadband noise (bbn) appear not to be affected by the presence of the beam. on the contrary, the magnitude of the 2x bpf-25xbpf harmonics increases up to 30 db compared to the case without beam, with an envelope consisting of two humps: one centered around 5xbpf and another around 20xbpf-25xbpf. for the first hump, a dipole-like pattern with minimal amplitude aligned with the beam can be observed, whereas another dipole-like pattern is observed for the higher frequency hump, but with a minimal amplitude over all the rotor disk plane. compared to the two-bladed rotor, the presence of the beam has an effect on the mean aerodynamic performance of the three- and four-bladed rotors, increasing both the torque and the thrust at isorotational speed. this change leads to a change in the directivity of the bpf tone that decreases at a latitude angle of theta = 0 degrees and increases at a latitude angle of theta = 40 degrees. moreover, the same two competing humps are observed on the bpf harmonics envelope. interestingly, the frequency range over which an amplification of the harmonic magnitude is observed seems not to be influenced by the number of blades. finally, the magnitude of the low frequency hump increases with the beam diameter, the rotational speed, and the number of blades but decreases with the rotor-beam distance. that of the high frequency hump increases also with the rotational speed and the number of blades, but not anymore with the beam diameter, and reaches a maximum value when the rotor-beam distance is at an intermediate distance of l = 25mm. this hump is also influenced, to a lesser extent, by the shape of the beam. the two different evolutions permit us to conclude that the noise generation mechanisms leading to the two humps must be different. scaling laws of the acoustical energy are derived for all those parameters. as already done for previous experiments without beam, all of the results are made available as an open database, at https://dataverse.isae-supaero.fr/. (c) 2023 author(s). all article content, except where otherwise noted, is licensed under a creative commons attribution (cc by) license ().",AB_0461
"we present a system that learns diverse, physically simulated tennis skills from large-scale demonstrations of tennis play harvested from broadcast videos. our approach is built upon hierarchical models, combining a low-level imitation policy and a high-level motion planning policy to steer the character in a motion embedding learned from broadcast videos. when deployed at scale on large video collections that encompass a vast set of examples of real-world tennis play, our approach can learn complex tennis shotmaking skills and realistically chain together multiple shots into extended rallies, using only simple rewards and without explicit annotations of stroke types. to address the low quality of motions extracted from broadcast videos, we correct estimated motion with physics-based imitation, and use a hybrid control policy that overrides erroneous aspects of the learned motion embedding with corrections predicted by the high-level policy. we demonstrate that our system produces controllers for physically-simulated tennis players that can hit the incoming ball to target positions accurately using a diverse array of strokes (serves, forehands, and backhands), spins (topspins and slices), and playing styles (one/two-handed backhands, left/right-handed play). overall, our system can synthesize two physically simulated characters playing extended tennis rallies with simulated racket and ball dynamics. code and data for this work is available at https://research.nvidia.com/labs/toronto-ai/vid2player3d/.",AB_0461
"commonsense knowledge (csk) about concepts and their properties is helpful for ai applications. prior works, such as conceptnet, have compiled large csk collections. however, they are restricted in their expressiveness to subject-predicate-object (spo) triples with simple concepts for s and strings for p and o. this paper presents a method called ascent++ to automatically build a large-scale knowledge base (kb) of csk assertions, with refined expressiveness and both better precision and recall than prior works. ascent++ goes beyond spo triples by capturing composite concepts with subgroups and aspects, and by refining assertions with semantic facets. the latter is essential to express the temporal and spatial validity of assertions and further qualifiers. furthermore, ascent++ combines open information extraction (openie) with judicious cleaning and ranking by typicality and saliency scores. for high coverage, our method taps into the large-scale crawl c4 with broad web contents. the evaluation with human judgments shows the superior quality of the ascent++ kb, and an extrinsic evaluation for qa-support tasks underlines the benefits of ascent++. a web interface, data, and code can be accessed at https://ascentpp.mpi-inf.mpg.de/.",AB_0461
"objective the aim of this initiative was to develop a ranked list of hydrocephalus research priorities as determined by the hydrocephalus patient community in conjunction with the healthcare and scientific community.methods using the validated methodology published by the james lind alliance (jla), the hydrocephalus associa-tion (ha) administered two surveys and hosted a final prioritization workshop. survey one solicited open-ended re- sponses from the community. from these responses, a long list of priority statements was developed. this list was then consolidated into a short list of research priority statements, which, after a nonsystematic literature review, were verified as being research uncertainties. survey two asked the community members to select their top 10 priorities from the short list. the final prioritization leading to a final ranked top 20 list of hydrocephalus research priorities took place at a virtual workshop led by a team of trained facilitators, by means of an iterative process of consensus building.results from survey one, 3703 responses from 890 respondents were collected, leading to a long list of 146 priority statements. the consolidated short list contained 49 research priority statements, all of which were verified as uncertain- ties in hydrocephalus research. from an analysis of survey two responses, the top 21 research priority statements were determined. a consensus on these statements was reached at the virtual workshop, leading to a final ranked top 20 list of hydrocephalus research priorities, within which needs were apparent in several areas: development of noninvasive and/or one-time therapies, reduction of the burden of current treatments, improvement of the screening and diagnosis of hydrocephalus, improved quality of life, and improved access to care.conclusions by gathering extensive input from the hydrocephalus community and using an iterative process of con-sensus building, a ranked list of the top 20 hydrocephalus research priorities was developed. the ha will use this ranked list to guide future research programs and encourages the healthcare and scientific community to do the same. https://thejns.org/doi/abs/10.3171/2022.10.jns22753",AB_0461
"mass spectrometryimaging (msi) techniques generate data that revealspatial distributions of molecules on a surface with high sensitivityand selectivity. however, processing large volumes of mass spectrometrydata into useful ion images is not trivial. furthermore, data frommsi techniques using continuous ionization sources where data areacquired in line scans require different data handling strategiescompared to data collected from pulsed ionization sources where dataare acquired in grids. in addition, for continuous ionization sources,the pixel dimensions are influenced by the mass spectrometer dutycycle, which, in turn, can be controlled by the automatic gain control(agc) for each spectrum (pixel). currently, there is a lack of data-handlingsoftware for msi data generated with continuous ionization sourcesand agc. here, we present ion-to-image (i2i), which is a matlab-basedapplication for msi data acquired with continuous ionization sources,agc, high resolution, and one or several scan filters. the sourcecode and a compiled installer are available at https://github.com/lanekofflab/i2i. the application includes both quantitative, targeted, and nontargeteddata processing strategies and enables complex data sets to be processedin minutes. the i2i application has high flexibility for generating,processing, and exporting msi data both from simple full scans andmore complex scan functions interlacing msn and sim scandata sets, and we anticipate that it will become a valuable additionto the existing msi software toolbox.",AB_0461
"mapping and localization using surface features is prone to failure due to environment changes such as inclement weather. recently, localizing ground penetrating radar (lgpr) has been proposed as an alternative means of localizing using underground features that are stable over time and less affected by surface conditions. however, due to the lack of commercially available lgpr sensors, the wider research community has been largely unable to replicate this work or build new and innovative solutions. we present grounded, an open dataset of lgpr scans collected in a variety of environments and weather conditions. by labeling these data with ground truth localization from an rtk-gps/inertial navigation system, and carefully calibrating and time-synchronizing the radar scans with ground truth positions, camera imagery, and lidar data, we enable researchers to build novel localization solutions that are resilient to changing surface conditions. we include 108 individual runs totaling 450 km of driving with lgpr, gps, odometry, camera, and lidar measurements. we also present two new evaluation benchmarks for 1) localizing in weather and 2) multi-lane localization, to enable comparisons of future work supported by the dataset. additionally, we present a first application of the new dataset in the form of lgprnet: an inception-based cnn architecture for learning localization that is resilient to changing weather conditions. the dataset can be accessed at http://lgprdata.com.",AB_0461
"saharan dust outbreaks have profound effects on ecosystems, climate, human health, and the cryosphere in europe. however, the spatial deposition pattern of saharan dust is poorly known due to a sparse network of ground measurements. following the extreme dust deposition event of february 2021 across europe, a citizen science campaign was launched to sample dust on snow over the pyrenees and the european alps. this somewhat improvised campaign triggered wide interest since 152 samples were collected from the snow in the pyrenees, the french alps, and the swiss alps in less than 4 weeks. among the 152 samples, 113 in total could be analysed, corresponding to 70 different locations. the analysis of the samples showed a large variability in the dust properties and amount. we found a decrease in the deposited mass and particle sizes with distance from the source along the transport path. this spatial trend was also evident in the elemental composition of the dust as the iron mass fraction decreased from 11% in the pyrenees to 2% in the swiss alps. at the local scale, we found a higher dust mass on south-facing slopes, in agreement with estimates from high-resolution remote sensing data. this unique dataset, which resulted from the collaboration of several research laboratories and citizens, is provided as an open dataset to benefit a large community and to enable further scientific investigations. data presented in this study are available at https://doi.org/10.5281/zenodo.7969515 (dumont et al., 2022a).",AB_0461
"the project optimus initiative by the fda's oncology center of excellence is widely viewed as a groundbreaking effort to change the status quo of conventional dose-finding strategies in oncology. unlike in other therapeutic areas where multiple doses are evaluated thoroughly in dose ranging studies, early-phase oncology dose-finding studies are characterized by the practice of identifying a single dose, such as the maximum tolerated dose (mtd) or the recommended phase 2 dose (rp2d). following the spirit of project optimus, we propose an multi-arm two-stage (mats) design for proof-of-concept (poc) and dose optimization that allows the evaluation of two selected doses from a dose-escalation trial. the design assesses the higher dose first across multiple indications in the first stage, and adaptively enters the second stage for an indication if the higher dose exhibits promising antitumor activities. in the second stage, a randomized comparison between the higher and lower doses is conducted to achieve poc and dose optimization. a bayesian hierarchical model governs the statistical inference and decision making by borrowing information across doses, indications, and stages. our simulation studies show that the proposed mats design yield desirable performance. an r shiny application has been developed and made available at https://matsdesign.shinyapps.io/mats/.",AB_0461
