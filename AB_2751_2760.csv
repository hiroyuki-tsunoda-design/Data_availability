AB,NO
"deep domain adaptation has achieved promising results in cross-domain hyperspectral image (hsi) classification. however, existing methods often focus on aligning data distributions without sufficient consideration of separability of source and target domain data themselves. in addition, current adversarial domain adaptation methods aim to achieve similar distributions between domains by confusing the discriminator, rather than obtaining a more compact distribution. in particular, existing methods are not discriminative enough for the target domain due to the difficulty of obtaining high-confidence labeled samples of the target domain. to address the above challenges, we propose a supervised contrastive learning (scl)-based unsupervised domain adaptation for hsi classification. an scl strategy is then performed in both the source and target domains, which allows samples from the same category to be pulled closer together and samples from different categories to be pushed further apart, thus enhancing the separability of the data within the domain. the domain adaptation task is treated as a one-class classification (occ) task, and a novel domain similarity loss based on occ is introduced to reduce the discrepancy between domains. finally, a confidence learning-based sample selection strategy is designed to select high-confidence labeled samples from the target domain to fine-tune the domain adaptation model, which can enhance the discrimination of the model to the target domain. experimental results on three cross-domain datasets demonstrate that our proposed method outperforms existing domain adaptation methods. our source code is available at https://github.com/li-zk/scluda-2023.",AB_0276
"change detection (cd) is an important earth observation task that can monitor change areas at two times from the view of space. however, fully supervised cd has a heavy dependence on numerous manually labeled data, limiting their applications in practice. beyond the fully supervised setting, semi-supervised cd (sscd), which uses a few labeled data to guide the unsupervised learning of dominant unlabeled data, has attracted increasing attention for its significant advantage in alleviating the demand for annotations. to this end, in this article we propose a joint self-training and rebalanced consistency learning (st-rcl) framework for sscd, which consists of a basic supervised branch for the labeled data and a novel unsupervised branch for the unlabeled data. to make full use of the unlabeled data, the unsupervised branch generates pseudolabels from weakly augmented unlabeled remote sensing image (rsi) pairs to supervise the cd of two strongly augmented counterparts, including an unrotated version and a rotated version. on one hand, the unrotated unlabeled rsi pairs are pseudosupervised with the pseudolabels by confidence-based self-training (st). on the other hand, to further enhance model robustness to rotation nonequivariance and imbalanced distribution, the predictions of rotated unlabeled rsi pairs are aligned to the pseudolabels by a well-designed rebalanced consistency learning (rcl) strategy based on uncertainty-based class weighting. extensive experiments are performed on four widely used cd datasets, and the proposed st-rcl yields new state-of-the-art results on all these datasets in comparison to some other sscd methods, demonstrating its effectiveness and generalization. our code will be available at https://github.com/zxt9/strcl-sscd.",AB_0276
"hyperspectral image (hsi) classification with limited training samples has been well studied in recent years. among them, the few-shot learning (fsl) technique demonstrates excellent processing capability under limited labeled samples. nevertheless, the current fsl-based works provide scarce attention to effective class prototypes and metric types, resulting in high generalization error and poor interpretation during the cross-domain testing phase. a dual-adjustment mode-based cross-domain meta-learning (dmcm) method for few-shot hsi classification is proposed to tackle this issue. specifically, a three-dimensional ghost attention network (tgan) with strong learning capability without massive parameters is first constructed. meanwhile, a dual-adjustment mode comprising intracorrection (ic) and interalignment (ia) learning strategies is then adopted to solve domain shift issue via episode-level meta-tasks, where ic and ia focus on effective class prototypes and data distribution differences between domains, respectively. afterward, considering that the traditional euclidean distance metric is insensitive to the distribution of within-class samples, the class-covariance metric (ccm) is employed to account for the distribution in feature space of each class to optimize decision boundary and alleviate the misclassification problem. extensive experiments on three publicly available target hyperspectral datasets demonstrate the effectiveness of the proposed method in comparison with other state-of-the-art (sota) methods. the codes will be available on the website at https://github.com/hlevag/dmcm.git.",AB_0276
"hyperspectral images (hsis) are typically utilized in a wide variety of practical applications. hsi is replete with spatial and spectral information, which provides precise data for material detection. hsis are characterized by a high degree of variations and undesirable pixel distributions, providing major processing challenges. this article introduces the fast hyperspectral image classification (fhic) model, a rapid model for classifying hsis and resolving their associated challenges. it uses the enhancing transformation reduction (etr) method to address the hsi difficulties and enhance classes' differentiation. it also uses exponential linear units (elus) to smooth and speed the classification processing. the structure of the fhic model is designed to be very flexible and suitable for a range of hsis. the model reduced execution time and ram consumption, and provided superior performance compared to seven of the most advanced analysis models for three well-known hsis. in some cases, it was 60% faster than other models. in addition, this work presents a new and highly effective method for measuring the performance of the compared models in terms of their accuracy and processing speed to provide an easy evaluation method. the code of the fhic model is available at this link: https://github.com/dalalal-alimi/fhic.",AB_0276
"using high-fidelity image compression makes it possible to transmit remote-sensing images in real-time. nevertheless, existing lossy remote-sensing image compression (rsic) methods have some inherent potential issues, including blocking and blurring effects, which are particularly problematic in low-compression-ratio (cr) settings. although numerous methods have been studied to address the aforementioned issue, the majority of them exploit the prior of local smoothness in images, which usually induces the over-smoothing of regions with noticeable structure (i.e., edges and textures). during this task, we developed an innovative end-to-end framework that enables high-fidelity rsic while retaining sharp edge and texture information. initially, we put forth an edge-guided adversarial network (ega-net) for simultaneously restoring edge structures and generating texture details. second, we impose an edge fidelity constraint to direct our network to optimize image content and structural information jointly. in addition, to facilitate this task, we have constructed a large-scale rsic dataset named nwpu-rs-compression (nwpu-rsc). this dataset contains over 300000 images of 30 categories, all with a fixed resolution of 600 x 600. finally, a new quantitative metric for full reference image quality that takes into account signal statistics and the characteristics of the human visual system (hvs) has been developed, which helps evaluate reconstructed remote-sensing images more objectively and accurately. experimental evidence has demonstrated that the ega-net surpasses several representative compression approaches regarding quality metrics on the nwpu-rsc, aid, and ispr vaihingen datasets. code, dataset, and more experimental results can be accessed at https://github.com/chenxi1510/remote-sensing-image-compression.",AB_0276
"single hyperspectral image (hsi) super-resolution (sr) is an important topic in the remote-sensing field. however, existing hsi sr methods mainly use the feed-forward upsampling technique and convolutional neural network (cnn) to learn the feature representation, failing to learn the complex mapping relationship between low-resolution (lr) and high-resolution (hr) and long-range joint spectral and spatial features. to address this issue, in this article, we propose the spatial-spectral interactive transformer u-net with alternating sampling (as(3)itransunet) for the hsi sr task. in this method, to mitigate the computational burden resulting from the high spectral dimension of the hsi, a group reconstruction strategy is adopted. to effectively explore the hierarchical features of the hsi, the u-net with alternating upsampling and downsampling is designed that allocates the task of learning the complex mapping relationship to each stage of the u-net. to fully extract the spatial-spectral features of the hsi, we propose the spatial-spectral interactive transformer (ssit) block and integrate it into the encoder and the decoder of u-net. the ssit block contains a cross-branch bidirectional interaction module, which further captures the complementary information between spatial and spectral dimensions. moreover, multistage complementary information learning (mscil) is proposed to capture the complementary information in the adjacent hsi groups for recovering the absent details in the current hsi group. the experiments on the three benchmark datasets demonstrate that the proposed as(3)itransunet can effectively improve the spatial resolution and preserve the spectral information at different scales. models and codes are available at https://github.com/liushiji666/as3-itransunet.",AB_0276
"vehicle detection methods based on deep learning have achieved remarkable results on remote-sensing images. however, the performance of the detector degrades when the test images are distinct from the training images. domain adaptive vehicle detection is a promising approach to bridging the domain gap. existing methods usually adopt fully shared networks, but ignore the problem that features from different domains may be incompatible within a single model. in this letter, we present a novel domain adaptive vehicle detection method based on patch-wise domain-specific channel recalibration (pdscr). the pdscr module routes the feature to the corresponding network branch and extracts the channel dependence using separate parameters. in this way, our method can explicitly capture domain-specific information for each domain. furthermore, we propose a dynamic weighted prototype alignment (dwpa) to avoid the negative effects of false pseudo-labels, especially in the early stage of training. experimental results of adaptation from our synthetic dataset to three real vehicle detection datasets demonstrate the effectiveness of our method. code and our synthetic data will be available at https://weix-liu.github.io/.",AB_0276
"visual question answering (vqa) aims to build an interactive system that infers the answer according to the input image and text-based question. recently, vqa for remote sensing has attracted considerable attention since it is essential and expedient for monitoring global resources and querying objective attributes. in reality, question-related semantic information is helpful for reasoning and understanding capabilities, which is always contained in remote sensing images or complex questions. to capture the valuable information and extend the applications of remote-sensing vqa (rsvqa), we propose an end-to-end multistep question-driven vqa (mqvqa) system for remote sensing. in mqvqa, we employ a multiple-step attention mechanism to interactively reason and concomitantly mark the region that is most related to the question. to understand the semantic information in complex questions, we build a question-driven module that classifies the question types and keywords, which will further guide the combination of image feature maps in different scales. to benchmark this model, we construct a new complex remote sensing vqa (crsvqa) dataset, wherein the question is asked in complex forms and involves various remote sensing scenes. the evaluation results on crsvqa, rsvqa, and remote sensing imagery vqa (rsivqa) datasets indicate that the proposed mqvqa model surpasses other rsvqa models. the visualization results demonstrate that mqvqa has a robust ability in reasoning and understanding the content from images and complex questions. our code and dataset are publicly available at: https://github.com/meimeizhang-data/mqvqa.",AB_0276
"convolutional neural networks (cnns) are powerful in extracting local information but lack the ability to model long-range dependencies. in contrast, the transformer relies on multihead self-attention mechanisms to effectively extract the global contextual information and thus model long-range dependencies. in this article, we propose a novel encoder-decoder structured semantic segmentation network, named cnn and multiscale transformer fusion network (cmtfnet), to extract and fuse local information and multiscale global contextual information of high-resolution remote-sensing images. specifically, to further process the output features from the cnn encoder, we build a transformer decoder based on the multiscale multihead self-attention (m2sa) module for extracting rich multiscale global contextual information and channel information. additionally, the transformer block introduces an efficient feed-forward network (e-ffn) to enhance the information interaction between different channels of the feature. finally, the multiscale attention fusion (maf) module fully fuses the feature information from different levels. we have conducted extensive comparison experiments and ablation experiments on the international society for photogrammetry and remote sensing (isprs) vaihingen and potsdam datasets. the extensive experimental results demonstrate that our proposed cmtfnet can obtain superior performance compared to the currently popular methods. the codes will be available at https://github.com/drwuhonglin/cmtfnet.",AB_0276
"convolution and self-attention are two powerful techniques for multisource remote sensing (rs) data fusion that have been widely adopted in earth observation tasks. however, convolutional neural networks (cnns) are inadequate for fully mining contextual information and representing the sequence attributes of spectral signatures. in addition, the specific self-attention mechanism often comes with high-computational costs, which hinders its application in the field of rs. to overcome the above limitations, this article proposes a unified framework called mixing self-attention and convolution network (macn) for comprehensive feature extraction and efficient feature fusion. first, the proposed macn utilizes two adaptive cnn encoders (aces) to extract shallow convolutional features from multisource rs data. second, taking the complexity and varying scales of rs data into account, the proposed mixing self-attention and convolution transformer (mact) layer achieves local and global multiscale perception through an elegant integration of self-attention and convolution. mact can extract abundant spatial and high-dimensional information (e.g., spectral and elevation information) while maintaining minimal computational overhead compared with pure convolution or self-attention counterparts. finally, a multisource cross-guided fusion (mcgf) module is designed to achieve deep fusion of multisource rs data features. mcgf utilizes a carefully designed cross-modal attention mechanism to capture the interaction between multisource data and aggregate contextual information. extensive tests on six public rs datasets have shown that our method outperforms other multisource fusion models, delivering state-of-the-art (sota) results on multiple rs data fusion tasks without specific tuning. the source code of the proposed method will be available publicly at https://github.com/like413/macn.",AB_0276
