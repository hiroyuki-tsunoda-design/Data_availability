AB,NO
"we present the first open-source, gpu-based code for complex plasmas. the code, opendust, pursues to provide researchers, both experimenters and theorists, a user-friendly and high-performance tool for self-consistent calculation forces, acting on microparticles, and microparticles' charges in a plasma flow. opendust performance originates from a highly-optimized cuda back-end and performs self-consistent calculations of plasma flow around microparticles in seconds. this code outperforms all available codes for self-consistent complex plasma simulation. moreover, opendust can also be used for the simulation of larger systems of dust microparticles, which was previously unavailable. opendust interface is written in python, which provides ease-of-use and simple installation from the conda repository.program summary program title: opendust cpc library link to program files: https://doi .org /10 .17632 /bs7rthk29w.1 developer's repository link: https://github .com /kolotinsky1998 /opendust code ocean capsule: https://codeocean .com /capsule /2557151 licensing provisions: mit programming language: python nature of problem: gpu cards can significantly speedup self-consistent calculations of forces, acting on microparticles in a plasma flow. however, the available codes are cpu-based, or not provided as a package that can also be easily used. therefore, researchers need to spend much time writing their own codes or use less effective ones. solution method: development of a highly-optimized gpu-accelerated library for self-consistent simula-tions of streaming plasma around microparticles. the functionality of the library is available through a python interface, which makes it easy to use.(c) 2023 elsevier b.v. all rights reserved.",AB_0563
"background: the management of invasive tamarix genotypes depends on reliable and accurate information of their extent and distribution. this study investigated the utility of the multispectral sentinel-2 imageries to map infestations of the invasive tamarix along three riparian ecosystems in the western cape province of south africa. methods: the sentinel-2 image was acquired from the glovis website (http://glovis. usgs.gov/). random forest (rf) and support vector machine (svm) algorithms were used to classify and estimate the spatial distribution of invasive tamarix genotypes and other land-cover types in three riparian zones viz. the leeu, swart and olifants rivers. a total of 888 reference points comprising of actual 86 gps points and additional 802 points digitized using the google earth pro free software were used to ground-truth the sentinel-2 image classification. results: the results showed the random forest classification produced an overall accuracy of 87.83% (with kappa value of 0.85), while svm achieved an overall accuracy of 86.31% with kappa value of 0.83. the classification results revealed that the tamarix invasion was more rampant along the olifants river near de rust with a spatial distribution of 913.39 and 857.74 ha based on the rf and svm classifiers, respectively followed by the swart river with tamarix coverage of 420.06 ha and 715.46 hectares, respectively. the smallest extent of tamarix invasion with only 113.52 and 74.27 hectares for svm and rf, respectively was found in the leeu river. considering the overall accuracy of 85% as the lowest benchmark for a robust classification, the results obtained in this study suggests that the svm and rf classification of the sentinel-2 imageries were effective and suitable to map invasive tamarix genotypes and discriminate them from other land-cover types.",AB_0563
"deep representation learning in image classification is an area in computer vision where deep convolutional neural networks (cnns) have flourished. nevertheless, developing an efficient image recognition model for real world applications is a challenging task, since image datasets are characterized by instances with a large amount of noise and redundant information. thus, it is essential to incorporate an intelligent feature extraction and filtering method in order to create robust and efficient image representations. in this work, we propose a multi-view-cnn framework which drastically boosts the performance of pre-trained cnn models, such as resnet and vgg in image classification applications. in this approach different type of views of the same initial image are used in order to extract different types of features utilizing pre-trained cnn models. however, in order to reduce the huge dimensional size of the raw cnn's output features and create a robust image representation, the principal component analysis (pca) dimension reduction method is applied. then, all these extracted feature vectors are concatenated building a final composite feature representation of the initial image dataset. finally, this augmented feature vector is used for training a linear model (logistic regression) in order to perform the final classification tasks. the main findings of this work are summarized as follows. first, the proposed multi-view-cnn framework managed to drastically increase the performance results of pre-trained cnn models. second, the incorporation of pca as a final layer into the main cnn topology, instead of using the classical dimension reduction layer components such as averaging and max pooling operations, managed to significantly improve the performance. the whole implementation code of this framework alongside with the datasets used in our experimental simulations was uploaded to our public github repository to the following link: https://github.com/emmanuelpintelas/a-multi-view-cnn-framework-for-deep-representation-learning-in-image-classification.",AB_0563
"to solve recurring problems in drug discovery, matched molecular pair (mmp) analysis is used to understand relationships between chemical structure and function. for the mmp analysis of large data sets (>10,000 compounds), available tools lack flexible search and visualization functionality and require computational expertise. here, we present matcher, an open-source application for mmp analysis, with novel search algorithms and fully automated querying-to-visualization that requires no programming expertise. matcher enables unprecedented control over the search and clustering of mmp transformations based on both variable fragment and constant environment structure, which is critical for disentangling relevant and irrelevant data to a given problem. users can exert such control through a built-in chemical sketcher and with a few mouse clicks can navigate between resulting mmp transformations, statistics, property distribution graphs, and structures with raw experimental data, for confident and accelerated decision making. matcher can be used with any collection of structure/property data; here, we demonstrate usage with a public chembl data set of about 20,000 small molecules with cyp3a4 and/or herg inhibition data. users can reproduce all examples demonstrated herein via unique links within matcher's interface-a functionality that anyone can use to preserve and share their own analyses. matcher and all its dependencies are open-source, can be used for free, and are available with containerized deployment from code at https://github.com/merck/matcher. matcher makes large structure/property data sets more transparent than ever before and accelerates the data-driven solution of common problems in drug discovery.",AB_0563
"gearboxes are integral elements in rotating machines and have a high tendency to fail due to their operation in harsh conditions. a robust method to estimate the fault size of gears is desirable for a successful prognostic process, which is, to date, still unavailable in the literature. the fault size can be estimated by vibration analysis, using signal processing and machine-learning tools. however, the availability of labeled or unlabeled vibration signals from faulty rotating machinery components is rare, making it challenging to apply machine-learning algorithms. therefore, some physical pre-knowledge should be incorporated in the model for a successful learning process. this can be done by exposing the learning model to simulated data, and by a physical pre-processing procedure. this paper suggests a novel algorithm to overcome the lack of faulty data (labeled and unlabeled), and it is trained on a combination of simulated data and some real data. the algorithm tunes the differences between simulation and experiment using one faulty experimental example, and transfers knowledge from simulation to reality by addressing the transfer function effects. it addresses the transfer function by spectrum background estimation and minimum phase estimation while also selecting features that are invariant to the unmitigated effects of the transfer function. the new algorithm is demonstrated on simulated signals and measured transfer function, and on experimental signals with known fault sizes.the codes and the data of the study are available via the link: https://github.com/omrimatania/ one_fault_shot_learning_for_gears_fault_severity_estimation.",AB_0563
"an efficient unsupervised method for obtaining low-density hyperplane separators is proposed. the method is based on a modified stochastic gradient descent applied on a convolution of the empirical distribution function with a smoothing kernel. low-density hyperplanes are motivated by the fact that they avoid intersecting high density regions, and so tend to pass between high density clusters, thus sep-arating them from one another, while keeping the individual clusters intact. multiple hyperplanes can be combined in a hierarchical model to obtain a complete clustering solution. a simple post-processing of solutions induced by large collections of hyperplanes yields an efficient and accurate clustering method, capable of automatically selecting the number of clusters. experiments show that the proposed method is highly competitive in terms of both speed and accuracy when compared with relevant benchmarks. code is available in the form of an r package at https://github.com/davidhofmeyr/imdh .(c) 2023 elsevier ltd. all rights reserved.",AB_0563
"a surrogate model-based method is proposed for optimising batch distillation processes and applied to the recovery of methanol from a five-component azeotropic waste solvent mixture, where pollutants are removed in two fore-cuts and an after-cut. the objective function is the profit of a single batch, while constraints are for the purity of the main cut and composition of the second fore-cut to be recycled. simulations are performed by a flow-sheet simulator in a set of points in the space of optimisation variables (reflux ratios of steps, stopping criteria of fore-cuts). algebraic surrogate models are fitted by alamo to simulation results to describe the objective function and the constraints. the resulting optimisation problem is solved numerically. the profit obtained is by 5% higher than the one previously obtained by genetic algorithm (commonly used for optimisation of batch distillation), while the number of simulations is reduced to its third. the highest profit, previously obtained by the nelder-mead simplex method, is approached within 1%. although the simplex method required fewer simulations, the new method proposed here is a global one. the process is re-optimised for different prices to investigate their influ-ence on the profit and optimal values of operational parameters.(c) 2023 the author(s). published by elsevier ltd on behalf of institution of chemical engineers. this is an open access article under the cc by-nc-nd license (http://creati-vecommons.org/licenses/by-nc-nd/4.0/).",AB_0563
"crop phenology is crucial information for crop yield estimation and agricultural management. traditionally, phenology has been observed from the ground; however earth observation, weather and soil data have been used to capture the physiological growth of crops. in this work, we propose a new approach for the within-season phenology estimation for cotton at the field level. for this, we exploit a variety of earth observation vegetation indices (derived from sentinel-2) and numerical simulations of atmospheric and soil parameters. our method is unsupervised to address the ever-present problem of sparse and scarce ground truth data that makes most supervised alternatives impractical in real-world scenarios. we applied fuzzy c-means clustering to identify the principal phenological stages of cotton and then used the cluster membership weights to further predict the transitional phases between adjacent stages. in order to evaluate our models, we collected 1,285 crop growth ground observations in orchomenos, greece. we introduced a new collection protocol, assigning up to two phenology labels that represent the primary and secondary growth stage in the field and thus indicate when stages are transitioning. our model was tested against a baseline model that allowed to isolate the random agreement and evaluate its true competence. the results showed that our model considerably outperforms the baseline one, which is promising considering the unsupervised nature of the approach. the limitations and the relevant future work are thoroughly discussed. the ground observations are formatted in an ready-to-use dataset and will be available at https://github.com/agri-hub/ cotton-phenology-dataset upon publication.",AB_0563
"background: skin prick tests (spts) are difficult to standardize, and spt performance mainly relies on the clinician's expertise. so far, the effect of various factors such as device types, shape, variety of material type, and applied force on the performance of spt has not been extensively investigated.objective: to investigate the effect of various factors, including type or shape of devices, material type, and applied force, on the performance of spt.methods: four spt devices with different shapes and materials were applied on 12 subjects under 3 different applied forces (30, 45, and 60 g). the results were compared with standard method using an alk lancet pricked by an experienced clinician.results: a total of 480 pricks were conducted on 12 subjects. the wheal sizes and sensitivities of all devices increased with higher applied forces. the thinner lancets with a long sharp tip had relatively higher analytical sensitivities and provided 100% sensitivity at applied forces of 45 g and above. the pain scores of all devices at applied forces of 30 to 60 g ranged from 1.00 to 1.81 with minimal incidences of bleeding (0%-4.17%), whereas the pain score of the standard method by the alk lancet was 2.08 with much higher incidences of bleeding at 27.08%.conclusion: the type/shape of the spt device and applied force are the essential factors affecting the performance of spt. the study result could pave the way toward higher performance and standardized spt.trial registration: the thai clinical trials registry identification number: tctr20220627004 (https://www.thai clinicaltrials.org/show/tctr20220627004).(c) 2022 american college of allergy, asthma & immunology. published by elsevier inc. all rights reserved.",AB_0563
"kinase drug selectivity is the ground challenge in cancer research. due to the structurally similar kinase drug pockets, off-target inhibitor toxicity has been a major cause for clinical trial failures. the pockets are similar but not identical. here, we describe a transformation invariant protocol to identify distinct geometric features in the drug pocket that can distinguish one kinase from all others. we integrate available experimental structures with the artificial intelligence-based structural kinome, performing a kinome-wide structural bioinformatic analysis to establish the structural principles of kinase drug selectivity. we generate the structural landscape from the experimental kinase-ligand complexes and propose a binary network that encapsulates the information. the results show that all kinases contain binary units that are shared by less than seven other kinases in the kinome. 331 kinases contain unique binary units that may distinguish them from all others. the structural features encoded by these binary units in the network represent the inhibitor-accessible geometric space that may capture the kinome-wide selectivity. our proposed binary network with the unsupervised clustering can serve as a general structural bioinformatic protocol for extracting the distinguishing structural features for any protein from their families. we apply the binary network to epidermal growth factor receptor tyrosine kinase inhibitor selectivity by targeting the gate area and the akt1 serine/ threonine kinase selectivity by binding to the alpha c-helix region and the allosteric pocket. finally, we develop the cross-platform software, kds (kinase drug selectivity), for customized visualization and analysis of the binary networks in the human kinome (https://github.com/cbiit/kds).",AB_0563
