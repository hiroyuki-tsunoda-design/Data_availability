AB,NO
"we present an expansion of fleet, a machine-learning algorithm optimized to select transients that are most likely tidal disruption events (tdes). fleet is based on a random forest algorithm trained on both the light curves and host galaxy information of 4779 spectroscopically classified transients. we find that for transients with a probability of being a tde, p(tde) > 0.5, we can successfully recover tdes with approximate to 40% completeness and approximate to 30% purity when using their first 20 days of photometry or a similar completeness and approximate to 50% purity when including 40 days of photometry, an improvement of almost 2 orders of magnitude compared to random selection. alternatively, we can recover tdes with a maximum purity of approximate to 80% and a completeness of approximate to 30% when considering only transients with p(tde) > 0.8. we explore the use of fleet for future time-domain surveys such as the legacy survey of space and time on the vera c. rubin observatory (rubin) and the nancy grace roman space telescope (roman). we estimate that similar to 10(4) well-observed tdes could be discovered every year by rubin and similar to 200 tdes by roman. finally, we run fleet on the tdes from our rubin survey simulation and find that we can recover similar to 30% of them at redshift z < 0.5 with p(tde) > 0.5, or similar to 3000 tdes yr-1 that fleet could uncover from the rubin stream. we have demonstrated that we will be able to run fleet on rubin photometry as soon as this survey begins. fleet is provided as an open source package on github:.https://github.com/ gmzsebastian/fleet.",AB_0445
"a foundational assumption in paleomagnetism is that the earth's magnetic field behaves as a geocentric axial dipole (gad) when averaged over sufficient timescales. compilations of directional data averaged over the past 5 ma yield a distribution largely compatible with gad, but the distribution of paleointensity data over this timescale is incompatible. reasons for the failure of gad include: (a) arbitrary selection criteria to eliminate unreliable data vary among studies, so the paleointensity database may include biased results. (b) the age distribution of existing paleointensity data varies with latitude, so different latitudinal averages represent different time periods. (c) the time-averaged field could be truly non-dipolar. here, we present a consistent methodology for analyzing paleointensity results and comparing time-averaged paleointensities from different studies. we apply it to data from plio/pleistocene hawai'ian igneous rocks, sampled from fine-grained, quickly cooled material (lava flow tops, dike margins and scoria cones) and subjected to the izzi-thellier technique; the data were analyzed using the bias corrected estimation of paleointensity method of cych et al. (2021, ), which produces accurate paleointensity estimates without arbitrarily excluding specimens from the analysis. we constructed a paleointensity curve for hawai'i over the plio/pleistocene using the method of livermore et al. (2018, https:// doi.org/10.1093/gji/ggy383), which accounts for the age distribution of data. we demonstrate that even with the large uncertainties associated with obtaining a mean field from temporally sparse data, our average paleointensities obtained from hawai'i and antarctica (reanalyzed from asefaw et al., 2021, https://doi. org/10.1029/2020jb020834) are not gad-like from 0 to 1.5 ma but may be prior to that.plain language summary paleomagnetists make the assumption that the earth's magnetic field behaves like a bar magnet centered at the spin axis, known as a geocentric axial dipole or gad. compilations of the magnetic field's direction are largely consistent with this assumption, but compilations of its strength (paleointensity) are not. a number of causes for this could be: (a) the different experimental methods and the criteria used to pass or exclude paleointensity data might cause differences in records. (b) the ages of records differ between locations. (c) the field really doesn't behave like a bar magnet. to test this, we performed paleointensity experiments on rocks collected in hawai'i and compared our results to results of similar age from other locations analyzed using the same methodology. the three locations analyzed in this study do not produce time-averaged paleointensities consistent with a gad field for the most recent 1.5 million years, but a gad field cannot be ruled out before this time. this indicates that differences in time-averaged field strength in global records can be unrelated to differences in methodology or age between studies.",AB_0445
"background samples provide key contextual information for segmenting regions of interest (rois). however, they always cover a diverse set of structures, causing difficulties for the segmentation model to learn good decision boundaries with high sensitivity and precision. the issue concerns the highly heterogeneous nature of the background class, resulting in multi-modal distributions. empirically, we find that neural networks trained with heterogeneous background struggle to map the corresponding contextual samples to compact clusters in feature space. as a result, the distribution over background logit activations may shift across the decision boundary, leading to systematic over-segmentation across different datasets and tasks. in this study, we propose context label learning (colab) to improve the context representations by decomposing the background class into several subclasses. specifically, we train an auxiliary network as a task generator, along with the primary segmentation model, to automatically generate context labels that positively affect the roi segmentation accuracy. extensive experiments are conducted on several challenging segmentation tasks and datasets. the results demonstrate that colab can guide the segmentation model to map the logits of background samples away from the decision boundary, resulting in significantly improved segmentation accuracy. code is available at https://github.com/zerojumpline/colab.",AB_0445
"immune receptor proteins play a key role in the immune system and have shown great promise as biotherapeutics. the structure of these proteins is critical for understanding their antigen binding properties. here, we present immunebuilder, a set of deep learning models trained to accurately predict the structure of antibodies (abodybuilder2), nanobodies (nanobodybuilder2) and t-cell receptors (tcrbuilder2). we show that immunebuilder generates structures with state of the art accuracy while being far faster than alphafold2. for example, on a benchmark of 34 recently solved antibodies, abodybuilder2 predicts cdr-h3 loops with an rmsd of 2.81 angstrom, a 0.09 angstrom improvement over alphafold-multimer, while being over a hundred times faster. similar results are also achieved for nanobodies, (nanobodybuilder2 predicts cdr-h3 loops with an average rmsd of 2.89 angstrom, a 0.55 angstrom improvement over alphafold2) and tcrs. by predicting an ensemble of structures, immunebuilder also gives an error estimate for every residue in its final prediction. immunebuilder is made freely available, both to download (https://github.com/oxpig/immunebuilder) and to use via our webserver (http://opig.stats.ox.ac.uk/webapps/newsabdab/sabpred). we also make available structural models for similar to 150 thousand non-redundant paired antibody sequences (https://doi.org/10.5281/zenodo.7258553).",AB_0445
"motivation: the advent of long-read dna sequencing is allowing complete assembly of highly repetitive genomic regions for the first time, including the megabase-scale satellite repeat arrays found in many eukaryotic centromeres. the assembly of such repetitive regions creates a need for their de novo annotation, including patterns of higher order repetition. to annotate tandem repeats, methods are required that can be widely applied to diverse genome sequences, without prior knowledge of monomer sequences. results: tandem repeat annotation and structural hierarchy (trash) is a tool that identifies and maps tandem repeats in nucleotide sequence, without prior knowledge of repeat composition. trash analyses a fasta assembly file, identifies regions occupied by repeats and then precisely maps them and their higher order structures. to demonstrate the applicability and scalability of trash for centromere research, we apply our method to the recently published col-cen genome of arabidopsis thaliana and the complete human chm13 genome. availability and implementation: trash is freely available at:https://github.com/vlothec/trash and supported on linux.",AB_0445
"we introduce the open force field (openff) 2.0.0 smallmoleculeforce field for drug-like molecules, code-named sage, which buildsupon our previous iteration, parsley. openff force fields are basedon direct chemical perception, which generalizes easily to highlydiverse sets of chemistries based on substructure queries. like theprevious openff iterations, the sage generation of openff force fieldswas validated in protein-ligand simulations to be compatiblewith amber biopolymer force fields. in this work, we detail the methodologyused to develop this force field, as well as the innovations and improvementsintroduced since the release of parsley 1.0.0. one particularly significantfeature of sage is a set of improved lennard-jones (lj) parametersretrained against condensed phase mixture data, the first refit oflj parameters in the openff small molecule force field line. sagealso includes valence parameters refit to a larger database of quantumchemical calculations than previous versions, as well as improvementsin how this fitting is performed. force field benchmarks show improvementsin general metrics of performance against quantum chemistry referencedata such as root-mean-square deviations (rmsd) of optimized conformergeometries, torsion fingerprint deviations (tfd), and improved relativeconformer energetics (delta delta e). we presenta variety of benchmarks for these metrics against our previous forcefields as well as in some cases other small molecule force fields.sage also demonstrates improved performance in estimating physicalproperties, including comparison against experimental data from variousthermodynamic databases for small molecule properties such as delta h (mix), rho-(x), delta g (solv), and delta g (trans). additionally, we benchmarked against protein-ligand bindingfree energies (delta g (bind)), where sageyields results statistically similar to previous force fields. allthe data is made publicly available along with complete details onhow to reproduce the training results at https://github.com/openforcefield/openff-sage.",AB_0445
"the widespread adoption of effective hybrid closed loop systems would represent an important milestone of care for people living with type 1 diabetes (t1d). these devices typically utilise simple control algorithms to select the optimal insulin dose for maintaining blood glucose levels within a healthy range. online reinforcement learning (rl) has been utilised as a method for further enhancing glucose control in these devices. previous approaches have been shown to reduce patient risk and improve time spent in the target range when compared to classical control algorithms, but are prone to instability in the learning process, often resulting in the selection of unsafe actions. this work presents an evaluation of offline rl for developing effective dosing policies without the need for potentially dangerous patient interaction during training. this paper examines the utility of bcq, cql and td3-bc in managing the blood glucose of the 30 virtual patients available within the fda-approved uva/padova glucose dynamics simulator. when trained on less than a tenth of the total training samples required by online rl to achieve stable performance, this work shows that offline rl can significantly increase time in the healthy blood glucose range from 61.6 +/- 0.3% to 65.3 +/- 0.5% when compared to the strongest state-of-art baseline (p < 0.001). this is achieved without any associated increase in low blood glucose events. offline rl is also shown to be able to correct for common and challenging control scenarios such as incorrect bolus dosing, irregular meal timings and compression errors. the code for this work is available at: https://github.com/hemerson1/offline- glucose.",AB_0445
"motivation: various computational biology calculations require a probabilistic optimization protocol to determine the parameters that capture the system at a desired state in the configurational space. many existing methods excel at certain scenarios, but fail in others due, in part, to an inefficient exploration of the parameter space and easy trapping into local minima. here, we developed a general-purpose optimization engine in r that can be plugged to any, simple or complex, modelling initiative through a few lucid interfacing functions, to perform a seamless optimization with rigorous parameter sampling. results: roptimus features simulated annealing and replica exchange implementations equipped with adaptive thermoregulation to drive monte carlo optimization process in a flexible manner, through constrained acceptance frequency but unconstrained adaptive pseudo temperature regimens. we exemplify the applicability of our r optimizer to a diverse set of problems spanning data analyses and computational biology tasks. availability and implementation: roptimus is written and implemented in r, and is freely available from cran (http://cran.r-project.org/web/ packages/roptimus/index.html) and github (http://github.com/sahakyanlab/roptimus). [graphics] .",AB_0445
"background social prescribing is a mechanism of connecting patients with non-medical forms of support within the community and has been shown to improve mental health and wellbeing in adult populations. in the last few years, it has been used in child and youth settings with promising results. currently, pathways are being developed for social prescribing in child and adolescent mental health services (camhs) to support children and young people on treatment waiting lists. the wellbeing while waiting study will evaluate whether social prescribing benefits the mental health and wellbeing of children and young people. methods this study utilises an observational, hybrid type ii implementation-effectiveness design. up to ten camhs who are developing social prescribing pathways as part of a programme run across england with support from the social prescribing youth network will participate. outcomes for children and young people receiving social prescribing whilst on camhs waiting lists will be compared to a control group recruited prior to the pathway roll-out. questionnaire data will be collected at baseline, 3 months and 6 months. primary outcomes for children and young people are mental health symptoms (including anxiety, depression, stress, emotional and behavioural difficulties). secondary outcomes include: loneliness, resilience, happiness, whether life is worthwhile, life satisfaction, and service use. an implementation strand using questionnaires and interviews will explore the acceptability, feasibility, and suitability of the pathway, potential mechanisms of action and their moderating effects on the outcomes of interest, as well as the perceived impact of social prescribing. questionnaire data will be analysed mainly using differencein-differences or controlled interrupted time series analysis. interview data will be analysed using reflexive thematic analysis. discussion the wellbeing while waiting study will provide the first rigorous evidence of the impact of social prescribing for children and young people on waiting lists for mental health treatment. findings will help inform the prioritisation, commissioning, and running of social prescribing in other camhs. to maximise impact, findings will be available on the study website (https://sbbresearch.org) and disseminated via national and international networks.",AB_0445
"the local inertial two-dimensional (2d) flow model on lisflood-fp, the so-called acceleration (acc) uniform grid solver, has been widely used to support fast, computationally efficient fluvial/pluvial flood simulations. this paper describes new releases, on lisflood-fp 8.1, for parallelised flood simulations on the graphical processing units (gpus) to boost efficiency of the existing parallelised acc solver on the central processing units (cpus) and enhance it further by enabling a new non-uniform grid version. the non-uniform solver generates its grid using the multiresolution analysis (mra) of the multiwavelets (mws) to a galerkin polynomial projection of the digital elevation model (dem). this sensibly coarsens the resolutions where the local topographic details are below an error threshold e and allows classes of land use to be properly adapted. both the grid generator and the adapted acc solver on the non-uniform grid are implemented in a gpu new codebase, using the indexing of z-order curves alongside a parallel tree traversal approach. the efficiency performance of the gpu parallelised uniform and non-uniform grid solvers is assessed for five case studies, where the accuracy of the latter is explored for e=10(-4) and 10(-3) in terms of how close it can reproduce the prediction of the former. on the gpu, the uniform acc solver is found to be 2-28 times faster than the cpu predecessor with increased number of elements on the grid, and the non-uniform solver can further increase the speed up to 320 times with increased reduction in the grid's elements and decreased variability in the resolution. lisflood-fp 8.1, therefore, allows faster flood inundation modelling to be performed at both urban and catchment scales. it is openly available under the gpl v3 license, with additional documentation at https://www.seamlesswave.com/lisflood8.0 (last access: 12 march 2023).",AB_0445
