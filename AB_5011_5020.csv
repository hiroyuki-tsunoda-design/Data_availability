AB,NO
"there are various binary semantic segmentation tasks in remote sensing (rs) that aim to extract the foreground areas of interest, such as buildings and roads, from the background in satellite images. in particular, semisupervised learning (ssl), which can use limited labeled data to guide a large amount of unlabeled data for model training, can significantly promote the fast applications of these tasks in practice. however, due to the predominance of the background in rs images, the foreground only accounts for a small proportion of the pixels. it poses a challenge: models are biased toward the majority class of the background, leading to poor performance on the minority class of the foreground. to address this issue, this article proposes a novel and effective ssl framework, adaptive matching (adaptmatch), for rs binary segmentation. adaptmatch calculates individual and adaptive thresholds of the foreground and background based on their convergence difficulty in an online manner at the training stage; the adaptive thresholds are then used to select the high-confidence pseudo-labeled data of the two classes for model self-training in turn. extensive experiments are conducted on two widely studied rs binary segmentation tasks, building footprint extraction and road extraction, to demonstrate the effectiveness and generalizability of the proposed method. the results show that the proposed adaptmatch achieves superior performance compared with some state-of-the-art semisupervised methods in rs binary segmentation tasks. the codes will be publicly available at https://github.com/zhu-xlab/adaptmatch.",AB_0502
"identifying manipulated regions in images is a challenging task due to the existence of very accurate image inpainting techniques leaving almost unnoticeable traces in tampered regions. these image inpainting methods can be used for multiple purposes (e.g., removing objects, reconstructing corrupted areas, eliminating various types of distortion, etc.) makes creating forensic detectors for image manipulation an extremely difficult and time-consuming procedure. the aim of this paper is to localize the tampered regions manipulated by image inpainting methods. to do this, we propose a novel cnn-based deep learning model called irl-net which includes three main modules: enhancement, encoder, and decoder modules. to evaluate our method, three image inpainting methods have been used to reconstruct the missed regions in two face and scene image datasets. we perform both qualitative and quantitative evaluations on the generated datasets. experimental results demonstrate that our method outperforms previous learning-based manipulated region detection methods and generates realistic and semantically plausible images. we also provide the implementation of the proposed approach to support reproducible research via https://github.com/amiretefaghi/irl-net.",AB_0502
"background: mental disorders impact both individuals and health systems. symptoms and syndromes often remain undetected and untreated, resulting in chronification. besides limited health care resources, within-person barriers such as the lack of trust in professionals, the fear of stigmatization, or the desire to cope with problems without professional help contribute to the treatment gap. self-guided mental health apps may support treatment seeking by reducing within-person barriers and facilitating mental health literacy. digital mental health interventions may also improve mental health related self-management skills and contribute to symptom reduction and the improvement of quality of life. objective: this study aims to investigate the effects of a self-guided transdiagnostic app for mental health on help seeking, reduced stigma, mental health literacy, self-management skills, mental health symptoms, and quality of life using a randomized controlled design.methods: overall, 1045 participants (recruited via open, blinded, and web-based recruitment) with mild to moderate depression or anxiety-, sleep-, eating-, or somatization-related psychopathology were randomized to receive either access to a self-guided transdiagnostic mental health app (minddoc) in addition to care as usual or care as usual only. the core features of the app were regular self-monitoring, automated feedback, and psychological courses and exercises. the coprimary outcomes were mental health literacy, mental health-related patient empowerment and self-management skills (mhpss), attitudes toward help seeking, and actual mental health service use. the secondary outcomes were psychopathological symptom burden and quality of life. data were collected at baseline and 8 weeks and 6 months after randomization. treatment effects were investigated using analyses of covariance, including baseline variables as predictors and applying multiple imputation.results: we found small but robust between-group effects for mhpss (cohen d=0.29), symptoms burden (cohen d=0.28), and quality of life (cohen d=0.19) 8 weeks after randomization. the effects on mhpss were maintained at follow-up. follow-up assessments also showed robust effects on mental health literacy and preliminary evidence for the improvement of help seeking. predictors of attrition were lower age and higher personality dysfunction. among the non-attritors, predictors for deterioration were less outpatient treatment and higher initial symptom severity. conclusions: a self-guided transdiagnostic mental health app can contribute to lasting improvements in patient empowerment. symptoms of common mental disorders and quality of life improved faster in the intervention group than in the control group. therefore, such interventions may support individuals with symptoms of 1 or more internalizing disorders, develop health-centered coping skills, prevent chronification, and accelerate symptom improvement. although the effects for individual users are small and predictors of attrition and deterioration need to be investigated further, the potential public health impact of a self-guided intervention can be large, given its high scalability.trial registration: german clinical trials register drks00022531; https://drks.de/search/de/trial/drks00022531",AB_0502
"leading a healthy lifestyle significantly reduces the risk of developing non-communicable diseases (ncd). however, defining and monitoring healthy eating behaviour depends on multiple factors and usually requires the intervention of experts. on the other hand, nowadays millions of images are shared on social media and web platforms. in particular, many of them are food images taken from a smartphone over time, providing information related to the individual's diet. consequently, exploiting recent advances in image processing and artificial intelligence (ai), this scenario represents an excellent opportunity to: i) create new methods that analyse the individuals' health from what they eat, and ii) develop personalised recommendations to improve nutrition and diet under specific circumstances (e.g., obesity or covid). this article introduces the ai4food-nutritionfw, a framework that facilitates the creation of food image datasets tailored to configurable eating behaviours. the framework considers various aspects such as region and lifestyle, simulating a user-friendly scenario where individuals capture food images using their smartphones. the study also presents a novel food image dataset comprising 4,800 diverse weekly diets from 15 distinct profiles, ranging from healthy eating habits to unhealthy ones. finally, we evaluate the healthy eating behaviours through a score based on the normalised mahalanobis distance (nmd), achieving promising results (99.53% and 99.60% accuracy and sensitivity, respectively). we also release to the research community a software implementation of our proposed ai4food-nutritionfw (https://github.com/bidalab/ai4food-nutritionfw) and the mentioned food image dataset created with it.",AB_0502
"dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. extracting unknown dataset mentions which are not part of the training data of the model is even harder. we address this challenge in two ways. first, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. we consider multiple transformer-based models and strong baselines for this task. subsequently, the dataset is extracted from the positive context. second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. for the extraction of datasets, we consider transformer models in named entity recognition (ner) mode. we contrast ner with the transformers' capabilities for question answering (qa). we use the coleridge initiative show us the datadataset consisting of 14.3k scientific papers with about 35k mentions of datasets. we found that using transformers in qa mode is a better choice than ner for extracting unknown datasets. the rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. comparing the two-step versus the one-step approach, we found contrasting strengths. a two-step dataset extraction using an mlp for filtering and roberta in qa mode extracts more dataset mentions than a one-step system, but at the cost of a lower f1-score of 62.7%. a one-step extraction with deberta in qa achieves the highest f1-score of 92.88% at the cost of missing dataset mentions. we recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. the source code is available at https://github.com/yousef-younes/dataset_mention_extraction.",AB_0502
"reliably predicting the motion of contestant vehicles surrounding an autonomous racecar is crucial for effective and performant ego-motion planning. although highly expressive, deep neural networks are black-box models, making their usage challenging in this safety-critical applications of autonomous racing. on the other hand, physics-based models provide high safety guarantees for the predicted trajectory but lack accuracy. the method presented in this paper targets this trade-off. we introduce a method to predict the trajectories of opposing racecars with deep neural networks considering physical constraints to restrict the output and to improve its feasibility. we report the method's performance against an lstm-based encoder-decoder architecture on data acquired from multi-agent racing simulations. the proposed method outperforms the baseline model in prediction accuracy and robustness. still, it fulfills quality guarantees of smoothness and consistency of the predicted trajectory and prevents out-of-track predictions. thus, a robust real-world application of the model with high prediction accuracy is proven. the presented model was deployed on the racecar of the technical university of munich for the indy autonomous challenge 2021. the code used in this research is available as open-source software at https://www.github.com/tumftm/mixnet.",AB_0502
"medical image classification is critical, where reliability and transparency are crucial for the safe and accurate diagnosis of diseases. deep convolutional neural networks (dcnns) are widely used in medical image classification due to their high performance. however, they are often considered black-boxes because they offer little insight into decision-making. therefore, improving the interpretability of dcnns is crucial for their adoption in medical diagnoses. this paper proposes a novel three-tier self-interpretable dcnn (ts-cnn) architecture for multi-region medical image classification, which improves classification performance while being inherently interpretable. the proposed ts-cnn architecture is well-suited for medical images with multiple regions, such as images with scattered and randomly shaped lesions. the proposed architecture has three branches: a global branch that learns the relevant patterns from the raw input image; an attention branch that selects the important regions and discards the irrelevant parts for the local branch to learn; and a fusion branch that distills knowledge from both the global and local branches for classification. the proposed architecture is flexible in terms of the backbone cnns used for classification and post-hoc interpretability methods used for attention capture. we demonstrate the flexibility and generalization of the architecture through a series of experiments involving multiple state-of-the-art cnn architectures such as densenet-121, inception, xception, and resnet-50 as the global/local branches, each paired with gradcam and saliency maps as attention modules. the proposed architecture outperformed the backbone model in classification tasks on two datasets: a custom-made blob dataset and a publicly available skin lesion pad-ufes-20 dataset, demonstrating its potential for improving accuracy in medical image classification tasks. the code related to this work can be found at: https://github.com/sikha2552/ts-cnn-a-three-tier-self-interpretable-cnn-for-medical-image-classification-empowered-with-post-hoc.git.",AB_0502
"the development of deep learning based image representation learning (irl) methods has attracted great attention for various image understanding problems. most of these methods require the availability of a set of high quantity and quality of annotated training images, which can be time-consuming, complex and costly to gather. to reduce labeling costs, crowdsourced data, automatic labeling procedures or citizen science projects can be considered. however, such approaches increase the risk of including label noise in training data. it may result in overfitting on noisy labels when discriminative reasoning is employed as in most of the existing methods. this leads to sub-optimal learning procedures, and thus inaccurate characterization of images. to address this issue, in this paper, we introduce a generative reasoning integrated label noise robust deep representation learning (grid) approach. the proposed grid approach aims to model the complementary characteristics of discriminative and generative reasoning for irl under noisy labels. to this end, we first integrate generative reasoning into discriminative reasoning through a supervised variational autoencoder. this allows the proposed grid approach to automatically detect training samples with noisy labels. then, through our label noise robust hybrid representation learning strategy, grid adjusts the whole learning procedure for irl of these samples through generative reasoning and that of the other samples through discriminative reasoning. our approach learns discriminative image representations while preventing interference of noisy labels during training independently from the irl method being selected. thus, unlike the existing label noise robust methods, grid does not depend on the type of annotation, label noise, neural network architecture, loss function or learning task, and thus can be directly utilized for various image understanding problems. experimental results show the effectiveness of the proposed grid approach compared to the state-of-the-art methods. the code of the proposed approach is publicly available at https://github.com/gencersumbul/grid.",AB_0502
"hyperspectral images (hsis) comprise plenty of information in the spatial and spectral domain, which is highly beneficial for performing classification tasks in a very accurate way. recently, attention mechanisms have been widely used in the hsi classification due to their ability to extract relevant spatial and spectral features. notwithstanding their positive results, most of the attentional strategies usually introduce a significant number of parameters to be trained, making the models more complex and increasing the computational load. in this article, we develop a new parameter-free attention network for hsi classification. the main advantage of our model is that it does not add parameters to the original network (as opposed to other state-of-the-art approaches) while providing higher classification accuracies. extensive experimental validations and quantitative comparisons are conducted-using different benchmark hsis-to illustrate these advantages. the code is available on https://github.com/mhaut/free2resnet.",AB_0502
"neural networks (nns) have gained importance in hyperspectral image (hsi) segmentation for earth observation (eo) due to their unparalleled data-driven feature extraction capability. however, in many real-life situations, ground truth is not available, and the performance of unsupervised nns is still susceptible to enhancement. to overcome this challenge, this letter presents a new loss function to improve the performance of unsupervised hsi segmentation models. the spectral loss function, sl, which can be included in different models, is based on the purity of the unmixing endmembers and the spectral similarity of the clusters provided by the nn to determine the classes. it is incorporated into a 3-d convolutional autoencoder (ae) to validate its performance on four standard hsi benchmarks. furthermore, its performance has been qualitatively examined in a real case study, an oil spill without ground truth. the results show that sl is a breakthrough in unsupervised hs segmentation, obtaining the best overall performance and highlighting the importance of spectral signatures. additionally, the dimensional reduction is also vital in compacting the spectral information, which facilitates its segmentation. the source code is available at https://github.com/mhaut/hsi-3dsploss.",AB_0502
