AB,NO
"solar energetic protons (seps) have been shown to contribute significantly to the inner zone trapped proton population for energies & lt;100 mev and l & gt; 1.3 (selesnick et al., 2007, https://doi. org/10.1029/2006sw000275). the relativistic electron proton telescope (rept) on the van allen probes launched 30 august 2012 observed a double-peaked (in l) inner zone population throughout the 7-year lifetime of the mission. it has been proposed that a strong sep event accompanied by a cme-shock in early march 2012 provided the sep source for the higher l trapped proton population, which then diffused radially inward to be observed by rept at l & sim; 2. here, we follow trajectories of sep protons launched isotropically from a sphere at 7 re in 15 s cadence fields from an lyon-fedder-mobarry coupled to rice convection model global magnetohydrodynamic (mhd) simulation driven by measured upstream solar wind parameters. the timescale of the interplanetary shock arrival is captured, launching a magnetosonic impulse propagating azimuthally along the dawn and dusk flanks inside the magnetosphere, shown previously to produce sep trapping. the mhd-test particle simulation uses geostationary operational environmental satellite (goes) proton energy spectra to weight the initial radial profile required for the radial diffusion calculation over the following 2 years. goes proton measurements also provide a dynamic outer boundary condition for radial diffusion. a direct comparison with rept measurements 20 months following the trapping event in march 2012 supports this novel combination of short-term and long-term evolution of the newly trapped protons.",AB_0618
"surgical techniques targeting behavioral disorders date back thousands of years. in this review, the authors discuss the history of neurosurgery for psychiatric disorders, starting with trephination in the stone age, progressing through the fraught practice of prefrontal lobotomy, and ending with modern neurosurgical techniques for treating psychiatric conditions, including ablative procedures, conventional deep brain stimulation, and closed-loop neurostimulation. despite a tumultuous past, psychiatric neurosurgery is on the cusp of becoming a transformative therapy for patients with psychiatric dysfunction, with an ever-increasing evidence base suggesting reproducible and ethical therapeutic benefit. https://thejns.org/doi/abs/10.3171/2022.11.focus22622",AB_0618
"background: conducting an embedded pragmatic clinical trial in the workflow of a healthcare system is a complex endeavor. the complexity of the intervention delivery can have implications for study planning, ability to maintain fidelity to the intervention during the trial, and/or ability to detect meaningful differences in outcomes. methods: we conducted a literature review, developed a tool, and conducted two rounds of phone calls with nih pragmatic trials collaboratory demonstration project principal investigators to develop the intervention delivery complexity tool. after refining the tool, we piloted it with collaboratory demonstration projects and developed an online version of the tool using the r shiny application (https://duke-som.shinyapps.io/ict-epct/ ). results: the 6-item tool consists of internal and external factors. internal factors pertain to the intervention itself and include workflow, training, and the number of intervention components. external factors are related to intervention delivery at the system level including differences in healthcare systems, the dependency on setting for implementation, and the number of steps between the intervention and the outcome. conclusion: the intervention delivery complexity tool was developed as a standard way to overcome communication challenges of intervention delivery within an embedded pragmatic trial. this version of the tool is most likely to be useful to the trial team and its health system partners during trial planning and conduct. we expect further evolution of the tool as more pragmatic trials are conducted and feedback is received on its performance outside of the nih pragmatic trials collaboratory.",AB_0618
"study objectives: examine bidirectional associations between daytime napping and nighttime sleep among pregnant individuals with insomnia disorder.methods: we used baseline data from a randomized controlled trial of insomnia treatment during pregnancy (n = 116). participants in their second or third trimester of pregnancy self-reported daytime napping and nighttime sleep parameters using a sleep diary and wore an actiwatch-2 during the same 7-day period. linear regression models, accounting for intraindividual correlation, were used to estimate associations between daytime napping and nighttime sleep parameters (duration, efficiency, quality, awakenings). models were also stratified by trimester of pregnancy.results: sixty-three percent of participants reported napping on at least 1 day. among participants in the second trimester (65%), napping 15-59 minutes was associated with 6.3% greater self-reported sleep efficiency (95% confidence interval: 2.3, 10.2) and 0.5 units greater self-reported sleep quality (95% confidence interval: 0.0, 0.9) that night; napping 60+ minutes was associated with 0.6 hours shorter actigraphy-measured sleep duration (95% confidence interval:-1.0,-0.2). napping was not associated with nighttime sleep overall or during the third trimester. nighttime sleep parameters were not associated with napping duration the following day.conclusions: among pregnant individuals with insomnia in the second trimester, short napping duration was associated with higher self-reported sleep efficiency and quality; long napping duration was associated with shorter actigraphy-measured sleep duration. additional research is needed to examine the interaction between nap duration and nap timing. in the future, these results may lead to more nuanced recommendations for daytime napping among pregnant individuals with insomnia disorder. clinical trial registration: registry: clinicaltrials.gov; name: treatment for insomnia during pregnancy; url: https://clinicaltrials.gov/ct2/show/nct01846585; identifier: nct01846585.",AB_0618
"objective the authors' objective was to investigate the impact of the global covid-19 pandemic on hospital presen-tation and process of care for the treatment of traumatic brain injuries (tbis). improved understanding of these effects will inform sociopolitical and hospital policies in response to future pandemics.methods the michigan trauma quality improvement program (mtqip) database, which contains data from 36 level i and ii trauma centers in michigan and minnesota, was queried to identify patients who sustained tbi on the basis of head/neck abbreviated injury scale (ais) codes during the periods of march 13 through july 2 of 2017-2019 (pre- covid-19 period) and march 13, 2020, through july 2, 2020 (covid-19 period). analyses were performed to detect differences in incidence, patient characteristics, injury severity, and outcomes.results there was an 18% decrease in the rate of encounters with tbi in the first 8 weeks (march 13 through may 7), followed by a 16% increase during the last 8 weeks (may 8 through july 2), of our covid-19 period compared with the pre-covid-19 period. cumulatively, there was no difference in the rates of encounters with tbi between the covid-19 and pre-covid-19 periods. severity of tbi, as measured with maximum ais score for the head/neck region and glasgow coma scale score, was also similar between periods. during the covid-19 period, a greater proportion of pa-tients with tbi presented more than a day after sustaining their injuries (p = 0.046). covid-19 was also associated with a doubling in the decubitus ulcer rate from 1.0% to 2.1% (p = 0.002) and change in the distribution of discharge status (p = 0.01). multivariable analysis showed no differences in odds of death/hospice discharge, intensive care unit stay of at least a day, or need for a ventilator for at least a day between the covid-19 and pre-covid-19 periods.conclusions during the early months of the covid-19 pandemic, the number of patients who presented with tbi was initially lower than in the years 2017-2019 prior to the pandemic. however, there was a subsequent increase in the rate of encounters with tbi, resulting in overall similar rates of tbi between march 13 through july 2 during the covid-19 period and during the pre-covid-19 period. the covid-19 cohort was also associated with negative impacts on time to presentation, rate of decubitus ulcers, and discharge with supervision. policies in response to future pandemics must consider the resources necessary to care for patients with tbi. https://thejns.org/doi/abs/10.3171/2022.5.jns22244",AB_0618
"the introduction of high-throughput chromosome conformation capture (hi-c) into metagenomics enables reconstructing high-quality metagenome-assembled genomes (mags) from microbial communities. despite recent advances in recovering eukaryotic, bacterial, and archaeal genomes using hi-c contact maps, few of hi-c-based methods are designed to retrieve viral genomes. here we introduce viralcc, a publicly available tool to recover complete viral genomes and detect virus-host pairs using hi-c data. compared to other hi-c-based methods, viralcc leverages the virus-host proximity structure as a complementary information source for the hi-c interactions. using mock and real metagenomic hi-c datasets from several different microbial ecosystems, including the human gut, cow fecal, and wastewater, we demonstrate that viralcc outperforms existing hi-c-based binning methods as well as state-of-the-art tools specifically dedicated to metagenomic viral binning. viralcc can also reveal the taxonomic structure of viruses and virus-host pairs in microbial communities. when applied to a real wastewater metagenomic hi-c dataset, viralcc constructs a phage-host network, which is further validated using crispr spacer analyses. viralcc is an open-source pipeline available at https://github.com/dyxstat/viralcc. metagenomic hi-c enables genome retrieval in microbial samples. here, the authors develop an integrative method to recover complete viral genomes and detect virus-host pairs using metagenomic hi-c data.",AB_0618
"background: there is substantial uncertainty regarding the effects of restrictive postoperative transfusion among patients who have underlying cardiovascular disease. the top trial's objective is to compare adverse outcomes between liberal and restrictive transfusion strategies in patients undergoing vascular and general surgery op-erations, and with a high risk of postoperative cardiac events.methods: a two-arm, single-blinded, randomized controlled superiority trial will be used across 15 veterans affairs hospitals with expected enrollment of 1520 participants. postoperative transfusions in the liberal arm commence when hb is <10 g/ dl and continue until hb is greater than or equal to 10 g/dl. in the restrictive arm, transfusions begin when hb is <7 g/dl and continue until hb is greater than or equal to 7 g/dl. study duration is estimated to be 5 years including a 3-month start-up period and 4 years of recruitment. each randomized participant will be followed for 90 days after randomization with a mortality assessment at 1 year.results: the primary outcome is a composite endpoint of all-cause mortality, myocardial infarction (mi), coro-nary revascularization, acute renal failure, or stroke occurring up to 90-days after randomization. events rates will be compared between restrictive and liberal transfusion groups.conclusions: the top trial is uniquely positioned to provide high quality evidence comparing transfusion stra-tegies among patients with high cardiac risk. results will clarify the effect of postoperative transfusion strategies on adverse outcomes and inform postoperative management algorithms.trial registration: http://clinicaltrials.gov identifier: nct03229941",AB_0618
"hydrogen-deuterium exchange mass spectrometry (hdx-ms) is a powerful protein footprinting technique to study protein dynamics and binding; however, hdx-ms data analysis is often challenging and time-consuming. moreover, the hdx community is expanding to investigate multiprotein and highly complex protein systems which further complicates data analysis. thus, a simple, open-source software package designed to analyze large and highly complex protein systems is needed. in this vein, we have developed the deuterium calculator, a python-based software package for hdx-ms data analysis. the deuterium calculator is capable of differential and nondifferential hdx-ms analysis, produces standardized data files according to recommendations from the international conference on hydrogen-exchange mass spectrometry (ic-hdx) to increase transparency in data analysis, and generates woods' plots for statistical analysis and data visualization. this standard output can be used to perform time dependent deuteration studies and for the study of protein folding kinetics or differential uptake. moreover, the deuterium calculator is capable of performing these analyses on large hdx-ms data sets (e.g., lc-hdx-ms from cell lysate digest). the deuterium calculator is freely available for download at https://github. com/ouwulab/thedeuteriumcalculator.git. data are available via proteomexchange with identifier pxd036813.",AB_0618
"high-throughput chromatin conformation capture technologies, such as hi-c and micro-c, have enabled genome-wide view of chromatin spatial organization. most recently, hi-c-derived enrichment-based technologies, including hichip and plac-seq, offer attractive alternatives due to their high signal-to-noise ratio and low cost. while a series of computational tools have been developed for hi-c data, methods tailored for hichip and plac-seq data are still under development. here we present hptad, a computa-tional method to identify topologically associating domains (tads) from hichip and plac-seq data. we performed comprehensive benchmark analysis to demonstrate its superior performance over existing tad callers designed for hi-c data. hptad is freely available at https://github.com/yunliunc/hptad. (c) 2023 the authors. published by elsevier b.v.",AB_0618
"background. performing back trajectory and forward trajectory using the hybrid single-particle lagrangian integrated trajectory model (hysplit) is a reliable approach for assessing particle transport after release among mid-field atmospheric models. hysplit has an externally facing online interface that allows non-expert users to run the model trajectories without requiring extensive training or programming. however, the existing hysplit interface is limited if simulations have a large amount of meteorological data and timesteps that are not coincident. the objective of this study is to design and develop a more robust tool to rapidly evaluate hazard transport conditions and to perform risk analysis, while still maintaining an intuitive and user-friendly interface. methods. hysplit calculates forward and backward trajectories of particles based on wind speed, wind direction, and the corresponding location, timestamp, and pasquill stability classes of the regions of the atmosphere in terms of the wind speed, the amount of solar radiation, and the fractional cloud cover. the computed particle transport trajectories, combined with the online proton transfer reaction-mass spectrometry (ptr-ms) data (https://igshare.com/articles/dataset/arl_data_from_ pros_station_at_hanford_site/19993964), can be used to identify and quantify the sources and affected area of the hazardous chemicals' emission using the potential source distribution function (psdf). psdf is an improved statistical function based on the well-known potential source contribution function (pscf) in establishing the air pollutant source and receptor relationship. performing this analysis requires a range of meteorological and pollutant concentration measurements to be statistically meaningful. the existing hysplit graphical user interface (gui) does not easily permit computations of trajectories of a dataset of meteorological data in high temporal frequency. to improve the performance of hysplit computations from a large dataset and enhance risk analysis of the accidental release of material at risk, a geospatial risk analysis tool (grat-gui) is created to allow large data sets to be processed instantaneously and to provide ease of visualization. results. the grat-gui is a native desktop-based application and can be run in any windows 10 system without any internet access requirements, thus providing a secure way to process large meteorological datasets even on a standalone computer. grat-gui has features to import, integrate, and convert meteorological data with various formats for hazardous chemical emission source identification and risk analysis as a self-explanatory user interface. the tool is available at https://igshare.com/articles/ software/grat/19426742.",AB_0618
