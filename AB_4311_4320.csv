AB,NO
"in the context of classical molecular simulations, the accuracy of a force field is highly influenced by the values of the relevant simulation parameters. in this work, a parameter-space mapping (psm) workflow is proposed to aid in the calibration of force-field parameters, based mainly on the following features: (i) regular-grid discretization of the search space; (ii) partial sampling of the search-space grid; (iii) training of surrogate models to predict the estimates of the target properties for nonsampled parameter sets; (iv) post hoc interpretation of the results in terms of multiobjective optimization concepts; (v) attenuation of statistical errors achieved via empiric extension of the duration of the simulations; (vi) iterative search-space translation according to a user-defined scalar objective function that measures the accuracy of the force field (e.g., the weighted root-mean-square deviation of the target properties relative to the reference data). this combination of features results in a hybrid of a single-and a multiobjective optimization strategy, allowing for the approximate determination of both a local minimum of the chosen objective function and its neighboring pareto efficient points. the psm workflow is implemented in the extensible python program gmak, which is made available in the git repository at http://github.com/mssm-labmmol/gmak. using this implementation, the psm workflow was tested in a proof-of concept fashion in the recalibration of the lennard-jones parameters of the 3-point optimal point charge (opc3) water model for compatibility with the gromos treatment of nonbonded interactions. the recalibrated model reproduces typical pure-liquid properties with an accuracy similar to the original opc3 model and represents a significant improvement relative to the simple point charge (spc) model, which is the official recommendation for simulations using gromos force fields.",AB_0432
"motivation: during disease progression or organism development, alternative splicing may lead to isoform switches that demonstrate similar temporal patterns and reflect the alternative splicing co-regulation of such genes. tools for dynamic process analysis usually neglect alternative splicing. results: here, we propose spycone, a splicing-aware framework for time course data analysis. spycone exploits a novel is detection algorithm and offers downstream analysis such as network and gene set enrichment. we demonstrate the performance of spycone using simulated and real-world data of sars-cov-2 infection. availability and implementation: the spycone package is available as a pypi package. the source code of spycone is available under the gplv3 license at https://github.com/yollct/spycone and the documentation at https://spycone.readthedocs.io/en/latest/. contact: olga.tsoy@uni-hamburg.de supplementary information: supplementary data are available at bioinformatics online.",AB_0432
"molecular dynamics with excited normal modes (mdenm) is an enhanced sampling method for exploring conformational changes in proteins with minimal biases. the excitation corresponds to injecting kinetic energy along normal modes describing intrinsic collective motions. herein, we developed a new automated open-source implementation, mdexciter (https://github.com/mcosta27/mdexciter), enabling the integra-tion of mdenm with two commonly used simulation programs with gpu support. second, we generalized the method to include the excitation of principal components calculated from experimental ensembles. finally, we evaluated whether the use of coarse-grained normal modes calculated with elastic network representations preserved the performance and accuracy of the method. the advantages and limitations of these new approaches are discussed based on results obtained for three different protein test cases: two globular and a protein/membrane system.",AB_0432
"in recent years, advanced research has focused on the direct learning and analysis of remote-sensing images using natural language processing (nlp) techniques. the ability to accurately describe changes occurring in multi-temporal remote sensing images is becoming increasingly important for geospatial understanding and land planning. unlike natural image change captioning tasks, remote sensing change captioning aims to capture the most significant changes, irrespective of various influential factors such as illumination, seasonal effects, and complex land covers. in this study, we highlight the significance of accurately describing changes in remote sensing images and present a comparison of the change captioning task for natural and synthetic images and remote sensing images. to address the challenge of generating accurate captions, we propose an attentive changes-to-captions network, called chg2cap for short, for bitemporal remote sensing images. the network comprises three main components: 1) a siamese cnn-based feature extractor to collect high-level representations for each image pair; 2) an attentive encoder that includes a hierarchical self-attention block to locate change-related features and a residual block to generate the image embedding; and 3) a transformer-based caption generator to decode the relationship between the image embedding and the word embedding into a description. the proposed chg2cap network is evaluated on two representative remote sensing datasets, and a comprehensive experimental analysis is provided. the code and pre-trained models will be available online at https://github.com/shizhenchang/chg2cap.",AB_0432
"point cloud streaming has recently attracted research attention as it has the potential to provide six degrees of freedom movement, which is essential for truly immersive media. the transmission of point clouds requires high-bandwidth connections, and adaptive streaming is a promising solution to cope with fluctuating bandwidth conditions. thus, understanding the impact of different factors in adaptive streaming on the quality of experience (qoe) becomes fundamental. point clouds have been evaluated in virtual reality (vr), where viewers are completely immersed in a virtual environment. augmented reality (ar) is a novel technology and has recently become popular, yet quality evaluations of point clouds in ar environments are still limited to static images. in this paper, we perform a subjective study of four impact factors on the qoe of point cloud video sequences in ar conditions, including encoding parameters (quantization parameters, qps), quality switches, viewing distance, and content characteristics. the experimental results show that these factors significantly impact the qoe. the qoe decreases if the sequence is encoded at high qps and/or switches to lower quality and/or is viewed at a shorter distance, and vice versa. additionally, the results indicate that the end user is not able to distinguish the quality differences between two quality levels at a specific (high) viewing distance. an intermediate-quality point cloud encoded at geometry qp (g-qp) 24 and texture qp (t-qp) 32 and viewed at 2.5m can have a qoe (i.e., score 6.5 out of 10) comparable to a high-quality point cloud encoded at 16 and 22 for g-qp and t-qp, respectively, and viewed at a distance of 5m. regarding content characteristics, objects with lower contrast can yield better quality scores. participants' responses reveal that the visual quality of point clouds has not yet reached an immersion level as desired. the average qoe of the highest visual quality is less than 8 out of 10. there is also a good correlation between objective metrics (e.g., color peak signal-to-noise ratio (psnr) and geometry psnr) and the qoe score. especially the pearson correlation coefficients of color psnr is 0.84. finally, we found that machine learning models are able to accurately predict the qoe of point clouds in ar environments. the subjective test results and questionnaire responses are available on github: https://github.com/minhkstn/qoe-and-immersion-of-dynamic-point-cloud.",AB_0432
"automatic pronunciation assessment models are regularly used in language learning applications. common methodologies for pronunciation assessment use feature-based approaches, such as the goodness-of-pronunciation (gop) approach, or deep learning speech recognition models to perform speech assessment. with the rise of transformers, pre-trained self-supervised learning (ssl) models have been utilized to extract contextual speech representations, showing improvements in various downstream tasks. in this study, we propose the end-to-end regressor (e2e-r) model for pronunciation scoring. e2e-r is trained using a two-step training process. in the first step, the pre-trained ssl model is fine-tuned on a phoneme recognition task to obtain better representations for the pronounced phonemes. in the second step, transfer learning is used to build a pronunciation scoring model that uses a siamese neural network to compare the pronounced phoneme representations to embeddings of the canonical phonemes and produce the final pronunciation scores. e2e-r achieves a pearson correlation coefficient (pcc) of 0.68, which is almost similar to the state-of-the-art gopt-paii model while eliminating the need for training on additional native speech data, feature engineering, or external forced alignment modules. to our knowledge, this work presents the first utilization of a pre-trained ssl model for end-to-end phoneme-level pronunciation scoring on raw speech waveforms. the code is available at https://github.com/ai-zahran/e2e-r.",AB_0432
"the synthesis of high-resolution remote sensing images based on text descriptions has great potential in many practical application scenarios. although deep neural networks have achieved great success in many important remote sensing tasks, generating realistic remote sensing images from text descriptions is still very difficult. to address this challenge, we propose a novel text-to-image modern hopfield network (txt2img-mhn). the main idea of txt2img-mhn is to conduct hierarchical prototype learning on both text and image embeddings with modern hopfield layers. instead of directly learning concrete but highly diverse text-image joint feature representations for different semantics, txt2img-mhn aims to learn the most representative prototypes from text-image embeddings, achieving a coarse-to-fine learning strategy. these learned prototypes can then be utilized to represent more complex semantics in the text-to-image generation task. to better evaluate the realism and semantic consistency of the generated images, we further conduct zero-shot classification on real remote sensing data using the classification model trained on synthesized images. despite its simplicity, we find that the overall accuracy in the zero-shot classification may serve as a good metric to evaluate the ability to generate an image from text. extensive experiments on the benchmark remote sensing text-image dataset demonstrate that the proposed txt2img-mhn can generate more realistic remote sensing images than existing methods. code and pre-trained models are available online (https://github.com/yonghaoxu/txt2img-mhn).",AB_0432
"the process of constructing precise geometry of human jaws from cone beam computed tomography (cbct) scans is crucial for building finite element models and treatment planning. despite the success of deep learning techniques, they struggle to accurately identify delicate features such as thin structures and gaps between the tooth-bone interfaces where periodontal ligament resides, especially when trained on limited data. therefore, segmented geometries obtained through automated methods still require extensive manual adjustment to achieve a smooth and organic 3d geometry that is suitable for simulations. in this work, we require the model to provide anatomically correct segmentation of teeth and bones which preserves the space for the periodontal ligament layers. to accomplish the task with few accurate labels, we pre-train a modified multiplanar unet as the backbone model using inferior segmentations, i.e., tooth-bone segmentation with no space in the tooth-bone interfaces, and fine-tune the model with a dedicated loss function over accurate delineations that considers the space. we demonstrate that our approach can produce proper tooth-bone segmentations with gap interfaces that are fit for simulations when applied to human jaw cbct scans. furthermore, we propose a marker-based watershed segmentation applied on the multiplanar unet probability map to separate individual tooth. this has advantages when the segmentation task is challenged by common artifacts caused by restorative materials or similar intensities in the teeth-teeth interfaces in occurrence of crowded teeth phenomenon. code and segmentation results are available at https://github.com/diku-dk/autojawsegment.",AB_0432
"brain tumors are considered one of the most crucial and threatening diseases in the world as they affect the central nervous system and the main functionalities of the brain. early diagnosis and identification of brain tumors can significantly enhance the likelihood of patient survival. generally, deep neural networks require large samples of annotated data to achieve promising results. most studies in the medical domain suffer from limited data which negatively impacts the model performance. common ways to handle such problems are to generate new samples using basic augmentation techniques, generative adversarial networks, etc. in this study, we propose several novel augmentation techniques, named regioninpaint augmentation, cutoff augmentation, and regionmix augmentation to improve the performance of brain tumor identification and facilitate the training of deep learning models with limited samples. in addition, traditional augmentation techniques are used to extend the training samples. a pre-trained vgg19 model is experimented along with the proposed augmentation techniques and achieved an accuracy of 100% on the unseen validation set of the spmri small dataset using regioninpaint and cutoff augmentation techniques together. on the other hand, the best testing accuracy achieved is 96.88% on the br35h dataset which is obtained when using all the augmentation techniques together (i.e., regioninpaint, cutoff, regionmix, and basic augmentation techniques). compared to the state-of-the-art related studies, it has been observed that our results are superior which demonstrates the efficiency of our proposed augmentation techniques and the overall proposed methodology. the source code is available at https://github.com/omarsherif200/regioninpaint-cutoff-and-regionmix-augmentation-techniques.",AB_0432
"in the era of advanced computer vision and natural language processing, the use of social media as a source of information has become even more valuable in directing aid and rescuing victims. consequently, millions of texts and images can be processed in real-time, allowing emergency responders to efficiently assess evolving crises and appropriately allocate resources. the majority of the previous detection studies are text-only or image-only based, overlooking the potential benefits of integrating both modalities. in this paper, we propose multimodal channel attention (mca) block, which employs an adaptive attention mechanism, learning to assign varying importance to each modality. we then propose a novel deep multimodal crisis categorization (dmcc) framework, which employs a two-level fusion strategy for better integration of textual and visual information. the dmcc framework consists of feature-level fusion, which is accomplished through the mca block, and score-level fusion, whereby the decisions made by the individual modalities are integrated with those of the mca model. extensive experiments on publicly available datasets demonstrate the effectiveness of the proposed framework. through a comprehensive evaluation, it was found that the proposed framework achieves a performance enhancement compared to unimodal methods. furthermore, it outperforms the current state-of-the-art methods on crisis-related categorization tasks. the code is available at https://github.com/marihamr/categorizing-crises-from-social-media-feeds-via-multimodal-channel-attention.",AB_0432
