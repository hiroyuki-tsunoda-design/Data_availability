AB,NO
"in the remote sensing field, change detection (cd) aims to identify and localize the changed regions from dual-phase images over the same places. recently, it has achieved great progress with the advances of deep learning. however, current methods generally deliver incomplete cd regions and irregular cd boundaries due to the limited representation ability of the extracted visual features. to relieve these issues, in this work we propose a novel transformer-based learning framework named transy-net for remote sensing image cd, which improves the feature extraction from a global view and combines multilevel visual features in a pyramid manner. more specifically, the proposed framework first uses the advantages of transformers in long-range dependency modeling. it can help learn more discriminative global-level features and obtain complete cd regions. then, we introduce a novel pyramid structure to aggregate multilevel visual features from transformers for feature enhancement. the pyramid structure grafted with a progressive attention module (pam) can improve the feature representation ability with additional interdependencies through spatial and channel attentions. finally, to better train the whole framework, we use the deeply supervised learning with multiple boundary-aware loss functions. extensive experiments demonstrate that our proposed method achieves a new state-of-the-art performance on four optical and two sar image cd benchmarks. the source code is released at https://github.com/drchip61/transynet.",AB_0271
"change detection is a critical task in earth observation applications. recently, deep-learning-based methods have shown promising performance and are quickly adopted in change detection. however, the widely used multiple encoders and single decoder (mesd) as well as dual-encoder-decoder (ded) architectures still struggle to effectively handle change detection well. the former has problems of bitemporal feature interference in the feature-level fusion, while the latter is inapplicable to intraclass change detection (iccd) and multiview building change detection (mvbcd). to solve these problems, we propose a new strategy with an exchanging ded (eded) structure for binary change detection with semantic guidance and spatial localization. the proposed strategy solves the problems of bitemporal feature inference in mesd by fusing bitemporal features in the decision level and the inapplicability in ded by determining changed areas using bitemporal semantic features. we build a binary change detection model based on this strategy and then validate and compare it with 18 state-of-the-art change detection methods on six datasets in three scenarios, including iccd datasets (cdd and sysu), single-view building change detection (svbcd) datasets (whu, levir-cd, and levir-cd+), and an mvbcd dataset (njds). the experimental results demonstrate that our model achieves superior performance with high efficiency and outperforms all benchmark methods with f1-scores of 97.77%, 83.07%, 94.86%, 92.33%, 91.39%, and 74.35% on cdd, sysu, whu, levir-cd, levir-cd+, and njds datasets, respectively. the code of this work will be available at https://github.com/nju-lhrs/official-sgsln.",AB_0271
"a membrane protein's functions are significantly associated with its type, so it is crucial to identify the types of membrane proteins. conventional computational methods for identifying the species of membrane proteins tend to ignore two issues: high-order correlation among membrane proteins and the scenarios of multi-modal representations of membrane proteins, which leads to information loss. to tackle those two issues, we proposed a deep residual hypergraph neural network (drhgnn), which enhances the hypergraph neural network (hgnn) with initial residual and identity mapping in this paper. we carried out extensive experiments on four benchmark datasets of membrane proteins. in the meantime, we compared the drhgnn with recently developed advanced methods. experimental results showed the better performance of drhgnn on the membrane protein classification task on four datasets. experiments also showed that drhgnn can handle the over smoothing issue with the increase of the number of model layers compared with hgnn. the code is available at https://github.com/yunfighting/identification-of-membrane-protein-types-via-deep-residual-hypergraph-neural-network.",AB_0271
"segmentation-based text detectors are flexible to capture arbitrary-shaped text regions. due to large geometry variance, it is necessary to construct effective and robust representations to identify text regions with various shapes and scales. in this paper, we focus on designing effective multi-scale contextual features for locating text instances. specially, we develop a region context module (rcm) to summarize the semantic response and adaptively extract text-region-aware information in a limited local area. to construct complementary multi-scale contextual representations, multiple rcm branches with different scales are employed and integrated via progressive fusion module (pfm). our proposed rcm and pfm serve as the plug-and-play modules which can be incorporated into existing scene text detection platforms to further boost detection performance. extensive experiments show that our methods achieve state-of-the-art performances on total-text, scut-ctw1500 and msra-td500 datasets. the code with models will become publicly available at https://github.com/wqtwjt1996/rp-text.",AB_0271
"in this article, we undertake the task of fast geo-localization of a query ground image using geo-tagged aerial images. to this end, we propose a hashing strategy that fast searches the database of geo-tagged aerial images for the ground image's matches, whose geo-tags are exploited to estimate the ground geographic location. specifically, we commence by converting the aerial images into ground-view aerial images that have the common angle of view (i.e., horizontal view) with the ground image. we then develop a feature extraction model and a hash encoder for generating hash codes for the images. based on these models, the ground image and the geo-tagged aerial images are transformed to hash codes that comprehensively reflect their visual content similarity. fast searching the geo-tagged aerial image database for the ground image's matches is conducted subject to small hamming distance between the hash codes. we extract a geographical cluster from the matched aerial images subject to their geo-tags. in this way, the geographic location of the ground image is efficiently retrieved according to the geographical cluster. experiments on two datasets validate the efficiency and effectiveness of our proposed framework. we have released our implementation code at https://github.com/taoyiminr/hashing_for_geo-localization for public evaluation.",AB_0271
"change detection (cd) in remote sensing (rs) images is a critical task that has achieved significant success by deep learning. current networks often employ pixel-based differencing, proportion, classification-based, or feature concatenation methods to represent changes of interest. however, these methods fail to effectively detect the desired changes, as they are highly sensitive to factors such as atmospheric conditions, lighting variations, and phenological variations, resulting in detection errors. inspired by the transformer structure, we adopt a cross-attention mechanism to more robustly extract feature differences between bitemporal images. the motivation of the method is based on the assumption that if there is no change between image pairs, the semantic features from one temporal image can well be represented by the semantic features from another temporal image. conversely if there is a change, there are significant reconstruction errors. therefore, a cross swin transformer-based siamese u-shaped network namely cstsunet is proposed for rs cd. cstsunet consists of encoder, difference feature extraction, and decoder. the encoder is based on a hierarchical residual network (resnet) with the siamese u-net structure, allowing parallel processing of bitemporal images and extraction of multiscale features. the difference feature extraction consists of four difference feature extraction modules that compute difference feature at multiple scales. in this module, cross swin transformer is employed in each difference feature extraction module to communicate the information of bitemporal images. the decoder takes in the multiscale difference features as input, injects details and boundaries iteratively level by level, and makes the change map more and more accurate. we conduct experiments on three public datasets, and the experimental results demonstrate that the proposed cstsunet outperforms other state-of-the-art methods in terms of both qualitative and quantitative analyses. our code is available at https://github.com/l7170/cstsunet.git.",AB_0271
"few-shot segmentation (fss) is proposed to segment unknown class targets with just a few annotated samples. most current fss methods follow the paradigm of mining the semantics from the support images to guide the query image segmentation. however, such a pattern of learning from others struggles to handle the extreme intraclass variation, preventing fss from being directly generalized to remote sensing scenes. to bridge the gap of intraclass variance, we develop a dual-mining network named dmnet for cross-image mining and self-mining, meaning that it no longer focuses solely on support images but pays more attention to the query image itself. specifically, we propose a class-public region mining (cprm) module to effectively suppress irrelevant feature pollution by capturing the common semantics between the support-query image pair. the class-specific region mining (csrm) module is then proposed to continuously mine the class-specific semantics of the query image itself in a filtering and purifying manner. in addition, to prevent the coexistence of multiple classes in remote sensing scenes from exacerbating the collapse of fss generalization, we also propose a new known-class metasuppressor (kms) module to suppress the activation of known-class objects in the sample. extensive experiments on the isaid and loveda remote sensing datasets have demonstrated that our method sets the state of the art with a minimum number of model parameters. significantly, our model with the backbone of resnet-50 achieves the mean intersection over union (miou) of 49.58% and 51.34% on isaid under 1- and 5-shot settings, outperforming the state-of-the-art method by 1.8% and 1.12%, respectively. the code is publicly available at https://github.com/hanbobizl/dmnet/.",AB_0271
"detecting objects in remote sensing images (rsis), characterized by dramatic scale variation and complex backgrounds, has always been a challenging problem. these challenges can be further summarized into three aspects: 1) scale variation among objects; 2) feature fusion misalignment due to the semantic gap between adjacent feature layers and noise from backgrounds; and 3) boundary uncertainty under ambiguous and complex backgrounds. to alleviate these problems, we first utilize a global-local feature enhancement module (glfem) to capture local features with multiple receptive fields through cheap pooling operation and obtain global features through nonlocal block, thus alleviating the scale variation issues. subsequently, attentional feature fusion alignment (affa) module is designed to align adjacent feature levels in the feature pyramid from pixel and channel levels. finally, boundary-uncertainty aware head (buah) with distribution focal loss (dfl) is adopted to solve the boundary uncertainty problems. after fusing glfem, affa, and buah modules, we obtain gab-net. gab-net outperforms state-of-the-art methods on the dior and nwpu vhr-10 datasets, achieving map scores of 73.8% and 89.8%, respectively, without adding high computational costs. the code is available at: https://github.com/hong-yu-zhang/gab-net.",AB_0271
"autoencoders (aes) are commonly utilized for acquiring low-dimensional data representations and performing data reconstruction, which makes them suitable for hyperspectral unmixing (hu). however, ae networks trained pixel by pixel and those employing localized convolutional filters disregard the global material distribution and distant interdependencies, resulting in the loss of necessary spatial feature information essential for the unmixing process. to overcome this limitation, we propose an innovative deep neural network model named u-shaped transformer network using shifted windows (ust-net). ust-net prioritizes spatial information in the scene that is more discriminative and significant by using multihead self-attention blocks based on shifted windows. unlike patch-based unmixing networks, ust-net operates on the complete image, eliminating inconsistencies associated with patches. moreover, the downsampling and upsampling stages are used to extract hyperspectral image (hsi) feature maps at different scales. this process generates a context-rich and spatially accurate abundance map without losing local details. the experimental results of one synthetic dataset and three real datasets demonstrate that ust-net significantly outperforms both traditional and several other advanced neural network methods. our code is publicly available at https://github.com/upcgit/ust-net.",AB_0271
"given two remote sensing images, the goal of visual change detection task is to detect significantly changed areas between them. existing visual change detectors usually adopt convolutional neural networks (cnns) or transformers for feature representation learning and focus on learning effective representation for the changed regions between images. although good performance can be obtained by enhancing the features of the change regions, however, these works are still limited mainly due to the ignorance of mining the unchanged background context information. it is known that one main challenge for change detection is how to obtain the consistent representations for two images involving different variations, such as spatial variation and sunlight intensity. in this work, we demonstrate that carefully mining the common background information provides an important cue to learn the consistent representations for the two images which thus obviously facilitates the visual change detection problem. based on this observation, we propose a novel visual change transformer (vct) model for visual change detection problem. to be specific, a shared backbone network is first used to extract the feature maps for the given image pair. then, each pixel of feature map is regarded as a graph node and the graph neural network (gnn) is proposed to model the structured information for coarse change map prediction. top- $k$ reliable tokens can be mined from the map and refined by using the clustering algorithm. then, these reliable tokens are enhanced by first utilizing self/cross-attention (ca) schemes and then interacting with original features via an anchor-primary attention (apa) learning module. finally, the prediction head is proposed to get a more accurate change map. extensive experiments on multiple benchmark datasets validated the effectiveness of our proposed vct model. the source code and pre-trained models are available at https://github.com/event-ahu/vct_remote_sensing_change_detection.",AB_0271
