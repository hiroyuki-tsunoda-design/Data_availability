AB,NO
"the field of surgical computer vision has undergone considerable breakthroughs in recent years with the rising popularity of deep neural network-based methods. however, standard fully-supervised approaches for training such models require vast amounts of annotated data, imposing a prohibitively high cost; especially in the clinical domain. self-supervised learning (ssl) methods, which have begun to gain traction in the general computer vision community, represent a potential solution to these annotation costs, allowing to learn useful representations from only unlabeled data. still, the effectiveness of ssl methods in more complex and impactful domains, such as medicine and surgery, remains limited and unexplored. in this work, we address this critical need by investigating four state-of-the-art ssl methods (moco v2, simclr, dino, swav) in the context of surgical computer vision. we present an extensive analysis of the performance of these methods on the cholec80 dataset for two fundamental and popular tasks in surgical context understanding, phase recognition and tool presence detection. we examine their parameterization, then their behavior with respect to training data quantities in semi-supervised settings. correct transfer of these methods to surgery, as described and conducted in this work, leads to substantial performance gains over generic uses of ssl - up to 7.4% on phase recognition and 20% on tool presence detection - as well as state-of-the-art semi-supervised phase recognition approaches by up to 14%. further results obtained on a highly diverse selection of surgical datasets exhibit strong generalization properties. the code is available at https://github.com/camma-public/selfsupsurg.",AB_0464
"background: the extent to which ambient air pollution contributes to the pathogenesis of congenital heart defects remains uncertain.objective: we investigated whether first trimester exposure to ambient fine particulate matter (pm2.5) and nitrogen dioxide (no2) was associated with the risk of critical and noncritical heart defects in a large population-based cohort of births.methods: we carried out a retrospective cohort study of children conceived between 2000 and 2016 in quebec, canada. heart defects were identified via data from the maintenance and use of data for the study of hospital clientele registry. the main exposures were average concentration of pm2.5 and no2 in a) the first trimester and b) the month of conception. exposures were estimated at the residential postal code. associations with critical and noncritical heart defects were assessed using logistic regression models, adjusted for maternal and infant characteristics. we considered single-and two-pollutant models and assessed modifying effects of maternal comorbidity, including preexisting hypertension, preeclampsia, anemia, and diabetes.results: the cohort comprised 1,342,198 newborns, including 12,715 with heart defects. exposure in the first trimester and month of conception yielded similar results; both were associated with a greater risk of heart defects. adjusted odds ratios (or) for any heart defect per interquartile range increase were 1.02 (95% ci: 1.00, 1.05) for pm2.5 and 1.10 (95% ci: 1.07, 1.13) for no2. associations with atrial septal defects were 1.08 (95% ci: 1.03, 1.14) for pm2.5 and 1.19 (95% ci: 1.12, 1.25) for no2. corresponding ors for ventricular septal defects and individual critical heart defects were not significant. pm2.5 (or = 1.11; 95% ci: 1.06, 1.17) and no2 (or = 1.23; 95% ci: 1.17, 1.31) exposure were associated with a greater risk of heart defects in mothers with comorbidity.discussion: in this population-based cohort, prenatal exposure to ambient air pollution during the first trimester was associated with an increased risk of heart defects, particularly atrial septal defects. the association with heart defects was greater in mothers with comorbidity. https://doi.org/ 10.1289/ehp11120",AB_0464
"background: short bowel syndrome (sbs) is the main cause of intestinal failure in children. objectives: this single-center study evaluated the safety and efficacy of teduglutide in pediatric patients with sbs-associated intestinal failure (sbs-if). methods: children with sbs followed at our center with >= 2 y on parenteral nutrition (pn) and with small bowel length <80 cm who had reached a plateau were consecutively included in the study. at baseline, participants underwent a clinical assessment including a 3-d stool balance analysis, which was repeated at the end of the study. teduglutide was administered subcutaneously 0.05 mg/kg/d for 48 wk. pn dependence was expressed as the pn dependency index (pndi), which is the ratio pn non-protein energy intake/ree. safety endpoints included treatment-emergent adverse events and growth parameters. results: median age at inclusion was 9.4 y (range: 5-16). the median residual sb length was 26 cm (iqr: 12-40). at baseline, the median pndi was 94% (iqr: 74-119), (median pn intake: 38.9 calories/kg/d, iqr: 26.1-48.6). at week 24, 24 (96%) children experienced a reduction of >20% of pn requirements with a median pndi = 50% (iqr: 38-81), (pn intake: 23.5 calories/kg/d iqr: 14.6-26.2), p < 0.01. at week 48, 8 children (32%) were weaned completely off pn. plasma citrulline increased from 14 mu mol/l (iqr: 8-21) at baseline to 29 mu mol/l (iqr: 17-54) at week 48 (p < 0.001). weight, height, and bmi z-scores remained stable. the median total energy absorption rate increased from 59% (iqr: 46-76) at baseline to 73% (iqr: 58-81) at week 48 (p = 0.0222). fasting and postprandial endogenous glp-2 concentrations increased at weeks 24 and 48 compared with baseline. mild abdominal pain at the early phase of treatment, stoma changes, and redness at the injection site were commonly reported. conclusions: increased intestinal absorption and pn dependency reduction were observed with teduglutide treatment in children with sbs-if. trial registration: clinicaltrials.gov nct03562130. https://clinicaltrials.gov/ct2/show/nct03562130?term=nct03562130&draw=2&rank=1",AB_0464
"transformers have proven superior performance for a wide variety of tasks since they were introduced. in recent years, they have drawn attention from the vision community in tasks such as image classification and object detection. despite this wave, an accurate and efficient multiple-object tracking (mot) method based on transformers is yet to be designed. we argue that the direct application of a transformer architecture with quadratic complexity and insufficient noise-initialized sparse queries - is not optimal for mot. we propose transcenter, a transformer-based mot architecture with dense representations for accurately tracking all the objects while keeping a reasonable runtime. methodologically, we propose the use of image-related dense detection queries and efficient sparse tracking queries produced by our carefully designed query learning networks (qln). on one hand, the dense image-related detection queries allow us to infer targets' locations globally and robustly through dense heatmap outputs. on the other hand, the set of sparse tracking queries efficiently interacts with image features in our transcenter decoder to associate object positions through time. as a result, transcenterexhibits remarkable performance improvements and outperforms by a large margin the current state-of-the-art methods in two standard mot benchmarks with two tracking settings (public/private). transcenter is also proven efficient and accurate by an extensive ablation study and, comparisons to more naive alternatives and concurrent works. the code is made publicly available at https://github.com/yihongxu/transcenter.",AB_0464
"convolutional neural networks (cnns) have been very successful at solving a variety of computer vision tasks such as object classification and detection, semantic segmentation, activity understanding, to name just a few. one key enabling factor for their great performance has been the ability to train very deep networks. despite their huge success in many tasks, cnns do not work well with non-euclidean data, which is prevalent in many real-world applications. graph convolutional networks (gcns) offer an alternative that allows for non-eucledian data input to a neural network. while gcns already achieve encouraging results, they are currently limited to architectures with a relatively small number of layers, primarily due to vanishing gradients during training. this work transfers concepts such as residual/dense connections and dilated convolutions from cnns to gcns in order to successfully train very deep gcns. we show the benefit of using deep gcns (with as many as 112 layers) experimentally across various datasets and tasks. specifically, we achieve very promising performance in part segmentation and semantic segmentation on point clouds and in node classification of protein functions across biological protein-protein interaction (ppi) graphs. we believe that the insights in this work will open avenues for future research on gcns and their application to further tasks not explored in this paper. the source code for this work is available at https://github.com/lightaime/deep_gcns_torch and https://github.com/lightaime/deep_gcns for pytorch and tensorflow implementations respectively.",AB_0464
"genomic mutations drive the pathogenesis of myelodysplastic syndromes and acute myeloid leukemia. while morphological and clinical features have dominated the classical criteria for diagnosis and classification, incorporation of molecular data can illuminate functional pathobiology. here we show that unsupervised machine learning can identify functional objective molecular clusters, irrespective of anamnestic clinico-morphological features, despite the complexity of the molecular alterations in myeloid neoplasia. our approach reflects disease evolution, informed classification, prognostication, and molecular interactions. we apply machine learning methods on 3588 patients with myelodysplastic syndromes and secondary acute myeloid leukemia to identify 14 molecularly distinct clusters. remarkably, our model shows clinical implications in terms of overall survival and response to treatment even after adjusting to the molecular international prognostic scoring system (ipss-m). in addition, the model is validated on an external cohort of 412 patients. our subclassification model is available via a web-based open-access resource (https://drmz.shinyapps.io/mds_latent). myeloid neoplasias can show complex mutation patterns and molecular features. here, the authors apply machine learning to classify risk groups of myeloid neoplasia which may correlate with differential response to treatment.",AB_0464
"background: sepsis is a dysfunctional host response to infection. the syndrome leads to millions of deaths annually (19.7% of all deaths in 2017) and is the cause of most deaths from severe covid infections. high throughput sequencing or 'omics' experiments in molecular and clinical sepsis research have been widely utilized to identify new diagnostics and therapies. transcriptomics, quantifying gene expression, has dominated these studies, due to the efficiency of measuring gene expression in tissues and the technical accuracy of technologies like rna-seq. objective: most of these studies seek to uncover novel mechanistic insights into sepsis pathogenesis and diagnostic gene signatures by identifying genes differentially expressed between two or more relevant conditions. however, little effort has been made, to date, to aggregate this knowledge from such studies. in this study we sought to build a compendium of previously described gene sets that combines knowledge gained from sepsis-associated studies. this would enable the identification of genes most associated with sepsis pathogenesis, and the description of the molecular pathways commonly associated with sepsis. methods: pubmed was searched for studies using transcriptomics to characterize acute infection/sepsis and severe sepsis (i.e., sepsis combined with organ failure). several studies were identified that used transcriptomics to identify differentially expressed (de) genes, predictive/prognostic signatures, and underlying molecular responses and pathways. the molecules included in each gene set were collected, in addition to the relevant study metadata (e.g., patient groups used for comparison, sample collection time point, tissue type, etc.). results: after performing extensive literature curation of 74 sepsis-related publications involving transcriptomics, 103 unique gene sets (comprising 20,899 unique genes) from thousands of patients were collated together with associated metadata. frequently described genes included in gene sets as well as the molecular mechanisms they were involved in were identified. these mechanisms included neutrophil degranulation, generation of second messenger molecules, il-4 and -13 signaling, and il-10 signaling among many others. the database, which we named septisearch, is made available in a web application created using the shiny framework in r, (available at https://septisearch.ca). conclusions: septisearch provides members of the sepsis community the bioinformatic tools needed to leverage and explore the gene sets contained in the database. this will allow the gene sets to be further scrutinized and analyzed for their enrichment in user-submitted gene expression data and used for validation of in-house gene sets/signatures.",AB_0464
"objective: the study has dual objectives. our first objective (1) is to develop a community-of-practice-based evaluation methodology for knowledge-intensive computational methods. we target a whitebox analysis of the computational methods to gain insight on their functional features and inner workings. in more detail, we aim to answer evaluation questions on (i) support offered by computational methods for functional features within the application domain; and (ii) in-depth characterizations of the underlying computational processes, models, data and knowledge of the computational methods. our second objective (2) involves applying the evaluation methodology to answer questions (i) and (ii) for knowledge-intensive clinical decision support (cds) methods, which operationalize clinical knowledge as computer interpretable guidelines (cig); we focus on multimorbidity cig-based clinical decision support (mgcds) methods that target multimorbidity treatment plans. materials and methods: our methodology directly involves the research community of practice in (a) identifying functional features within the application domain; (b) defining exemplar case studies covering these features; and (c) solving the case studies using their developed computational methods-research groups detail their solutions and functional feature support in solution reports. next, the study authors (d) perform a qualitative analysis of the solution reports, identifying and characterizing common themes (or dimensions) among the computational methods. this methodology is well suited to perform whitebox analysis, as it directly involves the respective developers in studying inner workings and feature support of computational methods. moreover, the established evaluation parameters (e.g., features, case studies, themes) constitute a re-usable benchmark framework, which can be used to evaluate new computational methods as they are developed. we applied our community-of-practice-based evaluation methodology on mgcds methods. results: six research groups submitted comprehensive solution reports for the exemplar case studies. solutions for two of these case studies were reported by all groups. we identified four evaluation dimensions: detection of adverse interactions, management strategy representation, implementation paradigms, and human-in-the-loop support. based on our whitebox analysis, we present answers to the evaluation questions (i) and (ii) for mgcds methods.discussion: the proposed evaluation methodology includes features of illuminative and comparison-based approaches; focusing on understanding rather than judging/scoring or identifying gaps in current methods. it involves answering evaluation questions with direct involvement of the research community of practice, who participate in setting up evaluation parameters and solving exemplar case studies. our methodology was successfully applied to evaluate six mgcds knowledge-intensive computational methods. we established that, while the evaluated methods provide a multifaceted set of solutions with different benefits and drawbacks, no single mgcds method currently provides a comprehensive solution for mgcds.conclusion: we posit that our evaluation methodology, applied here to gain new insights into mgcds, can be used to assess other types of knowledge-intensive computational methods and answer other types of evaluation questions. our case studies can be accessed at our github repository (https://github.com/william-vw/mgcds).",AB_0464
"recent advances in dna sequencing technologies particularly long-read sequencing, greatly improved genomes assembly. however, this has created discrepancies between published annotations and epigenome tracks, which have not been updated to keep pace with the new assemblies. here, we used the latest improved telomere-to-telomere assembly of the model pennate diatom phaeodactylum tricornutum to lift over the gene models from phatr3, a previously annotated reference genome. we used the lifted genes annotation and newly published transposable elements to map the epigenome landscape, namely dna methylation and post-translational modifications of histones. this provides the community with phaeoepiview, a browser that allows the visualization of epigenome data and transcripts on an updated and contiguous reference genome, to better understand the biological significance of the mapped data. we updated previously published histone marks with a more accurate peak calling using mono instead of poly(clonal) antibodies and deeper sequencing. phaeoepiview (https://phaeoepiview.univ-nantes.fr) will be continuously updated with the newly published epigenomic data, making it the largest and richest epigenome browser of any stramenopile. in the upcoming era of molecular environmental studies, where epigenetics plays a significant role, we anticipate that phaeoepiview will become a widely used tool.",AB_0464
"long-read sequencing technologies have improved significantly since their emergence. their read lengths, potentially spanning entire transcripts, is advantageous for reconstructing transcriptomes. existing long-read transcriptome assembly methods are primarily reference-based and to date, there is little focus on reference-free transcriptome assembly. we introduce rna-bloom2 [https://github.com/bcgsc/rna-bloom], a reference-free assembly method for long-read transcriptome sequencing data. using simulated datasets and spike-in control data, we show that the transcriptome assembly quality of rna-bloom2 is competitive to those of reference-based methods. furthermore, we find that rna-bloom2 requires 27.0 to 80.6% of the peak memory and 3.6 to 10.8% of the total wall-clock runtime of a competing reference-free method. finally, we showcase rna-bloom2 in assembling a transcriptome sample of picea sitchensis (sitka spruce). since our method does not rely on a reference, it further sets the groundwork for large-scale comparative transcriptomics where high-quality draft genome assemblies are not readily available. most existing long-read transcriptome assembly methods rely on reference genomes and transcript annotations, while reference-free methods remain scarce. here, nip et al. introduce rna-bloom2, a reference-free method that requires substantially less memory and runtime than other reference-free methods.",AB_0464
