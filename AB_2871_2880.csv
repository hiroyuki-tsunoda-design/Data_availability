AB,NO
"infrared (ir) ship tracking is becoming increasingly important in various applications. however, it remains a challenging task as the information that can be obtained from ir images is limited. aiming at enhancing ir ship tracking accuracy, we propose an innovative approach by presenting feature integration module (fim) and backup matching module (bmm). fim takes appearance feature, complete intersection over union (ciou), and motion direction metrics (mdms) into account. regarding appearance feature extraction, an end-to-end characteristic learning strategy with a cross-guided multigranularity fusion network is proposed to obtain more integral appearance features and enhance reidentification (re-id) accuracy, which helps to distinguish individual ir ship targets better. besides, a backup matching strategy is then used to match the unmatched tracks and detections after cascaded matching. virtual trajectories are generated for the matched tracks to optimize parameters by parameter optimization module (pom). the accumulation of errors caused by the lack of observations in the kalman filter (kf) is reduced. thus, the position of ir ships can be estimated more accurately, and more robust ir ship tracking can be achieved. in addition, we present a sequential frame ir ship tracking (sfist) dataset, providing the first public benchmark for testing ir ship tracking performance. experimental results indicate that the multiple object tracking accuracy (mota), multiple objects tracking precision (motp), and identity switch (ids) of the proposed method are 73.441, 80.826, and 32, respectively, outperforming other state-of-the-art methods. this demonstrates the superior robustness of the proposed method, particularly when the ir ships are occluded or the target texture information is lacking. our dataset is available at https://github.com/echo-sky/sfist.",AB_0288
"the appearances of children are inherited from their parents, whichmakes it feasible to predict them. predicting realistic children's faces may help settle many social problems, such as ageinvariant face recognition, kinship verification, and missing child identification. it can be regarded as an image-to-image translation task. existing approaches usually assume domain information in the image-to-image translation can be interpreted by style, i.e., the separation of image content and style. however, such separation is improper for the child face prediction, because the facial contours between children and parents are not the same. to address this issue, we propose a new disentangled learning strategy for children's face prediction. we assume that children's faces are determined by genetic factors (compact family features, e.g., face contour), external factors (facial attributes irrelevant to prediction, such as moustaches and glasses), and variety factors (individual properties for each child). on this basis, we formulate predictions as a mapping from parents' genetic factors to children's genetic factors, and disentangle them from external and variety factors. in order to obtain accurate genetic factors and perform the mapping, we propose a childpredictor framework. it transfers human faces to genetic factors by encoders and back by generators. then, it learns the relationship between the genetic factors of parents and children through a mapping function. to ensure the generated faces are realistic, we collect a large family face database to train childpredictor and evaluate it on the ff-database validation set. experimental results demonstrate that childpredictor is superior to other well-known image-to-image translation methods in predicting realistic and diverse child faces. implementation codes can be found at https:// github.com/ zhaoyuzhi/childpredictor.",AB_0288
"recent deep learning methods for change detection focus on excavating more discriminative context within individual images. however, due to seasonal change, noise, and so on, the appearance of objects tends to be more heterogeneous among various scenes. consequently, the above intraimage context is inadequate to represent specific-category objects and pseudo changes would be inevitable in detection results. to deal with this issue, we propose a context aggregation network (canet) to mine interimage context over all training images for further enhancing intraimage context. specifically, a siamese network attached with temporal attention modules is served as a feature encoder to extract multiscale temporal features from bitemporal images. then, a context extraction module is devised to capture long-range spatial-channel context within individual images. meanwhile, context representations of underlying categories in the scene are inferred using all training images in an unsupervised manner. finally, these two kinds of contextual information are aggregated to one which is subsequently fed into a multiscale fusion module to produce the detection map. canet is compared with several state-of-the-art methods on three benchmark datasets, including the season-varying change detection (svcd) dataset, the sun yat-sen university change detection (sysu-cd) dataset, and the learning vision and remote sensing laboratory building change detection (levir-cd) dataset. it is demonstrated that our method outperforms all comparison methods in terms of f1, overall accuracy (oa), and intersection of union (iou). the results of canet on three datasets are available at https://github.com/nuistzf/canet-for-change-detection and codes will be made public soon.",AB_0288
"current state-of-the-art remote sensing salient object detectors always require high-resolution spatial context to ensure excellent performance, which incurs enormous computation costs and hinders real-time efficiency. in this work, we propose a universal super-resolution-assisted learning (sral) framework to boost performance and accelerate the inference efficiency of existing approaches. to this end, we propose to reduce the spatial resolution of the input remote sensing images (rsis), which is model-agnostic and can be applied to existing algorithms without extra computation cost. specifically, a transposed saliency detection decoder (tsdd) is designed to upsample interim features progressively. on top of it, an auxiliary sr decoder (asrd) is proposed to build a multitask learning (mtl) framework to investigate an efficient complementary paradigm of saliency detection and sr. furthermore, a novel task-fusion guidance module (tfgm) is proposed to effectively distill domain knowledge from the sr auxiliary task to the salient object detection task in optical rsis. the presented asrd and tfgm can be omitted in the inference phase without any extra computational budget. extensive experiments on three datasets show that the presented sral with 224 x 224 input is superior to more than 20 algorithms. moreover, it can be successfully generalized to existing typical networks with significant accuracy improvements in a parameter-free manner. codes and models are available at https://github.com/lyf0801/sral.",AB_0288
"benefiting from effective global information interaction, vision transformers (vits) have been widely used in the building extraction task. however, buildings in remote sensing (rs) images usually differ greatly in size. mainstream vit-based segmentation models for rs images are based on swin transformer, which lacks multiscale information inside the vit block. in addition, they only connect the output of the entire vit encoder block to the decoder, which ignore the similarity information of the attention maps inside the vit encoder block and are unable to provide better global dependencies for the decoder. to solve the above problems, we introduce a novel shunted transformer, which enables the model to capture multiscale information internally while fully establishing global dependencies, to build a pure vit-based u-shaped model for building extraction. furthermore, unlike the previous single-skip-connection structure of the u-shaped methods, we build a novel dual skip connection structure inside the model. it simultaneously transmits the attention maps inside the vit encoder block and its entire output to the decoder, thereby fully mining the information of the vit encoder block and providing better global information guidance for the decoder. thus, our model is named shunted dual skip connection unet (sdsc-unet). we also design a feature fusion module called dual skip upsample fusion module (dsufm) to aggregate the information. our model yields the state-of-the-art (sota) performance [83.02% intersection over union (iou)] on the inria aerial image labeling dataset. code will be available at https://github.com/stdcoutzrh/buildingextraction.",AB_0288
"hyperspectral image (hsi) anomaly detection (ad) generally considers background pixels as low-rank distribution and anomaly pixels as sparse distribution. however, it is usually difficult to construct an accurate background dictionary for the background pixels composed of different land covers, and completely separate sparse anomaly targets from various complicated background pixels with complex mixed noise interference. to address these challenges, we propose an antinoise hierarchical mutual-incoherence-induced discriminative learning (ahmid) method for the ad of hsi. a structural incoherence constraint is designed to constrain the inherent dissimilarity and incoherence between the background and anomalies for improving their separability. then, a first-order statistic constraint is conducted on targets to enhance the anomaly representation, and a decentralization constraint is used on the background to suppress the background representation. meanwhile, a mixed noise model is constructed by $\ell _{1,1}$ -norm and frobenius norm to improve the antinoise performance. finally, a hierarchical alternative strategy is developed to gradually optimize the background and anomalies. experiments on six hsi ad datasets show that the proposed method outperforms a few state-of-the-art ad algorithms. code: https://github.com/halongl/had-ahmid",AB_0288
"airport runway extraction is important for daily maintenance of civil airports, precision strikes at military airports, and safe landing of uavs. however, different from the typical object extraction tasks, objects such as runways, taxiways, and roads share extremely similar attributes in terms of material, texture, and shape, making it challenging to differentiate between them. besides, the gradients of some runway boundaries change slowly and are difficult to extract accurately. to address these problems, a dual-field-of-view context and boundary perception network (dcbp) is proposed, which can combine long-short-term contexts and boundary information of runways. specifically, the dual-field-of-view context aggregation (dca) module can discover semantic representations from two perspectives by exploring the interaction between long-term and short-term contexts. meanwhile, the detailed features learned from the high-resolution branch are used to guide the boundary perception (bp) module in learning the location of the runway boundaries. in addition, we provide the research community with a precisely labeled dataset, named the airport runway segmentation (ars) dataset, to advance runway segmentation with remote sensing images. extensive experiments on the benchmark demonstrated that dcbp achieved more accurate extraction results and obtained sharper boundaries on various airport runways than other methods. the code and dataset are available at https://github.com/weiai1996/dcbp.",AB_0288
"supervised remote sensing (rs) image segmentation has achieved remarkable success with large amounts of manually labeled data, which may be difficult to acquire in some practical application scenarios. semisupervised rs image segmentation can efficiently utilize the knowledge embedded in unlabeled data to improve recognition performance, which is of great significance for the generalization application of segmentation models. in this work, we propose an end-to-end semisupervised rs image segmentation method based on weak-to-strong consistency learning (wscl). specifically, a common strong data augmentation technique for image segmentation is introduced to provide powerful input perturbation to decouple self-biased cognition. by forcing weakly augmented and strongly augmented perspectives from the same sample to be consistent, wscl not only enables the model to steadily learn knowledge contained in unlabeled data but also alleviates overfitting. in addition, a novel sparse dual-view cross-sample image generation method is presented to generate new training samples, which helps provide a more comprehensive diversity of perturbations. furthermore, an adaptive reweighting strategy based on the entropy maps of the outputs of strongly perturbed samples is proposed to suppress noise, guiding the training process in a positive direction. extensive experiments demonstrate the significant advantage of wscl over other advanced methods, achieving new state-of-the-art under several evaluation metrics on dfc22, isaid, mer, msl, vaihingen, and gid-15 datasets. the source code is open-sourced at https://github.com/xiaoqiang-lu/wscl.",AB_0288
"prohibited item segmentation has a wide range of applications in the security check field, such as computer-aided screening, threat image projection and material discrimination. however, the severe object overlapping in x-ray baggage images restricts the performance of common cnn-based segmentation methods greatly. worse, no public dataset can be used to promote research in this challenging and promising area. in this paper, to cope with these problems, we present the first prohibited item x-ray segmentation dataset named pixray. pixray comprises 5,046 x-ray images, in which 15 classes of 15,201 prohibited items are annotated as instance-level masks. besides, we contribute a dense de-overlap attention snake (ddoas) in the context of deep learning for automated and real-time prohibited item segmentation. ddoas mainly includes a dense de-overlap module (ddom) and an attention deforming module (adm). specifically, ddom is designed to infer prohibited item information accurately from extreme background overlaps through dense reversed connections. adm aims to improve the low learning efficiency introduced by large variations in shapes and sizes among different prohibited items. comprehensive evaluation on the pixray shows the effectiveness and superiority of ddom and adm. ddom excels at recognizing prohibited items from complex backgrounds than other in-domain methods and achieves consistent performance gain over various network backbones, extending the idea of tackling overlapping images data. adm can ease the model training and further refine the mask quality. furthermore, out-of-domain experiments prove that ddoas can also be applied to natural images and achieves comparable performance to the state-of-the-art methods, which implies its potential applications in other fields. the dataset and source code are available at https://github.com/mbwslib/ddoas.",AB_0288
"although remarkable progress has been made by existing federated learning (fl) platforms to provide infrastructures for development, these platforms may not well tackle the challenges brought by various types of heterogeneity. to fill this gap, in this paper, we propose a novel fl platform, named federatedscope, which employs an event-driven architecture to provide users with great flexibility to independently describe the behaviors of different participants. such a design makes it easy for users to describe participants with various local training processes, learning goals and backends, and coordinate them into an fl course with synchronous or asynchronous training strategies. towards an easy-to-use and flexible platform, federatedscope enables rich types of plug-in operations and components for efficient further development, and we have implemented several important components to better help users with privacy protection, attack simulation and auto-tuning. we have released federatedscope at https://github.com/alibaba/federatedscope to promote academic research and industrial deployment of federated learning in a wide range of scenarios.",AB_0288
