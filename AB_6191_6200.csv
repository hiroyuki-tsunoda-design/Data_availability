AB,NO
"motivation: scientists seeking to understand the genomic basis of bacterial phenotypes, such as antibiotic resistance, today have access to an unprecedented number of complete and nearly complete genomes. making sense of these data requires computational tools able to perform multiple-genome comparisons efficiently, yet currently available tools cannot scale beyond several tens of genomes. results: we describe prawns, an efficient and scalable tool for multiple-genome analysis. prawns defines a concise set of genomic features (metablocks), as well as pairwise relationships between them, which can be used as a basis for large-scale genotype-phenotype association studies. we demonstrate the effectiveness of prawns by identifying genomic regions associated with antibiotic resistance in acinetobacter baumannii. availability and implementation: prawns is implemented in c++ and python3, licensed under the gplv3 license, and freely downloadable from github (https://github.com/kiranjavkar/prawns.git). contact: mpop@umd.edu supplementary information: supplementary data are available at bioinformatics online.",AB_0620
"igv.js is an embeddable javascript implementation of the integrative genomics viewer (igv). it can be easily dropped into any web page with a single line of code and has no external dependencies. the viewer runs completely in the web browser, with no backend server and no data pre-processing required. availability and implementation the igv.js javascript component can be installed from npm at https://www.npmjs.com/package/igv. the source code is available at https://github.com/igvteam/igv.js under the mit open-source license. igv-web, the end-user application built around igv.js, is available at https://igv.org/app. the source code is available at https://github.com/igvteam/igv-webapp under the mit open-source license. contact: jrobinso@ucsd.edu supplementary information: supplementary information is available at bioinformatics online.",AB_0620
"motivation: in this work, we present an analytical method for quantifying both single-cell morphologies and cell network topologies of tumor cell populations and use it to predict 3d cell behavior. results: we utilized a supervised deep learning approach to perform instance segmentation on label-free live cell images across a wide range of cell densities. we measured cell shape properties and characterized network topologies for 136 single-cell clones derived from the yumm1.7 and yummer1.7 mouse melanoma cell lines. using an unsupervised clustering algorithm, we identified six distinct morphological subclasses. we further observed differences in tumor growth and invasion dynamics across subclasses in an in vitro 3d spheroid model. compared to existing methods for quantifying 2d or 3d phenotype, our analytical method requires less time, needs no specialized equipment and is capable of much higher throughput, making it ideal for applications such as high-throughput drug screening and clinical diagnosis. availability and implementation: https://github.com/trevor-chan/melanoma_networkmorphology. contact: michael.mak@yale.edu supplementary information: supplementary data are available at bioinformatics online.",AB_0620
"background: gene-based association tests provide a useful alternative and complement to the usual single marker association tests, especially in genome-wide association studies (gwas). the way of weighting for variants in a gene plays an important role in boosting the power of a gene-based association test. appropriate weights can boost statistical power, especially when detecting genetic variants with weak effects on a trait. one major limitation of existing gene-based association tests lies in using weights that are predetermined biologically or empirically. this limitation often attenuates the power of a test. on another hand, effect sizes or directions of causal genetic variants in real data are usually unknown, driving a need for a flexible yet robust methodology of gene based association tests. furthermore, access to individual-level data is often limited, while thousands of gwas summary data are publicly and freely available. results: to resolve these limitations, we propose a combination test named as owc which is based on summary statistics from gwas data. several traditional methods including burden test, weighted sum of squared score test [ssu], weighted sum statistic [wss], snp-set kernel association test [skat], and the score test are special cases of owc. to evaluate the performance of owc, we perform extensive simulation studies. results of simulation studies demonstrate that owc outperforms several existing popular methods. we further show that owc outperforms comparison methods in real-world data analyses using schizophrenia gwas summary data and a fasting glucose gwas meta-analysis data. the proposed method is implemented in an r package available at https://github.com/xuexia-wang/owc-r- package conclusions: we propose a novel gene-based association test that incorporates four different weighting schemes (two constant weights and two weights proportional to normal statistic z) and includes several popular methods as its special cases. results of the simulation studies and real data analyses illustrate that the proposed test, owc, outperforms comparable methods in most scenarios. these results demonstrate that owc is a useful tool that adapts to the underlying biological model for a disease by weighting appropriately genetic variants and combination of well-known gene-based tests.",AB_0620
"bayesian posterior inference of modern multi-probe cosmological analyses incurs massive computational costs. for instance, depending on the combinations of probes, a single posterior inference for the dark energy survey (des) data had a wall-clock time that ranged from 1 to 21 days using a state-of-the-art computing cluster with 100 cores. these computational costs have severe environmental impacts and the long wall-clock time slows scientific productivity. to address these difficulties, we introduce linna: the likelihood inference neural network accelerator. relative to the baseline des analyses, linna reduces the computational cost associated with posterior inference by a factor of 8-50. if applied to the first-year cosmological analysis of rubin observatory's legacy survey of space and time (lsst y1), we conservatively estimate that linna will save more than u.s. $300, 000 on energy costs, while simultaneously reducing co2 emission by 2, 400 tons. to accomplish these reductions, linna automatically builds training data sets, creates neural network emulators, and produces a markov chain that samples the posterior. we explicitly verify that linna accurately reproduces the first-year des (des y1) cosmological constraints derived from a variety of different data vectors with our default code settings, without needing to retune the algorithm every time. further, we find that linna is sufficient for enabling accurate and efficient sampling for lsst y10 multi-probe analyses. we make linna publicly available at https://github.com/chto/linna, to enable others to perform fast and accurate posterior inference in contemporary cosmological analyses.",AB_0620
"in blurry images, the degree of image blurs may vary drastically due to different factors, such as varying speeds of shaking cameras and moving objects, as well as defects of the camera lens. however, current end-to-end models failed to explicitly take into account such diversity of blurs. this unawareness compromises the specialization at each blur level, yielding sub-optimal deblurred images as well as redundant post-processing. therefore, how to specialize one model simultaneously at different blur levels, while still ensuring coverage and generalization, becomes an emerging challenge. in this work, we propose ada-deblur, a super-network that can be applied to a broad spectrum of blur levels with no re-training on novel blurs. to balance between individual blur level specialization and wide-range blur levels coverage, the key idea is to dynamically adapt the network architectures from a single well-trained super-network structure, targeting flexible image processing with different deblurring capacities at test time. extensive experiments demonstrate that our work outperforms strong baselines by demonstrating better reconstruction accuracy while incurring minimal computational overhead. besides, we show that our method is effective for both synthetic and realistic blurs compared to these baselines. the performance gap between our model and the state-of-the-art becomes more prominent when testing with unseen and strong blur levels. specifically, our model demonstrates surprising deblurring performance on these images with psnr improvements of around 1 db. our code is publicly available at https://github.com/wuqiuche/ada-deblur.",AB_0620
"there are demographic biases present in current facial recognition (fr) models. to measure these biases across different ethnic and gender subgroups, we introduce our balanced faces in the wild (bfw) dataset. this dataset allows for the characterization of fr performance per subgroup. we found that relying on a single score threshold to differentiate between genuine and imposters sample pairs leads to suboptimal results. additionally, performance within subgroups often varies significantly from the global average. therefore, specific error rates only hold for populations that match the validation data. to mitigate imbalanced performances, we propose a novel domain adaptation learning scheme that uses facial features extracted from state-of-the-art neural networks. this scheme boosts the average performance and preserves identity information while removing demographic knowledge. removing demographic knowledge prevents potential biases from affecting decision-making and protects privacy by eliminating demographic information. we explore the proposed method and demonstrate that subgroup classifiers can no longer learn from features projected using our domain adaptation scheme. for access to the source code and data, please visit https://github.com/visionjo/facerec-bias-bfw.",AB_0620
"perception-based image analysis technologies can be used to help visually impaired people take better quality pictures by providing automated guidance, thereby empowering them to interact more confidently on social media. the photographs taken by visually impaired users often suffer from one or both of two kinds of quality issues: technical quality (distortions), and semantic quality, such as framing and aesthetic composition. here we develop tools to help them minimize occurrences of common technical distortions, such as blur, poor exposure, and noise. we do not address the complementary problems of semantic quality, leaving that aspect for future work. the problem of assessing, and providing actionable feedback on the technical quality of pictures captured by visually impaired users is hard enough, owing to the severe, commingled distortions that often occur. to advance progress on the problem of analyzing and measuring the technical quality of visually impaired user-generated content (vi-ugc), we built a very large and unique subjective image quality and distortion dataset. this new perceptual resource, which we call the live-meta vi-ugc database, contains 40k real-world distorted vi-ugc images and 40k patches, on which we recorded 2.7m human perceptual quality judgments and 2.7m distortion labels. using this psychometric resource we also created an automatic limited vision picture quality and distortion predictor that learns local-to-global spatial quality relationships, achieving state-of-the-art prediction performance on vi-ugc pictures, significantly outperforming existing picture quality models on this unique class of distorted picture data. we also created a prototype feedback system that helps to guide users to mitigate quality issues and take better quality pictures, by creating a multi-task learning framework. the dataset and models can be accessed at: https://github.com/mandal-cv/visimpaired.",AB_0620
"we present the outcomes of a recent large-scale subjective study of mobile cloud gaming video quality assessment (mcg-vqa) on a diverse set of gaming videos. rapid advancements in cloud services, faster video encoding technologies, and increased access to high-speed, low-latency wireless internet have all contributed to the exponential growth of the mobile cloud gaming industry. consequently, the development of methods to assess the quality of real-time video feeds to end-users of cloud gaming platforms has become increasingly important. however, due to the lack of a large-scale public mobile cloud gaming video dataset containing a diverse set of distorted videos with corresponding subjective scores, there has been limited work on the development of mcg-vqa models. towards accelerating progress towards these goals, we created a new dataset, named the live-meta mobile cloud gaming (live-meta-mcg) video quality database, composed of 600 landscape and portrait gaming videos, on which we collected 14,400 subjective quality ratings from an in-lab subjective study. additionally, to demonstrate the usefulness of the new resource, we benchmarked multiple state-of-the-art vqa algorithms on the database. the new database will be made publicly available on our website: https://live.ece.utexas.edu/research/live-meta-mobile-cloud-gaming/index.html",AB_0620
"we have carried out a series of numerical experiments designed to evaluate the sensitivity of global magnetohydrodynamic simulations to changes in ionospheric conductance. we multiplied the precipitating energy fluxes due to both strong pitch angle scattering and parallel currents by constant factors in the robinson et al. (1987, ) model and used them as input to a simulated substorm interval on 14 march 2008. we also used the kaeppler et al. (2015, https://doi. org/10.1002/2015ja021396) model. when we reduced the energy fluxes by a factor of about three (0.34), the agreement on substorm location was best. larger conductances led to onset at later local times. for multiplication factors greater than one, the magnetotail became line tied and no flows entered the inner magnetotail region. relatively small (<50%) differences in the conductances lead to major changes in the tail configuration. we calculated auroral indices from the simulations and compared them to observations. none of the models reproduced the dp1 system but, we obtained much better agreement with the observed dp2 system. the model with the basic robinson formula gave the best agreement with the dp2 system. we also ran a generic simulation with constant solar wind and southward imf using the basic robinson et al. model. the results were very similar to those of the event simulation suggesting that the changes were due to the ionospheric boundary condition. in the simulation, the energy flux obtained by assuming strong pitch angle scattering makes a larger contribution to the conductance than the field-aligned current energy flux.",AB_0620
