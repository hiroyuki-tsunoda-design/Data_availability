AB,NO
"multimodal learning has shown great potential in remote sensing image classification and attracted increasing interest in the community. although it is preferable to collect multiple modalities for training, not all of them are available in practical scenarios. to this end, we propose a general diversity-guided distillation network (dgdnet) with modality-center regularization (mcr) to facilitate accurate model inference when modalities are missing. compared with existing modality reconstruction methods, dgdnet does not need prior knowledge of the missing modality and can handle various missing modalities via only one model. specifically, dgdnet consists of two components: the deployment network extracting the modality-invariant representation for robust inference and the teacher network transferring comprehensive multimodal information to the deployment network. this enables the deployment network to learn the modality invariant and specific information simultaneously while maintaining robustness for incomplete modality input. in particular, we design a novel diversity-guided distillation (dgd) method that transfers knowledge by matching the feature diversity. this helps overcome the representation heterogeneity when encouraging the deployment network to learn modality-specific information. besides, an mcr strategy is proposed to address the unbalanced training of teacher and deployment networks by constraining the intra-class inter-modality variations. this helps alleviate the underfitting for weak modality, improving the model performance. finally, extensive experiments demonstrate that the proposed dgdnet can address the problem of missing modalities effectively and achieves state-of-the-art performance. the code is available at https://github.com/shicaiwei123/tgrs-dgdnet.",AB_0268
"the task of unmanned aerial vehicle (uav)-view geo-localization is to estimate the localization of a query satellite/drone image by matching it against a reference dataset consisting of drone/satellite images. though tremendous strides have been made in feature alignment between satellite and drone views, vast differences in both inter and intraclass due to changes in viewpoint, altitude, and lighting remain a huge challenge. in this article, a style alignment-based dynamic observation method for uav-view geo-localization is proposed to meet the above challenges from two perspectives: visual style transformation and surrounding noise control. specifically, we introduce a style alignment strategy (sas) to transform the diverse visual style of drone-view images into a unified satellite images visual style. then, a dynamic observation module is designed to evaluate the spatial distribution of images by mimicking human observation habits. it is featured by the hierarchical attention block (hab) with a dual-square-ring stream structure, to reduce surrounding noise and geographical deformation. in addition, we propose a deconstruction loss (dc loss) to push away features of different geo-tags and squeeze knowledge from unmatched images by correlation calculation. the experimental results demonstrate the sota performance of our model on benchmarked datasets. in particular, when compared to the prior art on university-1652, our results surpass the best of them (fsra), while only requiring 2x fewer parameters. the code will be released at https://github.com/xcco1/sa_dom.",AB_0268
"human history is also the history of the fight against viral diseases. from the eradication of viruses to coexistence, advances in biomedicine have led to a more objective understanding of viruses and a corresponding increase in the tools and methods to combat them. more recently, antiviral peptides (avps) have been discovered, which due to their superior advantages, have achieved great impact as antiviral drugs. therefore, it is very necessary to develop a prediction model to accurately identify avps. in this paper, we develop the iavps-resbi model using k-spaced amino acid pairs (ksaap), encoding based on grouped weight (ebgw), enhanced grouped amino acid composition (egaac) based on the n5c5 sequence, composition, transition and distribution (ctd) based on physicochemical properties for multi-feature extraction. then we adopt bidirectional long short-term memory (bilstm) to fuse features for obtaining the most differentiated information from multiple original feature sets. finally, the deep model is built by combining improved residual network and bidirectional gated recurrent unit (bigru) to perform classification. the results obtained are better than those of the existing methods, and the accuracies are 95.07, 98.07, 94.29 and 97.50% on the four datasets, which show that iavps-resbi can be used as an effective tool for the identification of antiviral peptides. the datasets and codes are freely available at https://github.com/yunyunliang88/iavps-resbi.",AB_0268
"camouflage is a common visual phenomenon, which refers to hiding the foreground objects into the background images, making them briefly invisible to the human eye. previous work has typically been implemented by an iterative optimization process. however, these methods struggle in 1) efficiently generating camouflage images using foreground and background with flexible structure; 2) camouflaging foreground objects to regions with multiple appearances (e.g. the junction of the vegetation and the mountains), which limit their practical application. to address these problems, this paper proposes a novel location-free camouflage generation network (lcg-net) that fuse high-level features of foreground and background image, and generate result by one inference. specifically, a position-aligned structure fusion (psf) module is devised to guide structure feature fusion based on the point-to-point structure similarity of foreground and background, and introduce local appearance features point-by-point. to retain the necessary identifiable features, a new immerse loss is adopted under our pipeline, while a background patch appearance loss is utilized to ensure that the hidden objects look continuous and natural at regions with multiple appearances. experiments show that our method has results as satisfactory as state-of-the-art in the single-appearance regions and are less likely to be completely invisible, but far exceed the quality of the state-of-the-art in the multi-appearance regions. moreover, our method is hundreds of times faster than previous methods. benefitting from the unique advantages of our method, we provide some downstream applications for camouflage generation, which show its potential. the related code and dataset will be released at https://github.com/tale17/lcg-net.",AB_0268
"with the abundant emergence of remote sensing (rs) data sources, multimodal remote sensing observation has become an active field. extracting valuable information from multimodal data has the potential to make a significant contribution to applications such as urban planning and monitoring. however, existing studies are deficient in extracting spectral and spatial features from hyperspectral (hs) remote sensing data. meanwhile, the method of fusing multimodal features has limitations and poses a challenge to the convergence of the model loss function, which increases the complexity of the network model optimization process. therefore, this article proposes an adaptive multiscale spatial-spectral enhancement network for classification of hyperspectral and light detection and ranging (lidar) data called amsse-net. first, we perform deep mining of spectral features in hyperspectral images (hsis) by the involution operator. the main idea is to take full advantage of the involution operator in characterizing spectral features by using the property that the convolution kernel shares the feature channels within the group. furthermore, the multibranching approach is used to extract the multiscale information, and then the spectral-spatial features are formed with the strategy of hierarchical fusion. meanwhile, we employ three-layer convolution for extracting shallow features from lidar data, offering supplementary information. finally, we propose the adaptive feature fusion module (affm), an innovative and comprehensive mechanism designed for the fusion of features from diverse sources in multisource data fusion. these dynamically assigned weights guide the selection of the optimal model, which is determined by the joint loss across the three methods, ultimately leading to the generation of an accurate prediction map. this approach not only helps to deeply explore the spectral spatial information in the hyperspectral data, but also fuses the hyperspectral information effectively with the elevation information from the lidar data. the expression ability of model features is rapidly improved by adaptive weighting, which in turn enhances the performance and generalization ability of the model. compared with some existing methods, extensive experiments on three popular hsi and lidar datasets show that our proposed amsse-net can achieve better classification performance. the codes will be available at https://github.com/haofeng0003/amsse-net, contributing to the rs community.",AB_0268
"contrastive self-supervised learning (cssl) is a promising method for extracting effective features from unlabeled data. it performs well in image-level tasks, such as image classification and retrieval. however, the existing cssl methods are not suitable for pixel-level tasks, for example, change detection (cd), since they ignore the correlation between local patches or pixels. in this article, we first propose a multicue cssl (mc-cssl) method to derive dense features for cd. besides data augmentation, the mc-cssl takes advantage of more cues based on the semantic meaning and temporal correlation of local patches. specifically, the positive pair is built from local patches with similar semantic meanings or temporal ones with the same geographic location. the assumption is that local patches belonging to the same kind of land-covering tend to share similar features. second, the affinity matrix is truncated and introduced to extract change information between two temporal patches obtained from different types of sensors. as a result, some initial unchanged pixels are selected to serve as the supervision for mapping the dense features into a consistent space. based on the distance between all bitemporal pixels in the consistent space, a difference image (di) is generated and more unchanged pixels can be available. the dense feature mapping and unchanged pixel updating proceed alternately. the proposed cd method is evaluated in both homogeneous and heterogeneous cases, and the experimental results demonstrate its effectiveness and priority after comparison with some existing state-of-the-art methods. the source code will be available at https://github.com/yang202308/changedetection_cssl.",AB_0268
"pansharpening plays a crucial role in the domain of remote sensing image processing, as it allows for the generation of high-resolution multispectral (hrms) images. the main objective of pansharpening is to fuse low-resolution multispectral (lrms) and high-resolution panchromatic (hrpan) images, resulting in hrms images that exhibit uniform spectra and enhanced spatial details. consequently, the primary focus of related research is to preserve accurate features from both input images and achieve superior image reconstruction. in this article, we introduce a novel pansharpening framework called dual texture-edge maintaining transformer (dtempan). our framework achieves exceptional fusion results by leveraging a novel, more interpretable, and powerful architecture that considers pansharpening as dual, distinct deep sub-semantic branches. it independently reconstructs sub-semantic layer information, leading to improved performance. the dtempan architecture incorporates a dual transformer design comprising shared perception encoders and two parallel, effective semantic-level decoders. the hybrid multiscale texture maintaining (hmstm) decoder and the precise edge maintaining (pem) decoder are responsible for reconstructing the general low-frequency and rare high-frequency signals, respectively. through the integration of complementary information from both decoders, dtempan is capable of reconstructing accurate edges and high spatial information while preserving rich spectral details. extensive experimental evaluations have demonstrated it significantly outperforms the state-of-the-art methods on a number of benchmarks. our code is available at https://github.com/ d-walter/dtempan.",AB_0268
"remote-sensing image-text retrieval (rsitr) has attracted widespread attention due to its great potential for rapid information mining ability on remote-sensing images. although significant progress has been achieved, existing methods typically overlook the challenge posed by the extremely analogous descriptions, where the subtle differences remain largely unexploited or, in some cases, are entirely disregarded. to address the limitation, we propose a knowledge-aided momentum contrastive learning (kamcl) method for rsitr. specifically, we propose a novel knowledge-aided learning (kal) framework, including knowledge initialization, construction, filtration, and alignment operations, which aims at providing valuable concepts and learning discriminative representations. on this basis, we integrate momentum contrastive learning (mcl) to promote the capture of key concepts within the representation via expanding the scale of negative sample pairs. moreover, we designed a hierarchical aggregator (ha) module to better capture the multilevel information from remote-sensing images. finally, we introduce an innovative two-step training strategy designed to effectively harness the synergy among concepts and leverage their respective functionalities. extensive experiments conducted on the three public datasets showcase the remarkable performance of our approach in terms of retrieval accuracy and computational efficiency. for instance, compared with the existing state-of-the-art method, our method exhibits notable performance improvements of 2.65% on the rsicd dataset, simultaneously achieving improvements in inference efficiency by 48%. our source code will be released at https://github.com/mcx-mcx/kamcl.",AB_0268
"in recent years, optical remote sensing image (orsi) scene analysis has attracted increasing interest. however, existing networks show a trend of bifurcation. lightweight networks have very high inference speed but poor inference of contextual information in highly complex backgrounds. in contrast, networks with high-performance contextual information reasoning capability require many parameters and are computationally expensive. since the knowledge distillation (kd) method can greatly lighten the model, we propose a graph semantic guided network (gsgnet) that utilizes knowledge refinement for orsi scenario analysis, which has a high inference speed while maintaining practical contextual inference capability. rich semantic and detailed information facilitates the semantic segmentation of orsis. we design adjacent dynamic capture (adc) and local-global map inference modules that can effectively extract low-level spatial details and high-level contextual semantics. to improve the attention map relearning (ar) performance of the distillation method, we designed semantically guided fusion modules (sgfms) to locate spatial information and refine edge information. we also employed a structural relationship transfer (srt) distillation method in which the structural relationship knowledge of the teacher model (gsgnet-t) was used to guide the student model (gsgnet-s). we compared the performances of gsgnet-t and the gsgnet-s with kd (gsgnet-s*) with those of several state-of-the-art (soat) methods on the vaihingen and potsdam datasets. extensive experiments showed that gsgnet-s* outperformed most advanced methods with only 19.61 m parameters and a computation cost of 2.9 gflops. the experimental results and code of our network can be accessed at the following url: https://github.com/lyz00918/gsgnet-kd.",AB_0268
"multispectral and hyperspectral image fusion (mhif) involves the fusion of high-spatial-resolution multispectral images (hr-msis) and low-spatial-resolution hyperspectral images (lr-hsis) to generate high-spatial-resolution hyperspectral images (hr-hsis) and has gained significant attention in the field of remote-sensing imaging. while cnn and transformer models have shown effectiveness in mhif, existing cnn-or transformer-based algorithms are overburdened with model size, making it difficult to achieve an effective tradeoff between fusion accuracy and degree of lightweight. recently, implicit neural representation (inr) has been proven good interpretability and the ability to exploit coordinate information in 2-d tasks. nonetheless, inr-based fusion networks have certain limitations, such as the need for deeper super-resolution networks as shallow encoders, and insufficient representation capability on high upsampling ratios. to address these challenges, we present the quadtree implicit sampling (qis), which employs a hierarchical sampling from the perspective of the quadtree, to enhance the capacity of the overall network. furthermore, the remarkable design of qis allows us to adopt a lightweight structure as the shallow encoder, greatly alleviating the network burden and achieving lightweight. inspired by generative adversarial models, we incorporate qis as a lightweight generator into the generative adversarial network (gan) framework named qis-gan and leverage a discriminator to increase the fidelity of fused images. the results showcase the superior performance of qis-gan on the mhif tasks with upsampling ratios of x4, x8, and x16, surpassing the state-of-the-art (sota) in several datasets. the code for our approach will be available at https://github.com/chunyuzhu/qis-gan.",AB_0268
