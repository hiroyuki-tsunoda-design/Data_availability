AB,NO
"recent years have seen significant developments in the field of license plate recognition (lpr) through the integration of deep learning techniques and the increasing availability of training data. nevertheless, reconstructing license plates (lps) from low-resolution (lr) surveillance footage remains challenging. to address this issue, we introduce a single-image super-resolution (sisr) approach that integrates attention and transformer modules to enhance the detection of structural and textural features in lr images. our approach incorporates sub-pixel convolution layers (also known as pixelshuffle) and a loss function that uses an optical character recognition (ocr) model for feature extraction. we trained the proposed architecture on synthetic images created by applying heavy gaussian noise to high-resolution lp images from two public datasets, followed by bicubic downsampling. as a result, the generated images have a structural similarity index measure (ssim) of less than 0.10. our results show that our approach for reconstructing these low-resolution synthesized images outperforms existing ones in both quantitative and qualitative measures. our code is publicly available at https://github.com/valfride/lpr-rsr-ext/.",AB_0425
"polygenic risk scores (prss) are expected to play a critical role in precision medicine. currently, prs predictors are generally based on linear models using summary statistics, andmore recently individuallevel data. however, these predictors mainly capture additive relationships and are limited in data modalities they can use. we developed a deep learning framework (eir) for prs prediction which includes a model, genome-local-net (gln), specifically designed for large-scale genomics data. the framework supports multi-task learning, automatic integration of other clinical and biochemical data, and model explainability. when applied to individual-level data from the uk biobank, the gln model demonstrated a competitive performance compared to established neural network architectures, particularly for certain traits, showcasing its potential in modeling complex genetic relationships. furthermore, the gln model outperformed linear prs methods for type 1 diabetes, likely due to modeling non-additive genetic effects and epistasis. this was supported by our identification of widespread non-additive genetic effects and epistasis in the context of t1d. finally, we constructed prs models that integrated genotype, blood, urine, and anthropometric data and found that this improved performance for 93% of the 290 diseases and disorders considered. eir is available at https://github.com/arnor- sigurdsson/ eir.",AB_0425
"due to their complex history, plastids possess proteins encoded in the nuclear and plastid genome. moreover, these proteins localize to various subplastid compartments. since protein localization is associated with its function, prediction of subplastid localization is one of the most important steps in plastid protein annotation, providing insight into their potential function. therefore, we create a novel manually curated data set of plastid proteins and build an ensemble model for prediction of protein subplastid localization. moreover, we discuss problems associated with the task, e.g. data set sizes and homology reduction. plastogram classifies proteins as nuclear- or plastid-encoded and predicts their localization considering: envelope, stroma, thylakoid membrane or thylakoid lumen; for the latter, the import pathway is also predicted. we also provide an additional function to differentiate nuclear-encoded inner and outer membrane proteins. plastogram is available as a web server at https://biogenies.info/plastogram and as an r package at https://github.com/biogenies/plastogram. the code used for described analyses is available at https://github.com/biogenies/plastogram-analysis.",AB_0425
"carbohydrate-processing enzymes, cazymes, are classified into families based on sequence and three-dimensional fold. because many cazyme families contain members of diverse molecular function (different ec-numbers), sophisticated tools are required to further delineate these enzymes. such delineation is provided by the peptide-based clustering method cupp, conserved unique peptide patterns. cupp operates synergistically with the cazy family/subfamily categorizations to allow systematic exploration of cazymes by defining small protein groups with shared sequence motifs. the updated cupp library contains 21,930 of such motif groups including 3,842,628 proteins. the new implementation of the cupp-webserver, https://cupp.info/, now includes all published fungal and algal genomes from the joint genome institute (jgi), genome resources mycocosm and phycocosm, dynamically subdivided into motif groups of cazymes. this allows users to browse the jgi portals for specific predicted functions or specific protein families from genome sequences. thus, a genome can be searched for proteins having specific characteristics. all jgi proteins have a hyperlink to a summary page which links to the predicted gene splicing including which regions have rna support. the new cupp implementation also includes an update of the annotation algorithm that uses only a fourth of the ram while enabling multi-threading, providing an annotation speed below 1 ms/protein.",AB_0425
"introduction: patients with ms are mri scanned continuously throughout their disease course resulting in a large manual workload for radiologists which includes lesion detection and size estimation. though many models for automatic lesion segmentation have been published, few are used broadly in clinic today, as there is a lack of testing on clinical datasets. by collecting a large, heterogeneous training dataset directly from our ms clinic we aim to present a model which is robust to different scanner protocols and artefacts and which only uses mri modalities present in routine clinical examinations. methods: we retrospectively included 746 patients from routine examinations at our ms clinic. the inclusion criteria included acquisition at one of seven different scanners and an mri protocol including 2d or 3d t2-w flair, t2-w and t1-w images. reference lesion masks on the training (n = 571) and validation (n = 70) datasets were generated using a preliminary segmentation model and subsequent manual correction. the test dataset (n = 100) was manually delineated. our segmentation model https://github.com/caai/aims/ was based on the popular nnu-net, which has won several biomedical segmentation challenges. we tested our model against the published segmentation models hd-ms-lesions, which is also based on nnu-net, trained with a more homogenous patient cohort. we furthermore tested model robustness to data from unseen scanners by performing a leave-one-scanner-out experiment. results: we found that our model was able to segment ms white matter lesions with a performance comparable to literature: dsc = 0.68, precision = 0.90, recall = 0.70, f1 = 0.78. furthermore, the model outperformed hd-ms-lesions in all metrics except precision = 0.96. in the leave-one-scanner-out experiment there was no significant change in performance (p < 0.05) between any of the models which were only trained on part of the dataset and the full segmentation model. conclusion: in conclusion we have seen, that by including a large, heterogeneous dataset emulating clinical reality, we have trained a segmentation model which maintains a high segmentation performance while being robust to data from unseen scanners. this broadens the applicability of the model in clinic and paves the way for clinical implementation.",AB_0425
"sh2 domains are key mediators of phosphotyrosinebased signalling, and therapeutic targets for diverse, mostly oncological, disease indications. they have a highly conserved structure with a central beta sheet that divides the binding surface of the protein into two main pockets, responsible for phosphotyrosine binding (py pocket) and substrate specificity (py + 3 pocket). in recent years, structural databases have proven to be invaluable resources for the drug discovery community, as they contain highly relevant and up-to-date information on important protein classes. here, we present sh2db, a comprehensive structural database and webserver for sh2 domain structures. to organize these protein structures efficiently, we introduce (i) a generic residue numbering scheme to enhance the comparability of different sh2 domains, (ii) a structure-based multiple sequence alignment of all 120 human wild-type sh2 domain sequences and their pdb and alphafold structures. the aligned sequences and structures can be searched, browsed and downloaded from the online interface of sh2db (http://sh2db.ttk.hu), with functions to conveniently prepare multiple structures into a pymol session, and to export simple charts on the contents of the database. our hope is that sh2db can assist researchers in their day-to-day work by becoming a one-stop shop for sh2 domain related research.",AB_0425
"motivation: iso-seq rna long-read sequencing enables the identification of full-length transcripts and isoforms, removing the need for complex analysis such as transcriptome assembly. however, the raw sequencing data need to be processed in a series of steps before annotation is complete. here, we present nf-core/isoseq, a pipeline for automatic read processing and genome annotation. following nf-core guidelines, the pipeline has few dependencies and can be run on any of platforms. availability and implementation: the pipeline is freely available online on the nf-core website (https://nf-co.re/isoseq) and on github (https://github.com/nf-core/isoseq) under mit license",AB_0425
"in some contexts, achieving high predictive capability may be sufficient for a machine learning model. however, in many scenarios, it is necessary to understand the model's decisions to increase confidence in the predictions and direct the actions to be taken based on them. therefore, it is essential to provide interpretable models. however, some authors have pointed out the need to improve current interpretability methods to provide adequate explanations, especially for non-specialists in machine learning. the solution is to expand studies beyond computational issues to understand better how people receive explanations. based on the literature, we identified three aspects to be considered in the explanations: contrastive, selected, and social. the counterfactual approach, contrastive in nature, inform the user of how the decision by the model can be altered through minimal changes to the input features. given this, we introduce the agnostic method of counterfactual, selected, and social explanations (csse), capable of generating local explanations for classification models using a genetic algorithm. thus, as contributions, we highlight that the csse offers counterfactual explanations from learning models, presents explanations with diversity, without prolixity, and allows the user to restrict the features that appear in the explanation (actionability), besides other parameterization options for the user to communicate their preferences. a particular novelty of our work is the possibility for the user to adjust the importance he will give to sparsity (minimum number of changes) or similarity (minimizing the distance). furthermore, we indicate other possibilities for the actionability functionality, inherently used to lock immutable features, allowing users to block features according to their interests or expertise. these resources can help the user obtain explanations more targeted to their objective and advance further in interpretability, considering computational and social aspects in generating explanations. the experiments showed that csse presents relevant results compared to some existing approaches. the work also includes a case study of predicting the academic performance of children and adolescents with adhd, in which we applied the csse. thus, the proposed method advances interpretability by offering explanations aimed at the end user, which can generate greater acceptance, confidence, and understanding regarding the models' decisions. the method implementation is available at https://codeocean.com/capsule/7060371/tree.",AB_0425
"ion mobility massspectrometry (im-ms) techniques have become highlyvalued as a tool for structural characterization of biomolecular systemssince they yield accurate measurements of the rotationally averagedcollision cross-section (ccs) against a buffer gas. despite its enormouspotential, im-ms data interpretation is often challenging due to theconformational isomerism of metabolites, lipids, proteins, and otherbiomolecules in the gas phase. therefore, reliable and fast ccs calculationsare needed to help interpret im-ms data. in this work, we presentmassccs, a parallelized open-source code for computing ccs of moleculesranging from small organic compounds to massive protein assembliesat the trajectory method level of description using atomic and molecularbuffer gas particles. the performance of the code is comparable toother available software for small molecules and proteins but is significantlyfaster for larger macromolecular assemblies. we performed extensivetests regarding accuracy, performance, and scalability with systemsize and number of cpu cores. massccs has proven highly accurate andefficient, with execution times under a few minutes, even for large(84.87 mda) virus capsid assemblies with very modest computationalresources. massccs is freely available at https://github.com/cces-cepid/massccs.",AB_0425
"deforestation has become a major cause of climate change, and as a result, both characterizing the drivers and estimating segmentation maps of deforestation have piqued the interest of researchers. in the computer vision domain, vision transformers (vits) have shown their superiority compared to extensively utilized convolutional neural networks (cnns) over the last couple of years. although, vits has several challenges, specifically in remote sensing image processing, including their significant complexity that increases the computation costs and their need for much higher reference data than that of cnns. as such, in this paper, we introduce an attention gates aided transu-net, called transu-net++ for semantic segmentation with an application of deforestation mapping in two south american forest biomes, i.e., the atlantic forest and the amazon rainforest. the heterogeneous kernel convolution (hetconv), u-net, attention gates, and vits are all utilized in the proposed transu-net++ to their advantage. the transu-net++ significantly increased the performance of transu-net's over the atlantic forest dataset by about 4%, 6%, and 16%, respectively, in terms of overall accuracy, f1-score, and recall, respectively. moreover, the results show that the developed trasnu-net++ model (0.921) achieves the highest area under the roc curve value in the 3-band amazon forest dataset as compared to other segmentation models, including icnet (0.667), enet (0.69), segnet (0.788), u-net (0.871), attention u-net-2 (0.886), r2u-net (0.888), transu-net (0.889), swin u-net (0.893), resu-net (0.896), u-net+++ (0.9), and attention u-net (0.908), respectively. the code will be made publicly available at https://github.com/aj1365/transunetplus2.",AB_0425
