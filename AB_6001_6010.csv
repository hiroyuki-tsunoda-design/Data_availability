AB,NO
"backgroundfusion of rna-binding proteins (rbps) to rna base-editing enzymes (such as apobec1 or adar) has emerged as a powerful tool for the discovery of rbp binding sites. however, current methods that analyze sequencing data from rna-base editing experiments are vulnerable to false positives due to off-target editing, genetic variation and sequencing errors.resultswe present flagging areas of rna-editing enrichment (flare), a snakemake-based pipeline that builds on the outputs of the sailor edit site discovery tool to identify regions statistically enriched for rna editing. flare can be configured to analyze any type of rna editing, including c to u and a to i. we applied flare to c-to-u editing data from a rbfox2-apobec1 stamp experiment, to show that our approach attains high specificity for detecting rbfox2 binding sites. we also applied flare to detect regions of exogenously introduced as well as endogenous a-to-i editing.conclusionsflare is a fast and flexible workflow that identifies significantly edited regions from rna-seq data. the flare codebase is available at https://github.com/yeolab/flare.",AB_0601
"the authors present a historical analysis of the first neurosurgical service in texas. initially established as a subdivision within the department of surgery in the early 1900s, this service eventually evolved into the department of neurosurgery at the university of texas medical branch (utmb). the pivotal contributions of individual chiefs of neurosurgery throughout the years are highlighted, emphasizing their roles in shaping the growth of the neurosurgery division. the challenges faced by the neurosurgical division are documented, with particular attention given to the impact of hurricanes on galveston island, texas, which significantly disrupted hospital operations. additionally, a detailed account of recent clinical and research expansions is presented, along with the future directions envisioned for the department of neurosurgery. this work offers a comprehensive historical narrative of the neurosurgical service at utmb, chronicling its journey of growth and innovation, and underscoring its profound contributions to galveston's healthcare services, extending its impact beyond the local community. https://thejns.org/doi/abs/10.3171/2024.1.jns232418",AB_0601
"background: hemodynamically unstable high-risk, or massive, pulmonary embolism (pe) has a reported in-hospital mortality of over 25%. systemic thrombolysis is the guideline-recommended treatment despite limited evidence. the flame study (flowtriever for acute massive pe) was designed to generate evidence for interventional treatments in high-risk pe.methods: the flame study was a prospective, multicenter, nonrandomized, parallel group, observational study of high-risk pe. eligible patients were treated with flowtriever mechanical thrombectomy (flowtriever arm) or with other contemporary therapies (context arm). the primary end point was an in-hospital composite of all-cause mortality, bailout to an alternate thrombus removal strategy, clinical deterioration, and major bleeding. this was compared in the flowtriever arm to a prespecified performance goal derived from a contemporary systematic review and meta-analysis.results: a total of 53 patients were enrolled in the flowtriever arm and 61 in the context arm. context arm patients were primarily treated with systemic thrombolysis (68.9%) or anticoagulation alone (23.0%). the primary end point was reached in 9/53 (17.0%) flowtriever arm patients, significantly lower than the 32.0% performance goal (p<0.01). the primary end point was reached in 39/61 (63.9%) context arm patients. in-hospital mortality occurred in 1/53 (1.9%) patients in the flowtriever arm and in 18/61 (29.5%) patients in the context arm.conclusions: among patients selected for mechanical thrombectomy with the flowtriever system, a significantly lower associated rate of in-hospital adverse clinical outcomes was observed compared with a prespecified performance goal, primarily driven by low all-cause mortality of 1.9%.registration: url: https://www.clinicaltrials.gov; unique identifier: nct04795167.",AB_0601
"objective the all patients refined diagnosis related group (apr-drg) modifiers-severity of illness (soi) and risk of mortality (rom)-inform hospital reimbursement nationally. the ubiquitous apr-drg data bear the potential to inform public health research; however, the algorithms that generate these modifiers are proprietary and therefore should be independently verified. this study evaluated the predictive value of apr-drg modifiers for the outcomes and costs of intracranial hemorrhage.methods the new york statewide planning and research cooperative system databases were accessed and searched for the intracranial hemorrhage diagnosis related group in records from 2012 to 2020. receiver operating characteristic and multiple logistic regressions characterized the predictive validity of the apr-drg modifiers for patient outcomes. one-way anova compared costs and charges between soi and rom designations.results among 46,019 patients, 12,627 (27.4%) died. the mean +/- sem costs per patient were $21,342 +/- $145 and the mean +/- sem charges per patient were $68,117 +/- $408. for prediction of mortality, the area under the curve (auc) was 0.74 for soi and 0.83 for rom. for prediction of discharge to a facility, auc was 0.62 for soi and 0.64 for rom. regression analysis showed that rom was a strong predictor of mortality, while soi was a weak predictor; both were modest predictors of discharge to a facility. soi and rom were significant predictors of costs and charges.conclusions compared with the prior studies, the authors identified several limitations of apr-drg modifiers, including low specificity, modest auc, and limited outcomes prediction. this report supports the limited use of apr-drg modifiers in independent research on intracranial hemorrhage epidemiology and reimbursement and advocates for general caution in their use for evaluation of neurosurgical disease. https://thejns.org/doi/abs/10.3171/2023.2.jns222255",AB_0601
"machine learning algorithms have been successfully applied in proteomics, genomics and transcriptomics. and have helped the biological community to answer complex questions. however, most machine learning methods require lots of data, with every data point having the same vector size. the biological sequence data, such as proteins, are amino acid sequences of variable length, which makes it essential to extract a definite number of features from all the proteins for them to be used as input into machine learning models. there are numerous methods to achieve this, but only several tools let re-searchers encode their proteins using multiple schemes without having to use different programs or, in many cases, code these algorithms themselves, or even come up with new algorithms. in this work, we created profeatx, a tool that contains 50 encodings to extract protein features in an efficient and fast way supporting desktop as well as high-performance computing environment. it can also encode concatenated features for protein-protein interactions. the tool has an easy-to-use web interface, allowing non-experts to use feature extraction techniques, as well as a stand-alone version for advanced users. profeatx is im-plemented in c++ and available on github at https://github.com/usubioinfo/profeatx. the web server is available at http://bioinfo.usu.edu/profeatx/.(c) 2023 the authors. published by elsevier b.v. on behalf of research network of computational and structural biotechnology. this is an open access article under the cc by-nc-nd license ().",AB_0601
"motivation genomic data are subject to various sources of confounding, such as demographic variables, biological heterogeneity, and batch effects. to identify genomic features associated with a variable of interest in the presence of confounders, the traditional approach involves fitting a confounder-adjusted regression model to each genomic feature, followed by multiplicity correction.results this study shows that the traditional approach is suboptimal and proposes a new two-dimensional false discovery rate control framework (2dfdr+) that provides significant power improvement over the conventional method and applies to a wide range of settings. 2dfdr+ uses marginal independence test statistics as auxiliary information to filter out less promising features, and fdr control is performed based on conditional independence test statistics in the remaining features. 2dfdr+ provides (asymptotically) valid inference from samples in settings where the conditional distribution of the genomic variables given the covariate of interest and the confounders is arbitrary and completely unknown. promising finite sample performance is demonstrated via extensive simulations and real data applications.availability and implementation r codes and vignettes are available at https://github.com/asmita112358/tdfdr.np.",AB_0601
"motivation while evolutionary approaches to medicine show promise, measuring evolution itself is difficult due to experimental constraints and the dynamic nature of body systems. in cancer evolution, continuous observation of clonal architecture is impossible, and longitudinal samples from multiple timepoints are rare. increasingly available dna sequencing datasets at single-cell resolution enable the reconstruction of past evolution using mutational history, allowing for a better understanding of dynamics prior to detectable disease. there is an unmet need for an accurate, fast, and easy-to-use method to quantify clone growth dynamics from these datasets.results we derived methods based on coalescent theory for estimating the net growth rate of clones using either reconstructed phylogenies or the number of shared mutations. we applied and validated our analytical methods for estimating the net growth rate of clones, eliminating the need for complex simulations used in previous methods. when applied to hematopoietic data, we show that our estimates may have broad applications to improve mechanistic understanding and prognostic ability. compared to clones with a single or unknown driver mutation, clones with multiple drivers have significantly increased growth rates (median 0.94 versus 0.25 per year; p = 1.6x10-6). further, stratifying patients with a myeloproliferative neoplasm (mpn) by the growth rate of their fittest clone shows that higher growth rates are associated with shorter time to mpn diagnosis (median 13.9 versus 26.4 months; p = 0.0026).availability and implementation we developed a publicly available r package, clonerate, to implement our methods (package website: https://bdj34.github.io/clonerate/). source code: https://github.com/bdj34/clonerate/.",AB_0601
"charting microrna (mirna) regulation across pathways is key to characterizing their function. yet, no method currently exists that can quantify how mirnas regulate multiple interconnected pathways or prioritize them for their ability to regulate coordinate transcriptional programs. existing methods primarily infer one-to-one relationships between mirnas and pathways using differentially expressed genes. we introduce panomir, an in silico framework for studying the interplay of mirnas and disease functions. panomir integrates gene expression, mrna-mirna interactions and known biological pathways to reveal coordinated multi-pathway targeting by mirnas. panomir utilizes pathway-activity profiling approaches, a pathway co-expression network and network clustering algorithms to prioritize mirnas that target broad-scale transcriptional disease phenotypes. it directly resolves differential regulation of pathways, irrespective of their differential gene expression, and captures co-activity to establish functional pathway groupings and the mirnas that may regulate them. panomir uses a systems biology approach to provide broad but precise insights into mirna-regulated functional programs. it is available at https://bioconductor.org/packages/panomir.",AB_0601
"identifying potential drug targets using metabolic modeling requires integrating multiple modeling methods and heterogeneous biological datasets, which can be challenging without efficient tools. we developed constraint-based optimization of metabolic objectives (como), a user-friendly pipeline that integrates multi-omics data processing, context-specific metabolic model development, simulations, drug databases and disease data to aid drug discovery. como can be installed as a docker image or with conda and includes intuitive instructions within a jupyter lab environment. it provides a comprehensive solution for the integration of bulk and single-cell rna-seq, microarrays and proteomics outputs to develop context-specific metabolic models. using public databases, open-source solutions for model construction and a streamlined approach for predicting repurposable drugs, como enables researchers to investigate low-cost alternatives and novel disease treatments. as a case study, we used the pipeline to construct metabolic models of b cells, which simulate and analyze them to predict metabolic drug targets for rheumatoid arthritis and systemic lupus erythematosus, respectively. como can be used to construct models for any cell or tissue type and identify drugs for any human disease where metabolic inhibition is relevant. the pipeline has the potential to improve the health of the global community cost-effectively by providing high-confidence targets to pursue in preclinical and clinical studies. the source code of the como pipeline is available at https://github.com/helikarlab/como. the docker image can be pulled at https://github.com/helikarlab/como/pkgs/container/como.",AB_0601
"metagenome binning is a key step, downstream of metagenome assembly, to group scaffolds by their genome of origin. although accurate binning has been achieved on datasets containing multiple samples from the same community, the completeness of binning is often low in datasets with a small number of samples due to a lack of robust species co-abundance information. in this study, we exploited the chromatin conformation information obtained from hi-c sequencing and developed a new reference-independent algorithm, metagenome binning with abundance and tetra-nucleotide frequencies-long range (metabat-lr), to improve the binning completeness of these datasets. this self-supervised algorithm builds a model from a set of high-quality genome bins to predict scaffold pairs that are likely to be derived from the same genome. then, it applies these predictions to merge incomplete genome bins, as well as recruit unbinned scaffolds. we validated metabat-lr's ability to bin-merge and recruit scaffolds on both synthetic and real-world metagenome datasets of varying complexity. benchmarking against similar software tools suggests that metabat-lr uncovers unique bins that were missed by all other methods. metabat-lr is open-source and is available at https://bitbucket.org/ project-metabat/metabat-lr.",AB_0601
