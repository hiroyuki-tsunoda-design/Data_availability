AB,NO
"coronavirus disease 2019 (covid-19) has become a severe global pandemic. accurate pneumonia infection segmentation is important for assisting doctors in diagnosing covid-19. deep learning-based methods can be developed for automatic segmentation, but the lack of large-scale well-annotated covid-19 training datasets may hinder their performance. semi-supervised segmentation is a promising solution which explores large amounts of unlabelled data, while most existing methods focus on pseudo-label refinement. in this paper, we propose a new perspective on semi-supervised learning for covid-19 pneumonia infection segmentation, namely pseudo-label guided image synthesis. the main idea is to keep the pseudo-labels and synthesize new images to match them. the synthetic image has the same covid-19 infected regions as indicated in the pseudo-label, and the reference style extracted from the style code pool is added to make it more realistic. we introduce two representative methods by incorporating the synthetic images into model training, including single-stage synthesis-assisted cross pseudo supervision (sa-cps) and multi-stage synthesis-assisted self-training (sa-st), which can work individually as well as cooperatively. synthesis-assisted methods expand the training data with high-quality synthetic data, thus improving the segmentation performance. extensive experiments on two covid-19 ct datasets for segmenting the infections demonstrate our method is superior to existing schemes for semi-supervised segmentation, and achieves the state-of-the-art performance on both datasets. code is available at: https://github.com/feilyu/sassl.",AB_0258
"image registration is a fundamental medical image analysis task, and a wide variety of approaches have been proposed. however, only a few studies have comprehensively compared medical image registration approaches on a wide range of clinically relevant tasks. this limits the development of registration methods, the adoption of research advances into practice, and a fair benchmark across competing approaches. the learn2reg challenge addresses these limitations by providing a multi-task medical image registration data set for comprehensive characterisation of deformable registration algorithms. a continuous evaluation will be possible at https://learn2reg.grand-challenge.org. learn2reg covers a wide range of anatomies (brain, abdomen, and thorax), modalities (ultrasound, ct, mr), availability of annotations, as well as intra- and inter-patient registration evaluation. we established an easily accessible framework for training and validation of 3d registration methods, which enabled the compilation of results of over 65 individual method submissions from more than 20 unique teams. we used a complementary set of metrics, including robustness, accuracy, plausibility, and runtime, enabling unique insight into the current state-of-the-art of medical image registration. this paper describes datasets, tasks, evaluation methods and results of the challenge, as well as results of further analysis of transferability to new datasets, the importance of label supervision, and resulting bias. while no single approach worked best across all tasks, many methodological aspects could be identified that push the performance of medical image registration to new state-of-the-art performance. furthermore, we demystified the common belief that conventional registration methods have to be much slower than deep-learning-based methods",AB_0258
"deep learning-based semi-supervised learning (ssl) algorithms are promising in reducing the cost of manual annotation of clinicians by using unlabelled data, when developing medical image segmentation tools. however, to date, most existing semi-supervised learning (ssl) algorithms treat the labelled images and unlabelled images separately and ignore the explicit connection between them; this disregards essential shared information and thus hinders further performance improvements. to mine the shared information between the labelled and unlabelled images, we introduce a class-specific representation extraction approach, in which a task-affinity module is specifically designed for representation extraction. we further cast the representation into two different views of feature maps; one is focusing on low-level context, while the other concentrates on structural information. the two views of feature maps are incorporated into the task-affinity module, which then extracts the class-specific representations to aid the knowledge transfer from the labelled images to the unlabelled images. in particular, a task-affinity consistency loss between the labelled images and unlabelled images based on the multi-scale class-specific representations is formulated, leading to a significant performance improvement. experimental results on three datasets show that our method consistently outperforms existing state-of-the-art methods. our findings highlight the potential of consistency between class-specific knowledge for semi-supervised medical image segmentation. the code and models are to be made publicly available at https://github.com/jingkunchen/tac.",AB_0258
"semantic segmentation aims to map each pixel of an image into its corresponding semantic label. most existing methods either mainly concentrate on high-level features or simple combination of low-level and high-level features from backbone convolutional networks, which may weaken or even ignore the compensation between different levels. to effectively take advantages from both shallow (textural) and deep (semantic) features, this paper proposes a novel plug-and-play module, namely feature enhancement module (fem). the proposed fem first uses an information extractor to extract the desired details or semantics from different stages, and then enhances target features by taking in the extracted message. two types of fem, i.e., detail fem and semantic fem, can be customized. concretely, the former type strengthens textural information to protect key but tiny/low-contrast details from suppression/removal, while the other one highlights structural information to boost segmentation performance. by equipping a given backbone network with fems, there might contain two information flows, i.e., detail flow and semantic flow. extensive experiments on the cityscapes, ade20k and pascal context datasets are conducted to validate the effectiveness of our design. the code has been released at https://github.com/superz-liu/fenet.",AB_0258
"plant trichomes are epidermal structures with a wide variety of functions in plant development and stress responses. although the functional importance of trichomes has been realized, the tedious and time-consuming manual phenotyping process greatly limits the research progress of trichome gene cloning. currently, there are no fully automated methods for identifying maize trichomes. we introduce trichomeyolo, an automated trichome counting and measuring method that uses a deep convolutional neural network, to identify the density and length of maize trichomes from scanning electron microscopy images. our network achieved 92.1% identification accuracy on scanning electron microscopy micrographs of maize leaves, which is much better performed than the other 5 currently mainstream object detection models, faster r-cnn, yolov3, yolov5, detr, and cascade r-cnn. we applied trichomeyolo to investigate trichome variations in a natural population of maize and achieved robust trichome identification. our method and the pretrained model are open access in github (https://github. com/yaober/trichomecounter). we believe trichomeyolo will help make efficient trichome identification and help facilitate researches on maize trichomes.",AB_0258
"background: many ultraprocessed food (upf)-derived by-products may play a role in the development of chronic kidney disease (ckd). although several studies have assessed the association of upfs with kidney function decline or ckd in various countries, no evidence has been shown in china and the united kingdom.objectives: this study aims to evaluate the association between upf consumption and risk of ckd in 2 large cohort studies from china and the united kingdom.methods: in total, 23,775 and 102,332 participants without baseline ckd were enrolled in the tianjin chronic low-grade systemic inflammation and health (tclsih) and uk biobank cohort studies, respectively. information on upf consumption was obtained from a validated food frequency questionnaire in the tclsih and 24-h dietary recalls in the uk biobank cohort. ckd was defined as an estimated glomerular filtration rate of <60 ml/ min/1.73 m2, albumin-to-creatinine ratio >= 30 mg/g, or as having a clinical diagnosis of ckd in both cohorts. multivariable cox proportional hazard models were used to examine the association between upf consumption and the risk of ckd.results: after a median follow-up of 4.0 and 10.1 y, the incidence rates of ckd were around 1.1% and 1.7% in the tclsih and uk biobank cohorts, respectively. the multivariable hazard ratio [95% confidence interval] of ckd across increasing quartiles (quartiles 1-4) of upf consumption were 1 (reference), 1.24 (0.89, 1.72), 1.30 (0.91, 1.87), and 1.58 (1.07, 2.34) (p for trend = 0.02) in the tclsih cohort and 1 (reference), 1.14 (1.00, 1.31), 1.16 (1.01, 1.33), and 1.25 (1.09, 1.43) (p for trend < 0.01) in the uk biobank cohort, respectively.conclusions: our finding indicated that higher upf consumption is associated with a higher risk of ckd. moreover, restricting upf consumption may potentially benefit the prevention of ckd. further clinical trials are required to clarify the causality. this trial was registered at umin clinical trials registry as umin000027174 (https://upload.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=r000031137).",AB_0258
"multi-exposure image fusion (mef) targets to integrate multiple shots with different exposures and generates a single higher dynamic image than each. existing deep learning-based mef approaches only adopt reference high dynamic images (hdr) as positive samples to guide the training of fusion networks. however, simply relying on these positive samples are difficult to find the optimal parameters for the network as a whole. thus, the structure or texture information is blurred or missed in the generated hdr results. moreover, few approaches attempted to prevent illumination degeneration during the fusion process, resulting in poor color saturation on their fused results. to address such limitations, in this paper, we introduce a novel holistic and local constraint built upon contrastive learning, namely holoco, to discover the intrinsic information of both source ldr images and the reference hdr one. in this manner, the generated fused images are pulled to the hdr image and pushed away from the ldr source images in both image-based and path-based latent feature spaces. besides, inspired by retinex theory, we propose a color correction module (ccm) to refine illumination features. ccm involves dual streams that can collaborate to ensure the nature of color information and details consistency. extensive experiments on two datasets show that our holoco can continuously generate visual-appealing hdr results with precise detail and vivid color rendition, performing favorably against the state-of-the-art mef approaches. source code is available in github https://github.com/jinyuanliu-cv/holoco.",AB_0258
"pancreatic cancer is a highly malignant cancer type with a high mortality rate. as no obvious symptoms are associated with this cancer type, most of the diagnoses are made when the patients are already in a late stage. in this work, we propose an automated method for effective early diagnosis of pancreatic cancer based on multiple instance learning with contrast-enhanced ct images. in this method, diagnosis stability and generalizability were improved through shape normalization based on anatomical structures as well as instance-level contrastive learning. specifically, anatomically-guided shape normalization were developed to reconstruct the pancreatic regions of interest by spatial transformations, account for larger tumor parts in these regions, and hence enhance the extraction of pancreatic features. moreover, instance-level contrastive learning was employed to aggregate different types of tumor features within the multiple instance learning framework. this learning approach can maintain the tumor feature integrity and enhance the diagnosis stability. finally, a balance-adjustment strategy was designed to alleviate the class imbalance problem caused by the scarcity of tumor samples. extensive experimental results demonstrated remarkable performance of our method when conducted cross-validation on an in-house dataset with 310 patients and independent test on two unseen datasets (a private test set with 316 and a publicly-available test set with 281). the proposed strategies also led to significant improvements in generalizability. besides, the clinical significance of the proposed method was further verified through two independent test results in which tumors smaller than 2 cm in diameter were identified at accuracies of 80.9% and 90.1%, respectively. overall, our method provides a potentially successful tool for early diagnosis of pancreatic cancer. our source codes will be released at https://github.com/sjtubme-qianlab/mil_padiagnosis.",AB_0258
"non-invasive prediction for kit/pdgfra status in gist is a challenging problem. this study aims to evaluate whether ct based sarcopenia could differentiate kit/pdgfra wild-type gastrointestinal stromal tumor (wt-gist) from the mutant-type gist (mu-gist), and to evaluate genetic features of gist. a total of 174 patients with gist (wt-gist = 52) were retrospectively identified between january 2011 to october 2019. a sarcopenia nomogram was constructed by multivariate logistic regression. the performance of the nomogram was evaluated by discrimination, calibration curve, and decision curve. genomic data was obtained from our own specimens and also from the open databases cbioportal. data was analyzed by r version 3.6.1 and clusterprofiler (http:// cbiop ortal. org/ mskimpact). there were significantly higher incidence (75.0% vs. 48.4%) and more severe sarcopenia in patients with wt-gist than in patients with mu-gist. multivariate logistic regression analysis showed that sarcopenia score (fitted based on age, gender and skeletal muscle index), and muscle fat index were independent predictors for higher risk of wt-gist (p < 0.05 for both the training and validation cohorts). our sarcopenia nomogram achieved a promising efficiency with an auc of 0.879 for the training cohort, and 0.9099 for the validation cohort with a satisfying consistency in the calibration curve. favorable clinical usefulness was observed using decision curve analysis. the additional gene sequencing analysis based on both our data and the external data demonstrated aberrant signal pathways being closely associated with sarcopenia in the wt-gist. our study supported the use of ct-based assessment of sarcopenia in differentiating the wt-gist from the mu-gist preoperatively.",AB_0258
"multimodal imaging with visible and thermal sensors attracts much attention due to its robustness under challenging illumination conditions. due to spectral differences, the visible and thermal images are typically misaligned, where image registration is necessary before high-level vision tasks such as information fusion, multimodal object detection, etc. contemporary registration methods measure either an angular distance or a linear distance as an objective function to solve affine parameters between visible and thermal images. it is noticeable that both distances have unique advantages in image alignment, which suggests that integrating these measurements may enhance the registration quality. thus, this paper proposes a new algorithm, namely multi-objective optimization-based image registration (moir), to align visible and thermal images. the moir comprises a normalized gradient measurement (ngm) that quantifies and normalizes differences of image gradient in both angular and linear aspects, and a regularized stochastic gradient descent (rsgd) that smoothly solves affine parameters. both quantitative and qualitative results validate the effectiveness and robustness of moir for the registration of visible and thermal images in both controlled and real-world scenarios. some animated examples can be found in https://github.com/jb2849/moir.",AB_0258
