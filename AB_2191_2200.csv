AB,NO
"motivation recent rapid developments in spatial transcriptomic techniques at cellular resolution have gained increasing attention. however, the unique characteristics of large-scale cellular resolution spatial transcriptomic datasets, such as the limited number of transcripts captured per spot and the vast number of spots, pose significant challenges to current cell-type deconvolution methods.results in this study, we introduce stvae, a method based on the variational autoencoder framework to deconvolve the cell-type composition of cellular resolution spatial transcriptomic datasets. to assess the performance of stvae, we apply it to five datasets across three different biological tissues. in the stereo-seq and slide-seqv2 datasets of the mouse brain, stvae accurately reconstructs the laminar structure of the pyramidal cell layers in the cortex, which are mainly organized by the subtypes of telencephalon projecting excitatory neurons. in the stereo-seq dataset of the e12.5 mouse embryo, stvae resolves the complex spatial patterns of osteoblast subtypes, which are supported by their marker genes. in stereo-seq and pixel-seq datasets of the mouse olfactory bulb, stvae accurately delineates the spatial distributions of known cell types. in summary, stvae can accurately identify spatial patterns of cell types and their relative proportions across spots for cellular resolution spatial transcriptomic data. it is instrumental in understanding the heterogeneity of cell populations and their interactions within tissues.availability and implementation stvae is available in github (https://github.com/lichen2018/stvae) and figshare (https://figshare.com/articles/software/stvae/23254538).",AB_0220
"motivation in recent years, there has been a breakthrough in protein structure prediction, and the alphafold2 model of the deepmind team has improved the accuracy of protein structure prediction to the atomic level. currently, deep learning-based protein function prediction models usually extract features from protein sequences and combine them with protein-protein interaction networks to achieve good results. however, for newly sequenced proteins that are not in the protein-protein interaction network, such models cannot make effective predictions. to address this, this article proposes the struct2go model, which combines protein structure and sequence data to enhance the precision of protein function prediction and the generality of the model.results we obtain amino acid residue embeddings in protein structure through graph representation learning, utilize the graph pooling algorithm based on a self-attention mechanism to obtain the whole graph structure features, and fuse them with sequence features obtained from the protein language model. the results demonstrate that compared with the traditional protein sequence-based function prediction model, the struct2go model achieves better results.availability and implementation the data underlying this article are available at https://github.com/lyjps/struct2go.",AB_0220
"disentanglement research is a critical and important issue in the field of image editing. in order to perform disentangled editing on images generated by generative models, this paper presents an unsupervised, model-agnostic, two-stage trained editing framework. this work addresses the problem of discovering interpretable, disentangled directions of edited image attributes in the latent space of generative models. this effort's primary objective was to address the limitations discovered in previous research, mainly (a) the discovered editing directions are interpretable but significantly entangled, i.e., changes to one attribute affect the others and (b) prior research has utilized direction discovery and direction disentanglement separately, and they can't work synergistically. more specifically, this paper proposes a two-stage training method that discovers the editing direction with semantics, perturbs the dimension of the direction vector, adjusts it with a penalty mechanism, and makes the editing direction more disentangled. this allows easy distinguishable image editing, such as age and facial expressions in facial images. experimentally compared to other methods, the proposed method outperforms them both qualitatively and quantitatively in terms of interpretability, disentanglement, and distinguishability of the generated images. the implementation of our method is available at https://github.com/ydniuyongjie/twostageforfaceedit.",AB_0220
"saliency-driven mesh simplification methods have shown promising results in main-taining visual detail, but effective simplification requires accurate 3d saliency maps. the conventional mesh saliency detection method may not capture salient regions in 3d models with texture. to address this issue, we propose a novel saliency detection method that fuses saliency maps from multi-view projections of textured models. specifically, we introduce a texel descriptor that combines local convexity and chromatic aberration to capture texel saliency at multiple scales. furthermore, we created a novel dataset that reflects human eye fixation patterns on textured models, which serves as an objective evaluation metric. our experimental results demonstrate that our saliency-driven method outperforms existing approaches on several evaluation metrics. our method source code can be accessed at https://github.com/bkballoon/mvsm-fusion and the dataset can be accessed at 10.5281/zenodo.8131602.",AB_0220
"as one of the most important post-translational modifications (ptm), lysine acetylation (kace) plays an important role in various biological activities. traditional experimental methods for identifying kace sites are inefficient and expensive. instead, several machine learning methods have been developed for kace site prediction, and hand-crafted features have been used to encode the protein sequences. however, there are still two challenges: the complex biological information may be under-represented by these manmade features and the small sample issue of some species needs to be addressed. we propose a novel model, mstl-kace, which was developed based on transfer learning strategy with pretrained bidirectional encoder representations from transformers (bert) model. in this model, the high-level embeddings were extracted from species-specific bert models, and a two-stage fine-tuning strategy was used to deal with small sample issue. specifically, a domain-specific bert model was pretrained using all of the sequences in our data sets, which was then fine-tuned, or two-stage fine-tuned based on the training data set of each species to obtain the species-specific bert models. afterward, the embeddings of residues were extracted from the fine-tuned model and fed to the different downstream learning algorithms. after comparison, the best model for the six prokaryotic species was built by using a random forest. the results for the independent test sets show that our model outperforms the state-of-the-art methods on all six species. the source codes and data for mstl-kace are available at https://github.com/leo97king/mstl-kace.",AB_0220
"background: n6, 2'-o-dimethyladenosine (m(6)am) is an abundant rna methylation modification on vertebrate mrnas and is present in the transcription initiation region of mrnas. it has recently been experimentally shown to be associated with several human disorders, including obesity genes, and stomach cancer, among others. as a result, n6,2 '-o-dimethyladenosine (m(6)am) site will play a crucial part in the regulation of rna if it can be correctly identified.results: this study proposes a novel deep learning-based m(6)am prediction model, emdl_m6am, which employs one-hot encoding to expressthe feature map of the rna sequence and recognizes m(6)am sites by integrating different cnn models via stacking. including densenet, inflated convolutional network (dcnn) and deep multiscale residual network (msrn), the sensitivity (sn), specificity (sp), accuracy (acc), mathews correlation coefficient (mcc) and area under the curve (auc) of our model on the training data set reach 86.62%, 88.94%, 87.78%, 0.7590 and 0.8778, respectively, and the prediction results on the independent test set are as high as 82.25%, 79.72%, 80.98%, 0.6199, and 0.8211.conclusions in conclusion, the experimental results demonstrated that emdl_m6am greatly improved the predictive performance of the m(6)am sites and could provide a valuable reference for the next part of the study. the source code and experimental data are available at: https://github.com/13133989982/emdl-m6am.",AB_0220
"despite the great success of deep learning approaches, retinal disease classification is still challenging as the early-stage pathological regions of retinal diseases may be extremely tiny and subtle, which are difficult for networks to detect. the feature representations learnt by deep learning models focusing more on the local view may lead to indiscriminative semantic-level representation. on the contrary, if they focus more on the global semantic-level, they may ignore the discerning subtle local pathological regions. to address this issue, in this paper, we propose a hybrid framework, combining the strong global semantic representation learning capability of the vision transformer (vit) and the excellent capacity of local representation extraction from the conventional multiple instance learning (mil). particularly, a multiple instance vision transformer (mil-vit) is implemented, where the vanilla vit branch and the mil branch generate semantic probability distributions separately, and a bag consistency loss is proposed to minimize the difference between them. moreover, a calibrated attention mechanism is developed to embed the instance representation into the bag representation in our mil-vit. to further improve the feature representation capability for fundus images, we pre-train the vanilla vit on a large-scale fundus image database. the experimental results validate that the generalization capability of the model using our pre-trained weights for fundus disease diagnosis is better than the one using imagenet pre-trained weights. extensive experiments on four publicly available benchmarks demonstrate that our proposed mil-vit outperforms latest fundus image classification methods, including various deep learning models and deep mil methods. all our source code and pre-trained models are publicly available at https://github.com/greentreeys/mil-vt.",AB_0220
"motivation drug combination therapy has exhibited remarkable therapeutic efficacy and has gradually become a promising clinical treatment strategy of complex diseases such as cancers. as the related databases keep expanding, computational methods based on deep learning model have become powerful tools to predict synergistic drug combinations. however, predicting effective synergistic drug combinations is still a challenge due to the high complexity of drug combinations, the lack of biological interpretability, and the large discrepancy in the response of drug combinations in vivo and in vitro biological systems.results here, we propose dgssynadr, a new deep learning method based on global structured features of drugs and targets for predicting synergistic anticancer drug combinations. dgssynadr constructs a heterogeneous graph by integrating the drug-drug, drug-target, protein-protein interactions and multi-omics data, utilizes a low-rank global attention (lrga) model to perform global weighted aggregation of graph nodes and learn the global structured features of drugs and targets, and then feeds the embedded features into a bilinear predictor to predict the synergy scores of drug combinations in different cancer cell lines. specifically, lrga network brings better model generalization ability, and effectively reduces the complexity of graph computation. the bilinear predictor facilitates the dimension transformation of the features and fuses the feature representation of the two drugs to improve the prediction performance. the loss function smooth l1 effectively avoids gradient explosion, contributing to better model convergence. to validate the performance of dgssynadr, we compare it with seven competitive methods. the comparison results demonstrate that dgssynadr achieves better performance. meanwhile, the prediction of dgssynadr is validated by previous findings in case studies. furthermore, detailed ablation studies indicate that the one-hot coding drug feature, lrga model and bilinear predictor play a key role in improving the prediction performance.availability and implementation dgssynadr is implemented in python using the pytorch machine-learning library, and it is freely available at https://github.com/dhudblab/dgssynadr.",AB_0220
"deep learning has been successfully applied to low-dose ct (ldct) image denoising for reducing potential radiation risk. however, the widely reported supervised ldct denoising networks require a training set of paired images, which is expensive to obtain and cannot be perfectly simulated. unsupervised learning utilizes unpaired data and is highly desirable for ldct denoising. as an example, an artifact disentanglement network (adn) relies on unpaired images and obviates the need for supervision but the results of artifact reduction are not as good as those through supervised learning. an important observation is that there is often hidden similarity among unpaired data that can be utilized. this paper introduces a new learning mode, called quasi-supervised learning, to empower adn for ldct image denoising. for every ldct image, the best matched image is first found from an unpaired normal-dose ct (ndct) dataset. then, the matched pairs and the corresponding matching degree as prior information are used to construct and train our adn-type network for ldct denoising. the proposed method is different from (but compatible with) supervised and semi-supervised learning modes and can be easily implemented by modifying existing networks. the experimental results show that the method is competitive with state-of-the-art methods in terms of noise suppression and contextual fidelity. the code and working dataset are publicly available at https://github.com/ruanyuhui/adn-qsdl.git.",AB_0220
"motivation: gene regulatory networks (grns) are a way of describing the interaction between genes, which contribute to revealing the different biological mechanisms in the cell. reconstructing grns based on gene expression data has been a central computational problem in systems biology. however, due to the high dimensionality and non-linearity of large-scale grns, accurately and efficiently inferring grns is still a challenging task. results: in this article, we propose a new approach, ilsgrn, to reconstruct large-scale grns from steady-state and time-series gene expression data based on non-linear ordinary differential equations. firstly, the regulatory gene recognition algorithm calculates the maximal information coefficient between genes and excludes redundant regulatory relationships to achieve dimensionality reduction. then, the feature fusion algorithm constructs a model leveraging the feature importance derived from xgboost (extreme gradient boosting) and rf (random forest) models, which can effectively train the non-linear ordinary differential equations model of grns and improve the accuracy and stability of the inference algorithm. the extensive experiments on different scale datasets show that our method makes sensible improvement compared with the state-of-the-art methods. furthermore, we perform cross-validation experiments on the real gene datasets to validate the robustness and effectiveness of the proposed method. availability and implementation: the proposed method is written in the python language, and is available at: https://github.com/lab319/ ilsgrn.",AB_0220
