AB,NO
"building change detection (bcd) from satellite imagery is critical for monitoring urbanization, managing agricultural land, and updating geospatial databases. however, complex variations in building roofs that resemble the background of their surroundings pose challenges for deep-learning-based change detection methods due to their focus on color and texture. additionally, downsampling can result in the loss of spatial information, leading to incomplete buildings and irregular output boundaries. to address these challenges, a novel siamese network called afde-net is proposed, which combines differential image features and attention modules using a learnable parameter. the afde-net employs an ensemble spatial-channel attention fusion (escaf) module, along with a deep supervision (ds) module, to mitigate the loss of spatial information and refine deep features in high-dimensional inputs. besides, we have created a new dataset (egy-bcd) comprising high-resolution and multitemporal satellite images captured in four urban and coastal areas in egypt to detect building changes. the egy-bcd dataset includes images with complex types of change, such as tall and dense buildings with roofs that resemble the background of their surroundings, which is a challenge for deep-learning algorithms. the proposed method outperforms other methods on the egy-bcd dataset with an overall accuracy (oa) of 94.3%, an f1-score of 88.8%, and an miou of 86.6%. the datasets and codes will be released at https://github.com/oshholail/egy-bcd.",AB_0285
"the emerging research line of cross-modal learning focuses on the issue of transferring feature representation manner learned from limited multimodal data with labelings to the testing phase with partial modalities. this is essentially common and practical in the remote sensing (rs) community when only modal-incomplete data are in users' hands due to inevitable imaging or access restrictions under large-scale observation scenarios. however, most of the existing cross-modal learning methods have been designed with exclusive reliance on labeling, which can be either limited or noisy due to their costly production. to address this issue, we explore, in this article, the possibility to learn cross-modal feature representation in an unsupervised fashion. by integrating the multimodal data into a fully recombined matrix form, we propose the following: 1) the use of common subspace representation as the regression target instead of conventionally adopted binary labels and 2) the orthogonality and manifold alignment regularization terms to shrink the solution space while preserving the pairwise manifold correlations. through this manner, the modality-specific and mutual latent representations in this common subspace as well as their corresponding projections can be learned simultaneously, and their optimums can be efficiently reached through a nearly one-step computation with the help of eigendecomposition. finally, we show the superiority of our method through extensive image classification experiments on three multimodal datasets with four remotely sensed modalities involved (i.e., hyperspectral (hs), multispectral, synthetic aperture radar, and light detection and ranging (lidar) data). the code and dataset will be made freely available at https://github.com/jingyao16/ucsl to encourage the reproduction of our method and further use.",AB_0285
"haze superimposes a veil over remote sensing images, which severely limits the extraction of valuable military information. to this end, we present a novel trinity model to restore realistic surface information by integrating the merits of both prior- and deep learning-based strategies. concretely, the critical insight of our trinity-net is to investigate how to incorporate prior information into convolutional neural networks (cnns) and swin transformer for reasonable estimation of haze parameters. then, haze-free images are obtained by reconstructing the remote sensing image formation model. although swin transformer has shown tremendous potential in the dehazing task, which typically results in ambiguous details, we devise a gradient guidance module that naturally inherits structure priors of gradient maps, guiding the deep model to generate visually pleasing details. in light of the generality of image formation parameters, we successfully promote trinity-net to natural image dehazing and underwater image enhancement tasks. notably, the acquisition of large-scale remote sensing hazy images and natural hazy images in military scenes is not feasible in practice. to bridge this gap, we construct a remote sensing image dehazing benchmark (rsid) and a natural image dehazing benchmark (nid), including 1000 real-world hazy images with corresponding ground-truth images. to our knowledge, this is the first exploration to develop dehazing benchmarks in the military field, alleviating the dilemma of data scarcity. extensive experiments on three vision tasks illustrate the superiority of our trinity-net against multiple state-of-the-art methods. the datasets and code are available at https://github.com/chi-kaichen/trinity-net.",AB_0285
"pansharpening is a procedure that fuses high-resolution panchromatic (pan) images and low-resolution multispectral (lms) images to derive high-resolution multispectral (hms) images. despite its rapid development, most existing pansharpening techniques integrate the information of pan and lms invariantly in the spatial dimension, ignoring the uneven spatial dependence of restoring hms with the aid of pan information and resulting in ineffective fusion results. in this work, we propose an uncertainty-aware adaptive pansharpening network (uapn) that integrates pan information spatial variantly to restore lms information with an uncertainty mechanism. specifically, we first estimate the epistemic and aleatoric uncertainties together, which model the spatial -variant distributions of restoring the lms image to the hms image. then, we introduce uncertainty-conditioned adaptive convolution (uac) to adaptively integrate lms and pan information, where its parameters are spatially variable by conditioning on the uncertainty estimations. furthermore, we propose a multistage uncertainty-driven loss function to explicitly force the network to concentrate on restoring challenging areas of the lms image. extensive experimental results demonstrate the superiority of our uapn with fewer parameters and flops, outperforming other state-of-the-art methods both qualitatively and quantitatively on multiple satellite datasets. the code is available at https://github.com/keviner1/uapn.",AB_0285
"estimating the direction of arrival (doa) of a single source signal from a single observation of an array data still plays an important part in practical application scenarios. to address the problem, this letter proposes two iterative, fast, and accurate approaches, namely, the q-shift-based doa estimation algorithm (qs-doaea) and the tradeoff between a&m and q-shift-based doa estimation algorithm (taq-doaea). qs-doaea adopts the interpolation of shifted discrete fourier transform (dft)coefficients to iteratively obtain the near-optimal estimation. taq-doaea employs the error function by simultaneously mixing the q-shifted and half-shifted dft coefficients, which only requires two iterations. numerical results reveal that qs-doaea achieves significant improvement in terms of estimation accuracy and taq-doaea expedites the convergence. the proposed estimation algorithms demonstrate that they have an asymptotic variance that is at least 1.0013 times the asymptotic cramer-rao bound (acrb), and provide more than 80x reduction in the computational cost, compared to the baseline method. all our proposed algorithms and code are available at https://github.com/jn-z/.",AB_0285
"the recent success of attention mechanism-driven deep models, like vision transformer (vit) as one of the most representatives, has intrigued a wave of advanced research to explore their adaptation to broader domains. however, current transformer-based approaches in the remote sensing (rs) community pay more attention to single-modality data, which might lose expandability in making full use of the ever-growing multimodal earth observation data. to this end, we propose a novel multimodal deep learning framework by extending conventional vit with minimal modifications, aiming at the task of land use and land cover (lulc) classification. unlike common stems that adopt either linear patch projection or deep regional embedder, our approach processes multimodal rs image patches with parallel branches of position-shared vits extended with separable convolution modules, which offers an economical solution to leverage both spatial and modality-specific channel information. furthermore, to promote information exchange across heterogeneous modalities, their tokenized embeddings are then fused through a cross-modality attention (cma) module by exploiting pixel-level spatial correlation in rs scenes. both of these modifications significantly improve the discriminative ability of classification tokens in each modality and thus further performance increase can be finally attained by a full token-based decision-level fusion module. we conduct extensive experiments on two multimodal rs benchmark datasets, i.e., the houston2013 dataset containing hyperspectral (hs) and light detection and ranging (lidar) data, and berlin dataset with hs and synthetic aperture radar (sar) data, to demonstrate that our extended vision transformer (exvit) outperforms concurrent competitors based on transformer or convolutional neural network (cnn) backbones, in addition to several competitive machine-learning-based models. the source codes and investigated datasets of this work will be made publicly available at https://github.com/jingyao16/exvit.",AB_0285
"cross-modal remote-sensing image-text retrieval (cmrsitr) is a challenging topic in the remote-sensing (rs) community. it has gained growing attention because it can be flexibly used in many practical applications. in the current deep era, with the help of deep convolutional neural networks (dcnns), many successful cmrsitr methods have been proposed. most of them first learn valuable features from rs images and texts, respectively. then, the obtained visual and textual features are mapped into a common space for the final retrieval. the above operations are feasible; however, two difficulties are still to be solved. one is that the semantics within the visual and textual features are misaligned due to the independent learning manner. the other one is that the deep links between rs images and texts cannot be fully explored by simple common space mapping. to overcome the above challenges, we propose a new model named interacting-enhancing feature transformer (ieft) for cmrsitr, which regards the rs images and texts as a whole. first, a simple feature embedding module (fem) is developed to map images and texts into the visual and textual feature spaces. second, an information interacting-enhancing module (iiem) is designed to simultaneously model the inner relationships between rs images and texts and enhance the visual features. iiem consists of three feature interacting-enhancing (fie) blocks, each of which contains an intermodality relationship interacting (imri) subblock and a visual feature enhancing (vfe) subblock. the duty of imri is to exploit the hidden relations between cross-modal data, while the responsibility of vfe is to improve the visual features. by combining them, semantic bias can be mitigated, and the complex contents of rs images can be studied. finally, the retrieval module (rm) is constructed to generate the matching scores for deciding the search results. extensive experiments are conducted on four public rs datasets. the positive results demonstrate that our ieft can achieve superior retrieval performance compared with many existing methods. our source codes are available at https://github.com/tangxu-group/cross-modal-remote-sensing-image-and-text-retrieval-models/tree/main/ieft.",AB_0285
"with the widespread use of face masks due to the covid-19 pandemic, accurate masked face recognition has become more crucial than ever. while several studies have investigated masked face recognition using convolutional neural networks (cnns), there is a paucity of research exploring the use of plain vision transformers (vits) for this task. unlike vit models used in image classification, object detection, and semantic segmentation, the model trained by modern face recognition losses struggles to converge when trained from scratch. to this end, this paper initializes the model parameters via a proxy task of patch reconstruction and observes that the vit backbone exhibits improved training stability with satisfactory performance for face recognition. beyond the training stability, two strategies based on prompts are proposed to integrate holistic and masked face recognition in a single framework, namely facet. along with popular holistic face recognition benchmarks, several open-sourced masked face recognition benchmarks are collected for evaluation. our extensive experiments demonstrate that the proposed facet performs on par or better than state-of-the-art cnns on both holistic and masked face recognition benchmarks. codes will be made available at https://github.com/zyainfal/joint-holistic-and-masked-face-recognition.",AB_0285
"remote-sensing (rs) images present unique challenges for computer vision (cv) due to lower resolution, smaller objects, and fewer features. mainstream backbone networks show promising results for traditional visual tasks. however, they use convolution to reduce feature map dimensionality, which can result in information loss for small objects in rs images and decreased performance. to address this problem, we propose a new and universal downsampling module named robust feature downsampling (rfd). rfd fuses multiple feature maps extracted by different downsampling techniques, creating a more robust feature map with a complementary set of features. leveraging this, we overcome the limitations of conventional convolutional downsampling, resulting in a more accurate and robust analysis of rs images. we develop two versions of the rfd module, shallow rfd (srfd) and deep rfd (drfd), tailored to adapt to different stages of feature capture and improve feature robustness. we replace the downsampling layers (dsl) of existing mainstream backbones with the rfd module and conduct comparative experiments on several public rs image datasets. the results show significant improvements compared to baseline approaches in rs image classification, object detection, and semantic segmentation. specifically, our rfd module achieved an average performance gain of 1.5% on the nwpu-resisc45 classification dataset without utilizing any additional pretraining data, resulting in state-of-the-art performance on this dataset. moreover, in detection and segmentation tasks on dataset for object detection in aerial images (dota) and instance segmentation in aerial images dataset (isaid), our rfd module outperforms the baseline approaches by 2%-7% when utilizing pretraining data from nwpu-resisc45. these results highlight the value of the rfd module in enhancing the performance of rs visual tasks. the code is available at https://github.com/lwcver/rfd.",AB_0285
"thanks to their capability of modeling global information, transformers have been recently applied to change detection (cd) in remote sensing images. generally, the changes in terms of shape and appearance of objects lead to relation changes among these objects in multitemporal images. however, in this context, the attention mechanism in transformers has not been fully explored yet to learn relation changes in the observed scenes. in this article, we analyze the relation changes in multitemporal images and propose a cross-temporal difference (ctd) attention to capturing these changes efficiently. through the ctd attention, the changed areas are distinguished better from the unchanged areas. based on the ctd attention, two ctd-transformer encoders are constructed to extract the features of changed areas from the embedded tokens of multitemporal images in a cross manner. then, the extracted features at the coarse scale are further improved to the fine-scale by the corresponding ctd-transformer decoders. in addition, consistency-perception blocks (cpbs) are designed to preserve the structures and contours of changed areas. finally, all extracted features from multitemporal images are concatenated to produce the desired change map. compared to state-of-the-art methods, experimental results on levir-cd, whu-cd, and clcd datasets demonstrate that the proposed method produces better performance. the source code is available at https://github.com/rsmagneto/ctd-former.",AB_0285
