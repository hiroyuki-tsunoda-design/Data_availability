AB,NO
"background: local assembly with short and long reads has proven to be very useful in many applications: reconstruction of the sequence of a locus of interest, gap-filling in draft assemblies, as well as alternative allele reconstruction of large structural variants. whereas linked-read technologies have a great potential to assemble specific loci as they provide long-range information while maintaining the power and accuracy of short-read sequencing, there is a lack of local assembly tools for linked-read data. results: we present mtg-link, a novel local assembly tool dedicated to linked-reads. the originality of the method lies in its read subsampling step which takes advantage of the barcode information contained in linked-reads mapped in flanking regions. we validated our approach on several datasets from different linked-read technologies. we show that mtg-link is able to assemble successfully large sequences, up to dozens of kb. we also demonstrate that the read subsampling step of mtg-link considerably improves the local assembly of specific loci compared to other existing short-read local assembly tools. furthermore, mtg-link was able to fully characterize large insertion variants and deletion breakpoints in a human genome and to reconstruct dark regions in clinically-relevant human genes. it also improved the contiguity of a 1.3 mb locus of biological interest in several individual genomes of the mimetic butterfly heliconius numata. conclusions: mtg-link is an efficient local assembly tool designed for different linkedread sequencing technologies. mtg-link source code is available at https://github. com/anne-gcd/mtg-link and as a bioconda package.",AB_0462
"in existing deep clustering methods, as the model gets deeper, extracted representations can be deteriorated due to a vanishing gradient, leading to reduced performance. also, existing deep clustering frameworks may distort the hidden space without true labels by learning from unreliable estimated pseudo-labels. therefore, the model learns non-discriminative features from the early epochs, which results in worse estimated pseudo-labels. moreover, in these models, it is assumed that all learned representations in the bottleneck layer have equal significance throughout the clustering procedure. to address the shortcomings, this paper introduces rdeic-lfw-dss, a resnet-based deep embedded clustering framework. the advantage of residual connection is that it enables the user to add residual connections for increased model capacity without incurring the cost of degradation for unsupervised feature learning compared to standard autoencoders. furthermore, an efficient local feature weighting mechanism is used to weight each cluster's representations correctly. in addition, we introduce a mechanism in which suitable samples with highly reliable estimated labels are selected to train the deep clustering framework. extensive experiments conducted on benchmark datasets and comparisons with state-of-the-art approaches confirm the high performance of rdeic-lfw-dss. the implementation- source code- of rdeic-lfw-dss is made publicly accessible at https://github.com/amin-golzari-oskouei/rdeic-lfw-dss.",AB_0462
"medical signal classification often focuses on one representation (raw signal or time frequency). in that context, recent works have shown the value of exploiting different representations simultaneously. we propose a regularized end-to-end trained model for classification in a medical context exploiting both the raw signal and a time-frequency representation (tfr). first, a 2d convolutional neural network (cnn) encoder and a 1d cnn-transformer encoder start by extracting embedded representations from the tfr and the raw signal, respectively. then, the obtained embeddings are fused to form a common latent space that is used for classification. we propose to guide the training of each encoder by applying two iterated losses. moreover, we propose to regularize the fused common latent space using deep embedded clustering. extensive experiments on three medical datasets and ablation studies show the adaptability and good performance of our method for medical signal classification. our method makes it possible to improve the classification performance from 4% to 12% mcc on a transcranial doppler dataset, when compared with single-feature counterparts, while giving more stable models. the code is available at: https://github.com/gdec-submission/gdec/ . & copy; 2023 elsevier ltd. all rights reserved.",AB_0462
"as the importance of eco-friendly transportation increases, providing an efficient approach for marine vessel operation is essential. methods for status monitoring with consideration to the weather condition and forecasting with the use of in-service data from ships requires accurate and complete models for predicting the energy efficiency of a ship. the models need to effectively process all the operational data in real-time. this paper presents models that can predict fuel consumption using in-service data collected from a passenger ship. statistical and domain-knowledge methods were used to select the proper input variables for the models. these methods prevent over-fitting, missing data, and multicollinearity while providing practical applicability. prediction models that were investigated include multiple linear regression (mlr), decision tree approach (dt), an artificial neural network (ann), and ensemble methods. the best predictive performance was from a model developed using the xgboost technique which is a boosting ensemble approach. our code is available on github at https://github.com/pagand/model_optimze_vessel/tree/oe for future research.",AB_0462
"the arctic ocean is subject to high rates of ocean warming and acidification, with critical implications for marine organisms as well as ecosystems and the services they provide. carbonate system data in the arctic realm are spotty in space and time, and, until recently, there was no time-series station measuring the carbonate chemistry at high frequency in this region, particularly in coastal waters. we report here on the first high-frequency (1 h), multi-year (5 years) dataset of salinity, temperature, co2 partial pressure (pco(2)) and ph at a coastal site (bottom depth of 12 m) in a high-arctic fjord (kongsfjorden, svalbard). discrete measurements of dissolved inorganic carbon and total alkalinity were also performed. we show that (1) the choice of formulations for calculating the dissociation constants of the carbonic acid remains unsettled for polar waters, (2) the water column is generally somewhat stratified despite the shallow depth, (3) the saturation state of calcium carbonate is subject to large seasonal changes but never reaches undersaturation (o-a ranges between 1.4 and 3.0) and (4) pco(2) is lower than atmospheric co2 at all seasons, making this site a sink for atmospheric co2 (-9 to -16.8 molco(2)m(-2)yr(-1), depending on the parameterisation of the gas transfer velocity). data are available onpangaea: https://doi.org/10.1594/pangaea.960131 (gattuso et al., 2023a).",AB_0462
"compared to nuclear genomes, mitochondrial genomes (mitogenomes) are small and usually code for only a few dozen genes. still, identifying genes and their structure can be challenging and time-consuming. even automated tools for mitochondrial genome annotation often require manual analysis and curation by skilled experts. the most difficult steps are (i) the structural modelling of intron-containing genes; (ii) the identification and delineation of group i and ii introns; and (iii) the identification of moderately conserved, non-coding rna (ncrna) genes specifying 5s rrnas, tmrnas and rnase p rnas. additional challenges arise through genetic code evolution which can redefine the translational identity of both start and stop codons, thus obscuring protein coding genes. further, rna editing can render gene identification difficult, if not impossible, without additional rna sequence data. current automated mitoand plastid-genome annotators are limited as they are typically tailored to specific eukaryotic groups. the mfannot annotator we developed is unique in its applicability to a broad taxonomic scope, its accuracy in gene model inference, and its capabilities in intron identification and classification. the pipeline leverages curated profile hidden markov models (hmms), covariance (cms) and erpin models to better capture evolutionarily conserved signatures in the primary sequence (hmms and cms) as well as secondary structure (cms and erpin). here we formally describe mfannot, which has been available as a web-accessible service (https://megasun.bch.umontreal.ca/apps/mfannot/) to the research community for nearly 16 years. further, we report its performance on particularly intron-rich mitogenomes and describe ongoing and future developments.",AB_0462
"we describe a freely available web server called retention index predictor (ripred) ( https://ripred.ca ) that rapidly and accurately predicts gas chromatographic kovats retention indices (ri) using smiles strings as chemical structure input. ripred performs ri prediction for three different stationary phases (semi-standard non-polar (ssnp), standard non-polar (snp), and standard polar (sp)) for both deriva-tized (trimethylsilyl (tms) and tert-butyldimethylsilyl (tbdms) derivatized) and underivatized (base com-pound) forms of gc-amenable structures. ripred was developed to address the need for freely available, fast, highly accurate ri predictions for a wide range of derivatized and underivatized chemicals for all common gc stationary phases. ripred was trained using a graph neural network (gnn) that used com-pound structures, their extracted features (mostly atom-level features) and the gc-ri data from the na-tional institute of standards and technology databases (nist 17 and nist 20). we curated this nist 17 and nist 20 gc-ri data, which is available for all three stationary phases, to create appropriate inputs (molecular graphs in this case) needed to enhance our model performance. the performance of different ripred predictive models was evaluated using 10-fold cross validation (cv). the best performing ripred models were identified and when tested on hold-out test sets from all stationary phases, achieved a mean absolute error (mae) of < 73 ri units (ssnp: 16.5-29.5, snp: 38.5-45.9, sp: 46.52-72.53). the mean ab-solute percentage error (mape) of these models were typically within 3% (ssnp: 0.78-1.62%, snp: 1.87- 2.88%, sp: 2.34-4.05%). when compared to the best performing model by qu et al., 2021, ripred per-formed similarly (mae of 16.57 ri units [ripred] vs. 16.84 ri units [qu et al., 2021 predictor] for deriva-tized compounds). ripred also includes-5 million predicted ri values for all gc-amenable compounds (-57,0 0 0) in the human metabolome database hmdb 5.0 (wishart et al., 2022).& copy; 2023 the author(s). published by elsevier b.v. this is an open access article under the cc by-nc-nd license (  )",AB_0462
"motivation: sequencing coverage is among key determinants considered in the design of omics studies. to help estimate cost-effective sequencing coverage for specific downstream analysis, downsampling, a technique to sample subsets of reads with a specific size, is routinely used. however, as the size of sequencing becomes larger and larger, downsampling becomes computationally challenging. results: here, we developed an approximate downsampling method called s-leaping that was designed to efficiently and accurately process large-size data. we compared the performance of s-leaping with state-of-the-art downsampling methods in a range of practical omics-study downsampling settings and found s-leaping to be up to 39% faster than the second-fastest method, with comparable accuracy to the exact downsampling methods. to apply s-leaping on fastq data, we developed a light-weight tool called fadso in c. using whole-genome sequencing data with 208 million reads, we compared fadso's performance with that of a commonly used fastq tool with the same downsampling feature and found fadso to be up to 12% faster with 21% lower memory usage, suggesting fadso to have up to 40% higher throughput in a parallel computing setting. availability and implementation: the c source code for s-leaping, as well as the fadso package is freely available at https://github.com/hkuwa hara/sleaping.",AB_0462
"unconstrained handwritten text recognition is a challenging computer vision task. it is traditionally handled by a two-step approach, combining line segmentation followed by text line recognition. for the first time, we propose an end-to-end segmentation-free architecture for the task of handwritten document recognition: the document attention network. in addition to text recognition, the model is trained to label text parts using begin and end tags in an xml-like fashion. this model is made up of an fcn encoder for feature extraction and a stack of transformer decoder layers for a recurrent token-by-token prediction process. it takes whole text documents as input and sequentially outputs characters, as well as logical layout tokens. contrary to the existing segmentation-based approaches, the model is trained without using any segmentation label. we achieve competitive results on the read 2016 dataset at page level, as well as double-page level with a cer of 3.43% and 3.70%, respectively. we also provide results for the rimes 2009 dataset at page level, reaching 4.54% of cer. we provide all source code and pre-trained model weights at https://github.com/factodeeplearning/dan.",AB_0462
"background: transanal endoscopic surgery is an organ-sparing treatment for early rectal cancer. patients with advanced lesions are recommended for total mesorectal excision. however, some patients have prohibitive comorbidities or refuse major surgery. objective: to assess the cancer outcomes of patients with t2 or t3 rectal cancers who received transanal endoscopic surgery as their sole surgical treatment. design: this study used a prospectively maintained database. setting: a tertiary hospital in canada. patients: patients who underwent transanal endoscopic surgery for pathology-confirmed t2 or t3 rectal adenocarcinomas from 2007-2020 were included. main outcome measures: disease-free survival and overall survival, stratified by tumor stage and reason for transanal endoscopic surgery. results: among the included 132 patients (t2, n = 96; t3, n = 36), average follow-up was 22 months. twenty-eight decline oncologic resection, whereas 104 had preclusive comorbidities. fifteen patients (11.4%) had disease recurrence (4 local, 11 metastatic). three-year disease-free survival was 86.5% (95% ci, 77.1-95.9) for t2 and 67.9% (95% ci, 46.3-89.5) for t3 tumors. mean disease-free survival was longer for t2 (75.0 mo; 95% ci, 67.8-82.1) compared to t3 cancers (50 mo; 95% ci, 37.7-62.3; p = 0.037). three-year disease-free survival for patients who declined radical excision was 84.0% (95% ci, 67.1-100) versus 80.7% (95% ci, 69.7-91.7) in patients too comorbid for surgery. three-year overall survival rate was 84.9% (95% ci, 73.9-95.9) for t2 and 49.0% (95% ci, 26.7-71.3) for t3 tumors. patients who declined radical resection had similar 3-year overall survival (89.7%; 95% ci, 76.2-100) compared to patients who were unable to undergo excision because of medical comorbidities (98.1%; 95% ci, 95.6-100). limitations: small sample, single institution, and surgeon experience. conclusions: oncologic outcomes are compromised in patients treated by transanal endoscopic surgery for t2 and t3 rectal cancer. transanal endoscopic surgery remains an option for informed patients who prefer to avoid radical resection. see video abstract at http:// links.lww.com/dcr/c200.",AB_0462
