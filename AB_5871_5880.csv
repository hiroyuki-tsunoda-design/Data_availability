AB,NO
"computer vision techniques have immense potential for materials design applications. in this work, we introduce an integrated and general-purpose atomvision library that can be used to generate and curate microscopy image (such as scanning tunneling microscopy and scanning transmission electron microscopy) data sets and apply a variety of machine learning techniques. to demonstrate the applicability of this library, we (1) establish an atomistic image data set of about 10 000 materials with large structural and chemical diversity, (2) develop and compare convolutional and atomistic line graph neural network models to classify the bravais lattices, (3) demonstrate the application of fully convolutional neural networks using u-net architecture to pixelwise classify atom versus background, (4) use a generative adversarial network for super resolution, (5) curate an image data set on the basis of natural language processing using an open-access arxiv data set, and (6) integrate the computational framework with experimental microscopy images for rh, fe3o4, and sns systems. the atomvision library is available at https://github.com/ usnistgov/atomvision.",AB_0588
"objective genetic alterations are increasingly recognized as etiologic factors linked to the pathogenesis and development of cerebrovascular anomalies. their identification allows for advanced screening and targeted therapeutic approaches. the authors aimed to describe the role of a collaborative approach to care and genetic testing in pediatric patients with neurovascular anomalies, with the objectives of identifying what genetic testing recommendations were made, the yield of genetic testing, and the implications for familial screening and management at present and in the future. methods the authors performed a descriptive retrospective cohort study examining pediatric patients genetically screened through the pediatric neurovascular program of a single treatment center. patients 18 years of age and younger with neurovascular anomalies, diagnosed radiographically or histopathologically, were evaluated for germline genetic testing. patient demographic data and germline genetic testing and recommendation, clinical, treatment, and outcome data were collected and analyzed.results sixty patients were included; 29 (47.5%) were female. the mean age at consultation was 11.0 +/- 4.9 years. diagnoses included cerebral arteriovenous malformations (avms) (n = 23), cerebral cavernous malformations (n = 19), non-neurofibromatosis/non-sickle cell moyamoya (n = 8), diffuse cerebral proliferative angiopathy, and megalencepha- ly-capillary malformation. of the 56 patients recommended to have genetic testing, 40 completed it. genetic alterations were found in 13 (23%) patients. four patients with avms had rasa1, gdf2, and acvrl1 mutations. four patients with cavernous malformations had krit1 mutations. one with moyamoya disease had an rnf213 mutation. three patients with megalencephaly-capillary malformation had pik3ca mutations, and 1 patient with a cavernous sinus lesion had an med12 mutation. the majority of avm patients were treated surgically. patients with diffuse cerebral proliferative angiopathy were treated medically with sirolimus. at-risk relatives of 3 patients positive for genetic anomalies had also been tested.conclusions this study demonstrates a role for exploring genetic alterations in the identification and treatment of pediatric neurovascular disease pathogenesis. germline genetic mutations were found in almost one-quarter of the patients screened in this study, results that helped to identify medically targeted treatment modalities for some pediatric neurovascular patients. insight into the genetic etiology of vascular anomalies may provide broader clinical implications for risk assessment, family screening, follow-up surveillance, and medical management. https://thejns.org/doi/abs/10.3171/2022.11.peds22392",AB_0588
"before the recent success of deep learning methods for automated medical image analysis, practitioners used handcrafted radiomic features to quantitatively describe local patches of medical images. however, extracting discriminative radiomic features relies on accurate pathology localization, which is difficult to acquire in real-world settings. despite advances in disease classification and localization from chest x-rays, many approaches fail to incorporate clinically-informed domainspecific radiomic features. for these reasons, we propose a radiomics-guided transformer (rgt) that fuses global image information with local radiomics-guided auxiliary information to provide accurate cardiopulmonary pathology localization and classification without any bounding box annotations. rgt consists of an image transformer branch, a radiomics transformer branch, and fusion layers that aggregate image and radiomics information. using the learned self-attention of its image branch, rgt extracts a bounding box for which to compute radiomic features, which are further processed by the radiomics branch; learned image and radiomic features are then fused and mutually interact via cross-attention layers. thus, rgt utilizes a novel end-to-end feedback loop that can bootstrap accurate pathology localization only using image-level disease labels. experiments on the nih chestxray dataset demonstrate that rgt outperforms prior works in weakly supervised disease localization (by an average margin of 3.6% over various intersection-over-union thresholds) and classification (by 1.1% in average area under the receiver operating characteristic curve). we publicly release our codes and pre-trained models at https://github.com/vitagroup/chext.",AB_0588
"phylogenetic placement, the problem of placing a query sequence into a precomputed phylogenetic backbone tree, is useful for constructing large trees, performing taxon identification of newly obtained sequences, and other applications. the most accurate current methods, such as pplacer and epa-ng, are based on maximum likelihood and require that the query sequence be provided within a multiple sequence alignment that includes the leaf sequences in the backbone tree. this approach enables high accuracy but also makes these likelihood-based methods computationally intensive on large backbone trees, and can even lead to them failing when the backbone trees are very large (e.g., having 50,000 or more leaves). we present scampp (scaling alignment-based phylogenetic placement), a technique to extend the scalability of these likelihood-based placement methods to ultra-large backbone trees. we show that pplacer-scampp and epa-ng-scampp both scale well to ultra-large backbone trees (even up to 200,000 leaves), with accuracy that improves on apples and apples-2, two recently developed fast phylogenetic placement methods that scale to ultra-large datasets. epa-ng-scampp and pplacer-scampp are available at https://github.com/chry04/plusplacer.",AB_0588
"background: metagenomic taxonomic profiling aims to predict the identity and relative abundance of taxa in a given whole-genome sequencing metagenomic sample. a recent surge in computational methods that aim to accurately estimate taxonomic profiles, called taxonomic profilers, has motivated community-driven efforts to create standardized benchmarking datasets and platforms, standardized taxonomic profile formats, and a benchmarking platform to assess tool performance. while this standardization is essential, there is currently a lack of tools to visualize the standardized output of the many existing taxonomic profilers. thus, benchmarking studies rely on a single-value metrics to compare performance of tools and compare to benchmarking datasets. this is one of the major problems in analyzing metagenomic profiling data, since single metrics, such as the f1 score, fail to capture the biological differences between the datasets. findings: here we report the development of tampa (taxonomic metagenome profiling evaluation), a robust and easy-to-use method that allows scientists to easily interpret and interact with taxonomic profiles produced by the many different taxonomic profiler methods beyond the standard metrics used by the scientific community. we demonstrate the unique ability of tampa to generate a novel biological hypothesis by highlighting the taxonomic differences between samples otherwise missed by commonly utilized metrics. conclusion: in this study, we show that tampa can help visualize the output of taxonomic profilers, enabling biologists to effectively choose the most appropriate profiling method to use on their metagenomics data. tampa is available on github, bioconda, and galaxy toolshed at https://github.com/dkoslicki/tampa and is released under the mit license.",AB_0588
"we propose destin2, a novel statistical and computational method for cross -modality dimension reduction, clustering, and trajectory reconstruction for single-cell atac-seq data. the framework integrates cellular-level epigenomic profiles from peak accessibility, motif deviation score, and pseudo-gene activity and learns a shared manifold using the multimodal input, followed by clustering and/or trajectory inference. we apply destin2 to real scatac-seq datasets with both discretized cell types and transient cell states and carry out benchmarking studies against existing methods based on unimodal analyses. using cell-type labels transferred with high confidence from unmatched single-cell rna sequencing data, we adopt four performance assessment metrics and demonstrate how destin2 corroborates and improves upon existing methods. using single-cell rna and atac multiomic data, we further exemplify how destin2's cross-modality integrative analyses preserve true cell-cell similarities using the matched cell pairs as ground truths. destin2 is compiled as a freely available r package available at https://github.com/yuchaojiang/destin2.",AB_0588
"mosdef-gomc is a python interface for the monte carlo software gomc to the molecular simulation design framework (mosdef) ecosystem. mosdef-gomc automates the process of generating initial coordinates, assigning force field parameters, and writing coordinate (pdb), connectivity (psf), force field parameter, and simulation control files. the software lowers entry barriers for novice users while allowing advanced users to create complex workflows that encapsulate simulation setup, execution, and data analysis in a single script. all relevant simulation parameters are encoded within the workflow, ensuring reproducible simulations. mosdef-gomc's capabilities are illustrated through a number of examples, including prediction of the adsorption isotherm for co2 in irmof-1, free energies of hydration for neon and radon over a broad temperature range, and the vapor-liquid coexistence curve of a four-component surrogate for the jet fuel s-8. the mosdef-gomc software is available on github at https://github.com/gomc-wsu/mosdef-gomc.",AB_0588
"a technique for the simulation of dark bremsstrahlung for electrons and muons in geant4 is presented. the total cross section is calculated using the weizsacker-williams approximation and the outgoing kinematics are produced by scaling events produced in madgraph/madevent to lower incident lepton energies, allowing the simulation to account for thick targets and lepton sources without fixed energies. compared with dedicated samples produced at an arbitrary particle energy, typical precision of better than 5% is achieved.program summaryprogram title: g4darkbremcpc library link to program files: https://doi .org /10 .17632 /3pxfrz8cmn .1licensing provisions: apache-2.0programming language: c++nature of problem: to accurately simulate the kinematics of the dark bremsstrahlung process for electrons and muons in thick targets and lepton sources without fixed energies. the process must be included at the level of experimental simulation instead of using initial state event generators to account for the possibility of energy loss through bremsstrahlung or multiple scattering prior to the dark matter interaction. accurate kinematic simulation of the outgoing lepton are required for optimal experimental sensitivity measurements and appropriate design of search strategies. solution method: we embed the dark bremsstrahlung process into geant4. the cross section is calculated using numerical integrals of the weizsacker-williams approximation, and the kinematics are simulated using a scaling technique of madgraph/madevent event libraries. the accuracy of the total cross section and kinematics is validated using madgraph/madevent samples.(c) 2023 elsevier b.v. all rights reserved.",AB_0588
"phase stability, defect formation energies, and carrier concentrations are closely interrelated features of semiconductors. due to their joint dependence on the multidimensional chemical potential space, it is challenging to quantitatively establish patterns between these quantities in a given semiconductor, especially when the semiconductor is comprised of multiple elements. to enable synchronous visualiza-tion and analysis of these complementary material properties and their interdependence, we developed the visualization toolkit for analyzing defects in materials (vtandem). this python-based toolkit allows users to interactively explore how defect formation energies and carrier concentrations vary across the composition and chemical potential spaces of multicomponent semiconductors. here, we illustrate the computational workflow that employs vtandem as a post-processing tool for first-principles calculations and describe the data organization and theory underlying the visualization scheme. we believe that this software will serve as a useful tool for simultaneously visualizing the often complex and non-intuitive chemical potential - defect - carrier concentration phase space of semiconductors.program summary program title: vtandem - visualization toolkit for analyzing defects in materialscpc library link to program files: https://doi .org /10 .17632 /hz7dyc489v.1developer's repository link: https://github .com /ertekin -research -group /vtandemlicensing provisions: mit licenseprogramming language: pythonnature of problem: defect thermodynamics are often studied from the perspective of phase stabil -ity and defect formation energetics using first-principles calculations. the results are comparable to experimentally-measurable carrier concentrations. however, visualizing all properties simultaneously by exploring the multidimensional chemical phase space is not trivial.solution method: vtandem offers a graphical interface that allows the user to interact directly with the chemical phase space of a given material and to visualize the defect formation energetics and ensuing carrier concentrations. the computational methods derive from standard defect theory within the su-percell approach. the synchronous visualization scheme provides a streamlined approach to analyzing defect-related properties in semiconductors and insulators, all in real time.additional comments including restrictions and unusual features: required packages, installation, and tutorials can be found on the github page.(c) 2023 elsevier b.v. all rights reserved.",AB_0588
"cloud-based software has many advantages. when services are divided into many independent components, they are easier to update. also, during peak demand, it is easier to scale cloud services (just hire more cpus). hence, many organizations are partitioning their monolithic enterprise applications into cloud-based microservices.recently there has been much work using machine learning to simplify this partitioning task. despite much research, no single partitioning method can be recommended as generally useful. more specifically, those prior solutions are brittle; i.e. if they work well for one kind of goal in one dataset, then they can be sub-optimal if applied to many datasets and multiple goals.this work extends prior work and proposes deeply to fix the brittleness problem. specifically, we use (a) hyper-parameter optimization to sample from the pareto frontier of configurations (b) a weighted loss to choose optimally from this pareto frontier (c) the 1cycle learning rate policy to avoid local minima with adam and (d) spectral clustering over k-means. our work shows that deeply outperforms other algorithms in this space across different metrics. moreover, our ablation study reveals that of the changes, the weighted loss is the most important, followed by hyper-parameter optimization (contrary to prior belief).to enable the reuse of this research, deeply is available on-line at https://bit.ly/2whfflb.",AB_0588
