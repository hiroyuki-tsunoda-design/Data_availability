AB,NO
"meta-heuristic search algorithms (mhss) are methods that take their inspiration from nature. however, the fitness value information used in the design of the update mechanism in mhss is insufficient to represent the concept of adaptation to the environment and the ability to survive in nature. this causes problems in the selection of survivors and the premature convergence in the search process. this article introduces the natural survivor method (nsm), developed as a design for population management as it occurs in nature, depending on analytical relationships and environmental factors. in the nsm, scores representing the adaptability of individuals to nature are calculated in order to determine the survivors. in this proposed method, the update mechanism is designed using nsm scores instead of fitness values. the nsm is the first study presented to the literature on this subject since the 1980s, when the meta-heuristics was introduced. the nsm was used for survivor selection by applying it to three different types of mhs algorithms based on physics (sfs), biology (tlabc), and evolution (lshade-spacma). thirty-nine global optimization problems in the ieee cec 2017/2020 benchmark suites and ten constrained real-world engineering problems were used in the experimental studies. data obtained from experimental studies were analyzed by using non-parametric statistical test methods. among the 25 competing algorithms according to friedman scores, the rankings of the three algorithms with nsm and their original versions are as follows: while tlabc is 18th, nsm-tlabc is 9th, sfs is 10th, nsm-sfs is 6th, lshade-spacma is 3rd, nsm-lshade-spacma is 1st. according to the results of the wilcoxon pairwise comparison test between the original and nsm versions of the algorithms, the nsm versions have a clear advantage in finding optimal solutions. however, the drawback of the proposed method is that it increases the computational complexity of the algorithms. the source codes of the nsm (nsm-lshade_spacma, nsm-tlabc and nsm-sfs) can be accessed at this link: https://www.mathworks. com/matlabcentral/fileexchange/126050-natural-survivor-method-nsm-for-metaheuristic-algorithms.",AB_0540
"proteomic studies characterize the protein composition of complex biological samples. despite recent advancements in mass spectrometry instrumentation and computational tools, low proteome coverage and interpretability remains a challenge. to address this, we developed proteome support vector enrichment (prose), a fast, scalable and lightweight pipeline for scoring proteins based on orthogonal gene co-expression network matrices. prose utilizes simple protein lists as input, generating a standard enrichment score for all proteins, including undetected ones. in our benchmark with 7 other candidate prioritization techniques, prose shows high accuracy in missing protein prediction, with scores correlating strongly to corresponding gene expression data. as a further proof-of-concept, we applied prose to a reanalysis of the cancer cell line encyclopedia proteomics dataset, where it captures key phenotypic features, including gene dependency. we lastly demonstrated its applicability on a breast cancer clinical dataset, showing clustering by annotated molecular subtype and identification of putative drivers of triple-negative breast cancer. prose is available as a user-friendly python module from https://github.com/bwbio/prose.",AB_0540
"introduction research using animal models suggests that intensive motor skill training in infants under 2 years old with cerebral palsy (cp) may significantly reduce, or even prevent, maladaptive neuroplastic changes following brain injury. however, the effects of such interventions to tentatively prevent secondary neurological damages have never been assessed in infants with cp. this study aims to determine the effect of the baby hand and arm bimanual intensive therapy including lower extremities (baby habit-ile) in infants with unilateral cp, compared with a control intervention. methods and analysis this randomised controlled trial will include 48 infants with unilateral cp aged (corrected if preterm) 6-18 months at the first assessment. they will be paired by age and by aetiology of the cp, and randomised into two groups (immediate and delayed). assessments will be performed at baseline and at 1 month, 3 months and 6 months after baseline. the immediate group will receive 50 hours of baby habit-ile intervention over 2 weeks, between first and second assessment, while the delayed group will continue their usual activities. this last group will receive baby habit-ile intervention after the 3-month assessment. primary outcome will be the mini-assisting hand assessment. secondary outcomes will include behavioural assessments for gross and fine motricity, visual-cognitive-language abilities as well as mri and kinematics measures. moreover, parents will determine and score child-relevant goals and fill out questionnaires of participation, daily activities and mobility. ethics and dissemination full ethical approval has been obtained by the comite d'ethique hospitalo-facultaire/universite catholique de louvain, brussels (2013/01mar/069 b403201316810g). the recommendations of the ethical board and the belgian law of 7 may 2004 concerning human experiments will be followed. parents will sign a written informed consent ahead of participation. findings will be published in peer-reviewed journals and conference presentations. trial registration number nct04698395. registered on the international clinical trials registry platform (ictrp) on 2 december 2020 and nih clinical trials registry on 6 january 2021. url of trial registry record: https://clinicaltrials.gov/ct2/show/nct04698395?term=bleyenheuft&draw=1&rank=7.",AB_0540
"recent progress in image recognition has stimulated the deployment of vision systems at an unprecedented scale. as a result, visual data are now often consumed not only by humans but also by machines. existing image processing methods only optimize for better human perception, yet the resulting images may not be accurately recognized by machines. this can be undesirable, e.g., the images can be improperly handled by search engines or recommendation systems. in this work, we examine simple approaches to improve machine recognition of processed images: optimizing the recognition loss directly on the image processing network or through an intermediate input transformation model. interestingly, the processing model's ability to enhance recognition quality can transfer when evaluated on models of different architectures, recognized categories, tasks, and training datasets. this makes the methods applicable even when we do not have the knowledge of future recognition models, e.g., when uploading processed images to the internet. we conduct experiments on multiple image processing tasks paired with imagenet classification and pascal voc detection as recognition tasks. with these simple yet effective methods, substantial accuracy gain can be achieved with strong transferability and minimal image quality loss. through a user study we further show that the accuracy gain can transfer to a black-box cloud model. finally, we try to explain this transferability phenomenon by demonstrating the similarities of different models' decision boundaries. code is available at https://github.com/liuzhuang13/transferable_ra.",AB_0540
"in mass spectrometry (ms)-based proteomics, protein inference from identified peptides (protein fragments) is a critical step. we present proinfer (protein inference), a novel protein assembly method that takes advantage of information in biological networks. proinfer assists recovery of proteins supported only by ambiguous peptides (a peptide which maps to more than one candidate protein) and enhances the statistical confidence for proteins supported by both unique and ambiguous peptides. consequently, proinfer rescues weakly supported proteins thereby improving proteome coverage. evaluated across thp1 cell line, lung cancer and raw267.4 datasets, proinfer always infers the most numbers of true positives, in comparison to mainstream protein inference tools fido, epifany and pia. proinfer is also adept at retrieving differentially expressed proteins, signifying its usefulness for func-tional analysis and phenotype profiling. source codes of proinfer are available at https:// github.com/pennhui2016/proinfer.",AB_0540
"atomistic modeling of nanostructures often leads to computationally challenging problems involving millions of atoms and tens of thousands of coulomb matrix elements. in our previous work, we presented a practical solution to this problem, where quasi-linear efficiency, both in time and memory, was obtained by utilizing the fast fourier transform. here, we present an updated version of our highly-parallelized computer program, named coulombo-lattice, that eliminates the necessity of introducing an auxiliary basis set for the wave-function transfer to the computational grid. here, we instead exploit the properties of the underlying crystal lattice and run calculations on a regular three-dimensional grid superimposed on the original, lower-symmetry lattice. due to removal of spurious interactions from other supercells, the resulting coulomb matrix elements are, up to numerical precision, identical to those obtained by the direct summation o (n2) method, yet our code maintains o (n log n) scaling. we illustrate our approach by calculations involving up to 1.7 million integrals, and number of atoms reaching up to 2.8 million, for the problem of dopant charging energy for a single phosphorus dopant embedded in a silicon lattice. next, to emphasize the broad applicability of our code, we show the results for mixed zinc-blend/wurtzite lattice systems, also known as crystal phase quantum dots.program summary programtitle: coulombo-lattice cpc library link to program files: https://doi .org /10 .17632 /jwkvh5ycbf .1licensing provisions: cc by 4.0programming language: c++nature of problem: computing the coulomb matrix elements (coulomb and exchange integrals), while being a demanding computational task, is a necessary step in a range of quantum mechanical calculations. for example, in the field of nanostructures, such as quantum dots and nanowires or novel single dopant devices, this stage of calculation (even after a series of approximations) is at least an o (n2) operation (a summation over all pairs of n atoms or grid points in the analyzed system). moreover, calculating the full coulomb matrix usually requires the computation of thousands of such elements, thus presenting a formidable computational challenge.solution method: we provide a ready-to-use implementation for calculating coulomb matrix elements for a given set of input wavefunctions in lcao (linear combination of atomic orbitals) form resulting from tight-binding calculation. this implementation is based on the approach introduced in [1,2], by using a fast fourier transform. in this work, we further significantly improved the method by eliminating the need to introduce wave-function transfer via auxiliary basis set.additional comments including restrictions and unusual features: the implementation is fully parallelized in a distributed-memory model, using mpi and parallel routines from fftw [3].",AB_0540
"this paper takes a parallel learning approach in continual learning scenarios. we define parallel continual learning as learning a sequence of tasks where the data for the previous tasks, whose distribution may have shifted over time, are also available while learning new tasks. we propose a parallel continual learning method by assigning subnetworks to each task, and simultaneously training only the assigned subnetworks on their corresponding tasks. in doing so, some parts of the network will be shared across multiple tasks. this is unlike the existing literature in continual learning which aims at learning incoming tasks sequentially, with the assumption that the data for the previous tasks have a fixed distribution. our proposed method offers promises in: (1) transparency in the network and in the relationship across tasks by enabling examination of the learned representations by independent and shared subnetworks, (2) representation generalizability through sharing and training subnetworks on multiple tasks simultaneously. our analysis shows that compared to many competing approaches such as continual learning, neural architecture search, and multi-task learning, parallel continual learning is capable of learning more generalizable representations. also, (3) parallel continual learning overcomes the common issue of catastrophic forgetting in continual learning algorithms. this is the first effort to train a neural network on multiple tasks and input domains simultaneously in a continual learning scenario. our code is available at https://github.com/yoursanonym/part.",AB_0540
"radiation effects analysis of instruments operative in harsh radiation environment is crucial for performance and functionality of electronic devices and components. engineering design of instruments is usually carried out in computer aided design (cad) engineering software. geant4-based monte carlo codes are extensively used for particle transport simulation and analysis. however, geant4 is not prepared to accept cad standard for the exchange of product data (step) format. mradsim-converter is a new software for step to geometry description markup language (gdml) format conversion, readable by geant4-based monte carlo codes. its validation with two different converters confirms its higher speed for importing cad geometries with arbitrary size and complexity having a user-friendly interface for modifying volumes properties.program summaryprogram title: mradsim-convertercpc library link to program files: https://doi .org /10 .17632 /8kzsyn79gh .1licensing provisions: lgplprogramming language: c++supplementary material: cmake, qt5, opencascadenature of problem: geant4 toolkit simulation code developed at cern is widely used for high energy physics community as well as health physics and radioprotections communities. however, geant4 is not prepared to receive the cad formats file, and the geometry implementation in geant4 is a time-consuming task as it requires the creation of each volume using specific classes.solution method: mradsim-converter is a software designed to perform a conversion between the files written in step format, prepared by cad software, into the gdml format, accepted as input in geant4. all existent volumes in the input step file are visualized in the graphical user interface (gui), where it is possible to interactively assign the material to the components and modify each volume's properties, such as material, color, and name.additional comments including restrictions and unusual features: mradsim-converter is tested under ubuntu, versions 18 and 20. all the libraries necessary to run the program are provided in the package hence no additional library or software is required to be installed. under macos and windows, mradsim-converter is running through virtual machine (vm) with above-mentioned ubuntu versions.(c) 2023 elsevier b.v. all rights reserved.",AB_0540
"microalgae produce and secrete large quantities of polysaccharides into the culture medium that is discarded when biomass is separated. the main objective of this study was to add value to the exhausted culture medium (ecm) of porphyridium cruentum through the recovery of extracellular polysaccharides (eps) by different extracting and purifying methods for the future biotechnological applications. the ecm was submitted to pre-cipitation with cold absolute ethanol, using ultrasound, dialysis, and trichloroacetic acid (tca) as purification methods. the purification provided a lower yield, with higher car-bohydrate content. the tca purified sample presented up to twice as much total carbo-hydrates as the non-purified samples, mainly composed of xylose, galactose, and glucose. scanning electron microscopy (sem) images showed the smooth structure of p. cruentum eps, in which tca post-treatment and dialysis allowed obtaining larger and purest par-ticles, being a good candidate for film making. microalgal polymeric by-products are a sustainable source to recover valuable compounds, and the purification treatment proved to be an important step to valorize this material.(c) 2023 the authors. published by elsevier ltd on behalf of institution of chemical engineers. this is an open access article under the cc by-nc-nd license (http://creati-vecommons.org/licenses/by-nc-nd/4.0/).",AB_0540
"single-cell rna sequencing (scrna-seq) has provided unprecedented opportunities for exploring gene expres-sion and thus uncovering regulatory relationships between genes at the single-cell level. however, scrna-seq relies on isolating cells from tissues. therefore, the spatial context of the regulatory processes is lost. a recent technological innovation, spatial transcriptomics, allows for the measurement of gene expression while preser-ving spatial information. an initial step in the spatial transcriptomic analysis is to identify the cell type, which requires a careful selection of cell-specific marker genes. for this purpose, currently, scrna-seq data is used to select a limited number of marker genes from among all genes that distinguish cell types from each other. this study proposes scmags (single-cell marker gene selection), a novel method for marker gene selection from scrna-seq data for spatial transcriptomics studies. scmags uses a filtering step in which the candidate genes are identified before the marker gene selection step. for the selection of marker genes, cluster validity indices, the silhouette index, or the calinski-harabasz index (for large datasets) are utilized. experimental results showed that, in comparison to the existing methods, scmags is scalable, fast, and accurate. even for large datasets with millions of cells, scmags could find the required number of marker genes in a reasonable amount of time with fewer memory requirements. scmags is made freely available at https://github.com/doganlab/scmags and can be downloaded from the python package directory (pypi) software repository with the command pip install scmags.",AB_0540
