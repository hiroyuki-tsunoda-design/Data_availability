AB,NO
"pose-guided person image generation that aims to transfer the pose of a given person to a target pose has recently received lots of research attention. due to the spatial misalignment and occlusions of different local body parts by pose variations, this task is still challenging especially in maintaining high-fidelity textures and body structures in generated images. besides, most works also suffer from the limited number of texture styles in the given person datasets, restricting the diversity of generated persons' appearances. to solve these problems, we design a kernel-based texture-fusion joint refinement network (tfjr-net) to jointly refine the structure and texture information of generated images. first, we leverage a bone-map representation to guide the generation of human parsing maps, which has more structure priors and richer context information than traditional key-point maps, thus reduce the uncertainty of generated body structures. next, a texture-kernel injection normalization module (tkin) is proposed to inject the per-region texture-kernel into the corresponding semantic region from the human parsing map, which decouples the texture and shape information, and also preserves fine-grained features for complex textures. furthermore, we are the first to introduce external texture patterns outside of the dataset in human semantic regions such as the upper clothes. we fuse the two texture domains in a shared texture space through our designed texture-fusion tkin modules. extensive experiments are conducted on the deepfashion dataset, with the dtd dataset as an external texture source. the experimental results demonstrate the superiority of our proposed method in generating persons of better textures and structures than state-of-the-art works, and also show the generalization ability of our proposed method to absorb diversified external textures for generating person images. the source codes are available at https://github.com/pilgrim00/tkin.",AB_0270
"deep neural network models significantly outperform classical algorithms in the hyperspectral image (hsi) classification task. these deep models improve generalization but incur significant computational demands. this article endeavors to alleviate the computational distress in a depthwise manner through the use of morphological operations. we propose the adaptive morphology filter (amf) to effectively extract spatial features like the conventional depthwise convolution layer. furthermore, we reparameterize amf into its equivalent form, i.e., a traditional binary morphology filter, which drastically reduces the number of parameters in the inference phase. finally, we stack multiple amfs to achieve a large receptive field and construct a lightweight amnet for classifying hsis. it is noteworthy that we prove the deep stack of depthwise amfs to be equivalent to structural element decomposition. we test our model on five benchmark datasets. experiments show that our approach outperforms state-of-the-art methods with fewer parameters (approximate to 10k ). the codes will be publicly available at https://github.com/zhu-xlab/adaptive-morphology-filter.",AB_0270
"hyperspectral image change detection (hsi-cd) is a technique that detects changes in land cover occurring in a specific area within a closed time. at present, most existing methods for hsi-cd employ exceedingly intricate network architectures, leading to a high model complexity that hampers the achievement of a favorable tradeoff between change detection (cd) accuracy and timeliness. furthermore, existing methods often confine the feature extraction process to a single scale rather than multiple diverse scales. however, employing a multiscale approach for feature extraction allows for capturing fine-grained features encompassing more intricate details, as well as coarse-grained features that aggregate local information over a larger range. on the other hand, most existing methods overemphasize the complexity of the feature extraction process and underestimate the importance of the conversion process from bitemporal features to valuable change features. to this end, a gated transmitting-based multiscale siamese network (gtmsiam) is proposed, which mainly contains the following two portions: 1) dual branches with the siamese structure, which capture spatial features of the hsis at multiple scales while preserving rich spectral information. moreover, the siamese design effectively reduces the network parameters, thereby alleviating the computational complexity of the model and 2) gated change information transmitting module (gtm), which utilizes gated neural units to transform bitemporal image features into land cover change information, while progressively transmitting change information at different scales. this enables the network to leverage diverse scale change information for comprehensive discrimination of land object changes. experimental results on three publicly available datasets demonstrate the superior performance of the proposed gtmsiam. simultaneously, the complexity analysis experiment proves that the gtmsiam can consider both detection performance and timeliness. the source code of this letter will be released at https://github.com/zkylnnu/gtmsiam.",AB_0270
"few-shot semantic segmentation (fss) has gained significant attention due to its ability to segment novel objects using only a limited number of labeled samples, thereby addressing the problem of overfitting caused by a lack of training data. although this technique is widely studied in the field of computer vision, there are few methods for remote-sensing images. prevalent fss methods can achieve remarkable results for natural images, but they are difficult to apply to remote-sensing image processing because existing methods rarely take into consideration the large-scale and resolution differences in remote-sensing images. consequently, it is hard for them to obtain correct semantic guidance from a few annotated remote-sensing images. to tackle these problems, this article proposes the pyramid correlation fusion network (pcfnet) to promote the ability to mine helpful information by calculating multiscale pixel-wise semantic correspondence. particularly, the dual-distance correlation (ddc) module is designed to simultaneously compute the cosine similarity and euclidean distance between query features and support features, producing adequate guidance information to determine the category of each pixel. moreover, to improve segmentation accuracy for small objects, the scale-aware cross-entropy loss (saceloss) is introduced to dynamically assign loss weights according to the actual sizes of objects. this enables smaller objects to be assigned larger weight values and thus receive more attention during training. comprehensive experiments on both the isaid- 5(i) and dlrsd- 5(i) datasets demonstrate that our method outperforms state-of-the-art fss methods. our code is available at https://github.com/tinyaway/pcfnet.",AB_0270
"domain adaptive object detection (daod) transfers an object detector from the labeled source domain to a novel unlabelled target domain. recent advances bridge the domain gap by aligning category-agnostic feature distribution and minimizing the domain discrepancy for adapting semantic distribution. though great success, these methods model domain discrepancy with prototypes within a batch, yielding a biased estimation of domain-level statistics. moreover, the category-agnostic alignment leads to the disagreement of the cross-domain semantic distribution with inevitable classification errors. to address these two issues, we propose an enhanced semantic conditioned adaptation (scan++) framework, which leverages unbiased semantics for daod. specifically, in the source domain, we design the conditional kernel to sample pixel of interests (pois), and aggregate pois with a cross-image graph to estimate an unbiased semantic sequence. conditioned on the semantic sequence, we further update the parameter of the conditional kernel in a semantic conditioned manifestation module, and establish a novel conditional graph in the target domain to model unlabeled semantics. after modeling the semantic distribution in both domains, we integrate the conditional kernel into adversarial alignment to achieve semantic-aware adaptation in a conditional kernel guided alignment (cka) module. meanwhile, the semantic sequence guided transport (sst) module is proposed to transfer reliable semantic knowledge to the target domain through solving the cross-domain optimal transport (ot) assignment, achieving unbiased adaptation at the semantic level. comprehensive experiments on four adaptation scenarios demonstrate that scan++ achieves state-of-the-art results. the code is available at https://github.com/cityu-aim-group/scan/tree/scan++.",AB_0270
"recently, spatiotemporal fusion technologies have been rapidly developed and widely applied, which generally require one or more pairs of coarse- and fine-resolution images as reference data and a coarse-resolution image at the prediction time to produce a fine-resolution image at the forecast time. consequently, most spatiotemporal fusion methods are phase-based and obtain temporal changing information from the coarse-resolution image pairs. consequently, they usually have rigid constraints on reference data selection using the temporal interval criterion. however, due to the relatively long revisit cycle and cloud contamination, it is difficult to prepare adequate high-quality reference data with little change, especially for large-scale fusion tasks. therefore, we propose a spatial-based fusion method that only requires the coarse images at the prediction time and a fine reference image selected by a spatial-spectral-similarity criterion, named super-resolution-based spatial fusion with the generative adversarial network (srsf-gan). srsf-gan uses a super-resolution (sr) module merely on the coarse images at the prediction time and then performs a multiscale fusion with the reference fine image. moreover, the spatial attention mechanism is adopted to achieve dynamic weight tuning, i.e., assigning more weight to the sr image for changed areas and more weight to the fine reference image for unchanged areas. comparison experiments based on three datasets show that our model can outperform the state-of-the-art methods, and the changes with different spatial and spectral ranges of variation can be recovered. the code will be uploaded to the following website: https://github.com/zhaosir996/srsfgan.",AB_0270
"manipulating visual attributes of an image through a natural language description, known as text-to-image attributes manipulation (t2am), is a challenging task. however, existing approaches tend to search the whole image to manipulate the target instance indicated by a description, thus they often fail to locate and manipulate the accurate text-relevant regions, and even disturb the text-irrelevant contents, e.g. texture and background. meanwhile, the model efficiency needs to be improved. to tackle the above issues, we introduce a novel yet simple gan-based approach, namely structuring image for manipulating (simgan), to narrow down the optimization areas from external to internal. it consists of two major components: 1) external structuring (exst), a pretrained segmentation network, for recognizing and separating the target instances and background from an image; and 2) internal structuring (inst) for seeking out and editing the text-relevant attributes of the target instances based on the given description and masked hierarchical image representations from exst. specifically, the inst structures target instances from outline to detail by firstly drawing the sketch and colors underpainting of instances with an outline-oriented structuring (oust), and then enhancing the text-relevant attributes and elaborating on details with a detail-oriented structuring (dest). extensive experiments on benchmark datasets demonstrate that our framework significantly outperforms state-of-the-art both quantitatively and qualitatively. compared with the state-of-the-art method manigan, our approach reduces the training time by 88%, while the inferring time is three times faster. in addition, our approach is easily extended to solve the instance-level image-to-image translation problem, and the results exhibit the versatility and effectiveness of our approach. this code is released in https://github.com/qikizh/simgan.",AB_0270
"online relevance feedback (rf) is widely utilized in instance search (ins) tasks to further refine imperfect ranking results, but it often has low interaction efficiency. the active learning (al) technique addresses this problem by selecting valuable feedback candidates. however, mainstream al methods require an initial labeled set for a cold start and are often computationally complex to solve. therefore, they cannot fully satisfy the requirements for online rf in interactive ins tasks. to address this issue, we propose a confidence-aware active feedback method (caaf) that is specifically designed for online rf in interactive ins tasks. inspired by the explicit difficulty modeling scheme in self-paced learning, caaf utilizes a pairwise manifold ranking loss to evaluate the ranking confidence of each unlabeled sample. the ranking confidence improves not only the interaction efficiency by indicating valuable feedback candidates but also the ranking quality by modulating the diffusion weights in manifold ranking. in addition, we design two acceleration strategies, an approximate optimization scheme and a top-$k$ search scheme, to reduce the computational complexity of caaf. extensive experiments on both image ins tasks and video ins tasks searching for buildings, landscapes, persons, and human behaviors demonstrate the effectiveness of the proposed method. notably, in the real-world, large-scale video ins task of nist trecvid 2021, caaf uses 25% fewer feedback samples to achieve a performance that is nearly equivalent to the champion solution. moreover, with the same number of feedback samples, caaf's map is 51.9%, significantly surpassing the champion solution by 5.9%. code is available at https://github.com/nercms-mmap/caaf.",AB_0270
"noise suppression for seismic data can meliorate the quality of many subsequent geophysical tasks. in this work, we propose a novel self-supervised learning method, the deep nonlocal regularizer (dnlr), for 3-d seismic denoising. our dnlr fully exploits the nonlocal self-similarity (nss) of seismic data under a self-supervised learning framework for noise attenuation. it can be flexibly combined with different hand-crafted regularizers, e.g., total variation (tv), nuclear norm (nn), and correlated tv (ctv), by performing the regularizer on nonlocal self-similar patches, which more effectively characterizes the intrinsic structures underlying seismic data. our dnlr can be easily plugged into existing self-supervised denoising methods, e.g., deep image prior (dip) and self2self (s2s), and consistently improve their performance. to make the optimization model tractable, an algorithm based on the alternating direction multiplier method (admm) is introduced to solve the dnlr-based seismic denoising problem. extensive seismic denoising experiments on synthetic and field data validate the superior performances of our dnlr as compared with state-of-the-art model-based and deep learning seismic denoising methods. code is available at https://github.com/xuzitai/dnlr.",AB_0270
"infrared dim and small target detection is one of the crucial technologies in the military field, but it faces various challenges such as weak features and small target scales. to overcome these challenges, this article proposes ir-transdet, which integrates the benefits of the convolutional neural network (cnn) and the transformer, to properly extract global semantic information and features of small targets. first, the efficient feature extraction module (efem) is designed, which uses depthwise convolution and pointwise convolution (pw conv) to effectively capture the features of the target. then, an improved residual sim atrous spatial pyramid pooling (aspp) module is proposed based on the image characteristics of infrared dim and small targets. the proposed method focuses on enhancing the edge information of the target. meanwhile, an ir-transformer module is devised, which uses the self-attention mechanism to investigate the relationship between the global image, the target, and neighboring pixels. finally, experiments were conducted on four open datasets, and the results indicate that ir-transdet achieves state-of-the-art performance in infrared dim and small target detection. to achieve a comparative evaluation of the existing infrared dim and small target detection methods, this study constructed the istd-benchmark tool, which is available at https://linaom1214.github.io/istd-benchmark.",AB_0270
