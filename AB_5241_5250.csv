AB,NO
"human body-pose estimation is a complex problem in computer vision. recent research interests have been widened specifically on the sports, yoga, and dance (syd) postures for maintaining health conditions. the syd pose categories are regarded as a fine-grained image classification (fgic) task due to the complex movement of body parts. deep convolutional neural networks (cnns) have attained significantly improved performance in solving various human body-pose estimation problems. though decent progress has been achieved in yoga postures recognition using deep-learning techniques, fine-grained sports and dance recognition necessitates ample research attention. however, no benchmark public image dataset with sufficient interclass and intraclass variations is available yet to address sports and dance postures classification. to solve this limitation, we have proposed two image datasets, one for 102 sport categories and another for 12 dance styles. two public datasets, yoga-82 that contains 82 classes and yoga-107 that represents 107 classes, are collected for yoga postures. these four syd datasets are experimented with the proposed deep model, syd-net, which integrates a patch-based attention (pba) mechanism on top of standard backbone cnns. the pba module leverages the self-attention mechanism that learns contextual information from a set of uniform and multiscale patches and emphasizes discriminative features to understand the semantic correlation among patches. moreover, random erasing data augmentation is applied to improve performance. the proposed syd-net has achieved state-of-the-art accuracy on yoga-82 using five base cnns. syd-net's accuracy on other datasets is remarkable, implying its efficiency. our sports-102 and dance-12 datasets are publicly available at https://sites.google.com/view/syd-net/home.",AB_0525
"video summarization aims to generate a compact summary of the original video for efficient video browsing. to provide video summaries which are consistent with the human perception and contain important content, supervised learning-based video summarization methods are proposed. these methods aim to learn important content based on continuous frame information of human-created summaries. however, simultaneously considering both of inter-frame correlations among non-adjacent frames and intra-frame attention which attracts the humans for frame importance representations are rarely discussed in recent methods. to address these issues, we propose a novel transformer-based method named spatiotemporal vision transformer (stvt) for video summarization. the stvt is composed of three dominant components including the embedded sequence module, temporal inter-frame attention (tia) encoder, and spatial intra-frame attention (sia) encoder. the embedded sequence module generates the embedded sequence by fusing the frame embedding, index embedding and segment class embedding to represent the frames. the temporal inter-frame correlations among non-adjacent frames are learned by the tia encoder with the multi-head self-attention scheme. then, the spatial intra-frame attention of each frame is learned by the sia encoder. finally, a multi-frame loss is computed to drive the learning of the network in an end-to-end trainable manner. by simultaneously using both inter-frame and intra-frame information, our method outperforms state-of-the-art methods in both of the summe and tvsum datasets. the source code of the spatiotemporal vision transformer will be available at https://github.com/nchucvml/stvt.",AB_0525
"learning pyramidal feature representations is important for many dense prediction tasks (e.g., object detection, semantic segmentation) that demand multi-scale visual understanding. feature pyramid network (fpn) is a well-known architecture for multi-scale feature learning, however, intrinsic weaknesses in feature extraction and fusion impede the production of informative features. this work addresses the weaknesses of fpn through a novel tripartite feature enhanced pyramid network (tfpn), with three distinct and effective designs. first, we develop a feature reference module with lateral connections to adaptively extract bottom-up features with richer details for feature pyramid construction. second, we design a feature calibration module between adjacent layers that calibrates the upsampled features to be spatially aligned, allowing for feature fusion with accurate correspondences. third, we introduce a feature feedback module in fpn, which creates a communication channel from the feature pyramid back to the bottom-up backbone and doubles the encoding capacity, enabling the entire architecture to generate incrementally more powerful representations. the tfpn is extensively evaluated over four popular dense prediction tasks, i.e., object detection, instance segmentation, panoptic segmentation, and semantic segmentation. the results demonstrate that tfpn consistently and significantly outperforms the vanilla fpn. our code is available at https://github.com/jamesliang819.",AB_0525
"one-class classification aims to learn one-class models from only in-class training samples. because of lacking out-of-class samples during training, most conventional deep learning based methods suffer from the feature collapse problem. in contrast, contrastive learning based methods can learn features from only in-class samples but are hard to be end-to-end trained with one-class models. to address the aforementioned problems, we propose alternating direction method of multipliers based sparse representation network (admm-srnet). admm-srnet contains the heterogeneous contrastive feature (hcf) network and the sparse dictionary (sd) network. the hcf network learns in-class heterogeneous contrastive features by using contrastive learning with heterogeneous augmentations. then, the sd network models the distributions of the in-class training samples by using dictionaries computed based on admm. by coupling the hcf network, sd network and the proposed loss functions, our method can effectively learn discriminative features and one-class models of the in-class training samples in an end-to-end trainable manner. experimental results show that the proposed method outperforms state-of-the-art methods on cifar-10, cifar-100 and imagenet-30 datasets under one-class classification settings. code is available at https://github.com/nchucvml/admm-srnet.",AB_0525
"modern privacy regulations grant citizens the right to be forgotten by products, services and companies. in case of machine learning (ml) applications, this necessitates deletion of data not only from storage archives but also from ml models. due to an increasing need for regulatory compliance required for ml applications, machine unlearning is becoming an emerging research problem. the right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ml model. practical considerations preclude retraining of the model from scratch after discarding the deleted data. the few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. however, strict regulatory compliance requires time-bound deletion of data. thus, in many cases, no data related to the training process or training samples may be accessible even for the unlearning purpose. we therefore ask the question: is it possible to achieve unlearning with zero training samples? in this paper, we introduce the novel problem of zero-shot machine unlearning that caters for the extreme but practical scenario where zero original data samples are available for use. we then propose two novel solutions for zero-shot machine unlearning based on (a) error minimizing-maximizing noise and (b) gated knowledge transfer. these methods remove the information of the forget data from the model while maintaining the model efficacy on the retain data. the zero-shot approach offers good protection against the model inversion attacks and membership inference attacks. we introduce a new evaluation metric, anamnesis index (ain) to effectively measure the quality of the unlearning method. the experiments show promising results for unlearning in deep learning models on benchmark vision data-sets. the source code is available here: https://github.com/ayu987/zero-shot-unlearning",AB_0525
"in recent years, handwritten numeral classification has achieved remarkable attention in the field of computer vision. handwritten numbers are difficult to recognize due to the different writing styles of individuals. in a multilingual country like india, negligible research attempts have been carried out for handwritten gujarati numerals recognition using deep learning techniques compared to the other regional scripts. the gujarati digit dataset is not available publicly and deep learning requires a large amount of labeled data for the training of the models. if the number of annotated data is not sufficient enough to train convolutional neural networks (cnn) from the scratch, transfer learning can be applied. however, the issue arises by using transfer learning is that how deep to fine-tune the pre-trained convolutional neural network while training the target model. in this paper, we addressed these problems using three deep transfer learning scenarios to classify handwritten gujarati numerals from the images of zero to nine. we presented transfer learning scenarios using ten pre-trained cnn architectures including lenet, vgg16, inceptionv3, resnet50, xception, resnet101, mobilenet, mobilenetv2, densenet169 and efficientnetv2s to find the best performing model by freezing and fine-tuning the weight parameters. we implemented the pre-trained models using a self-created handwritten gujarati digit dataset with 8000 images of zero to nine digits with data augmentation. exhaustive experiments are performed using various performance evaluation matrices. efficientnetv2s model showed promising results among all the models including three transfer learning scenarios and achieved 98.39% training accuracy, 97.92% testing accuracy, 97.69% f1-score, and 97.15% auc. our handwritten gujarati digit dataset is available on https://github.com/parth-goel/gujarati-handwritten-digit-dataset/.",AB_0525
"in this study, we propose a new deep convolutional generative adversarial kinematics network (dcgakn) to establish inverse kinematics of self-assembly robotic arm. we design that the robot system uses a depth sensor detecting an object by you only look once v4 (yolov4) algorithm, and then, our proposed dcgakn is with generator and discriminator of adversarial evolution training inverse kinematics model for controlling self-assembly robotic arm to solve the limited solution space to be more adaptive in the dynamic environment. the following are advancements of our proposed method: 1) generator neural network is trained by few-shot training data to control the self-assembly robotic arm to achieve high-accuracy position; 2) generator is evaluated with discriminator not only depending on training data but also on adaptive evolutionary; 3) the self-assembly robotic arm is like humanoid arm not traditional robotic arm structure and it is easy for self-assembly model to build inverse kinematics without computing inverse kinematics matrix; 4) the object is detected by the depth information based on yolov4; and 5) through generator evolutionary, the activity range of robotic armis not limited with training data range. the proposed dcgakn is compared with convolutional neural network (cnn) and deep neural network (dnn) that the accuracy rate and distance error achieve 87% and 1.26 cm, respectively. the source code of this work is at: https://github.com/yizenghsieh/dcgakn.",AB_0525
"due to the lack of a large annotated corpus, many resource-poor indian languages struggle to reap the benefits of recent deep feature representations in natural language processing (nlp). moreover, adopting existing language models trained on large english corpora for indian languages is often limited by data availability, rich morphological variation, syntax, and semantic differences. in this paper, we explore the traditional to recent efficient representations to overcome the challenges of a low resource language, telugu. in particular, our main objective is to mitigate the low-resource problem for telugu. overall, we present several contributions to a resource-poor language viz. telugu. (i) a large annotated data (35,142 sentences in each task) for multiple nlp tasks such as sentiment analysis, emotion identification, hate-speech detection, and sarcasm detection, (ii) we create different lexicons for sentiment, emotion, and hate-speech for improving the efficiency of the models, (iii) pretrained word and sentence embeddings, and (iv) different pretrained language models for telugu such as elmo-te, bert-te, roberta-te, albert-te, and distilbert-te on a large telugu corpus consisting of 8,015,588 sentences (1,637,408 sentences from teluguwikipedia and 6,378,180 sentences crawled from different telugu websites). further, we show that these representations significantly improve the performance of four nlp tasks and present the benchmark results for telugu. we argue that our pretrained embeddings are competitive or better than the existing multilingual pretrained models: mbert, xlm-r, and indicbert. lastly, the fine-tuning of pretrained models show higher performance than linear probing results on four nlp tasks with the following f1-scores: sentiment (68.72), emotion (58.04), hate-speech (64.27), and sarcasm (77.93). we also experiment on publicly available telugu datasets (named entity recognition, article genre classification, and sentiment analysis) and find that our telugu pretrained language models (bert-te and roberta-te) outperform the state-of-the-art system except for the sentiment task. we open-source our corpus, four different datasets, lexicons, embeddings, and code https://github.com/cha14ran/dream- t. the pretrained transformer models for telugu are available at https://huggingface.co/ltrctelugu.",AB_0525
"background: underwater images, in general, suffer from low contrast and high color distortions due to the non-uniform attenuation of the light as it propagates through the water. in addition, the degree of attenuation varies with the wavelength, resulting in the asymmetric traversing of colors. despite the prolific works for underwater image restoration (uir) using deep learning, the above asymmetricity has not been addressed in the respective network engineering. contributions: as the first novelty, this article shows that attributing the right receptive field size (context) based on the traversing range of the color channel may lead to a substantial performance gain for the task of uir. further, it is important to suppress the irrelevant multi-contextual features and increase the representational power of the model. therefore, as a second novelty, we have incorporated an attentive skip mechanism to adaptively refine the learned multi-contextual features. the proposed framework, called deep wavenet, is optimized using the traditional pixel-wise and feature-based cost functions. an extensive set of experiments have been carried out to show the efficacy of the proposed scheme over existing best-published literature on benchmark datasets. more importantly, we have demonstrated a comprehensive validation of enhanced images across various high-level vision tasks, e.g., underwater image semantic segmentation and diver's 2d pose estimation. a sample video to exhibit our real-world performance is available at https://tinyurl.com/yzcrup9n. also, we have open-sourced our framework at https://github.com/pksvision/deep-wavenet-underwaterimage- restoration.",AB_0525
"this article studies group-wise point set registration and makes the following contributions: fuzzygreg , which is a new fuzzy cluster-based method to register multiple point sets jointly, and fuzzyqa , which is the associated quality assessment to check registration accuracy automatically. given a group of point sets, fuzzygreg creates a model of fuzzy clusters and equally treats all the point sets as the elements of the fuzzy clusters. then, the group-wise registration is turned into a fuzzy clustering problem. to resolve this problem, fuzzygreg applies a fuzzy clustering algorithm to identify the parameters of the fuzzy clusters while jointly transforming all the point sets to achieve an alignment. next, based on the identified fuzzy clusters, fuzzyqa calculates the spatial properties of the transformed point sets and then checks the alignment accuracy by comparing the similarity degrees of the spatial properties of the point sets. when a local misalignment is detected, a local re-alignment is performed to improve accuracy. the proposed method is cost-efficient and convenient to be implemented. in addition, it provides reliable quality assessments in the absence of ground truth and user intervention. in the experiments, different point sets are used to test the proposed method and make comparisons with state-of-the-art registration techniques. the experimental results demonstrate the effectiveness of our method. the code is available at https://gitsvn-nt.oru.se/qianfang.liao/fuzzygregwithqa",AB_0525
