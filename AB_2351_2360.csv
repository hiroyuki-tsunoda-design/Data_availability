AB,NO
"in modern online services, it is of growing importance to process web-scale graph data and high-dimensional sparse data together into embeddings for downstream tasks, such as recommendation, advertisement, prediction, and classification. there exist learning methods and systems for either high-dimensional sparse data or graphs, but not both. there is an urgent need in industry to have a system to efficiently process both types of data for higher business value, which however, is challenging. the data in tencent contains billions of samples with sparse features in very high dimensions, and graphs are also with billions of nodes and edges. moreover, learning models often perform expensive operations with high computational costs. it is difficult to store, manage, and retrieve massive sparse data and graph data together, since they exhibit different characteristics. we present embedx, an industrial distributed learning framework from tencent, which is versatile and efficient to support embedding on both graphs and high-dimensional sparse data. embedx consists of distributed server layers for graph and sparse data management, and optimized parameter and graph operators, to efficiently support 4 categories of methods, including deep learning models on high-dimensional sparse data, network embedding methods, graph neural networks, and in-house developed joint learning models on both types of data. extensive experiments on massive tencent data and public data demonstrate the superiority of embedx. for instance, on a tencent dataset with 1.3 billion nodes, 35 billion edges, and 2.8 billion samples with sparse features in 1.6 billion dimension, embedx performs an order of magnitude faster for training and our joint models achieve superior effectiveness. embedx is deployed in tencent. a/b test on real use cases further validates the power of embedx. embedx is implemented in c++ and open-sourced at https://github.com/tencent/embedx.",AB_0236
"concerning the usability and efficiency to manage video data generated from large-scale cameras, we demonstrate dovedb, a declarative and low-latency video database. we devise a more comprehensive video query language called vmql to improve the expressiveness of previous sql-like languages, which are augmented with functionalities for model-oriented management and deployment. we also propose a light-weight ingestion scheme to extract tracklets of all the moving objects and build semantic indexes to facilitate efficient query processing. for user interaction, we construct a simulation environment with 120 cameras deployed in a road network and demonstrate three interesting scenarios. using vmql, users are allowed to 1) train a visual model using sql-like statement and deploy it on dozens of target cameras simultaneously for online inference; 2) submit multi-object tracking (mot) requests on target cameras, store the ingested results and build semantic indexes; and 3) issue an aggregation or top-k query on the ingested cameras and obtain the response within milliseconds. a preliminary video introduction of dovedb is available at https://www.youtube.com/watch?v=n139deyvajk",AB_0236
"we propose octree-based transformers, named octformer, for 3d point cloud learning. octformer can not only serve as a general and effective backbone for 3d point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. the key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. to combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. however, the point number in each window varies greatly, impeding the efficient execution on gpu. observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. andwe also introduce dilated octree attention to expand the receptive field further. our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200... built upon the octree attention, octformer can be easily scaled up and achieves state-of-the-art performances on a series of 3d semantic segmentation and 3d object detection benchmarks, surpassing previous sparse-voxel-based cnns and point cloud transformers in terms of both efficiency and effectiveness. notably, on the challenging scannet200 dataset, octformer outperforms sparse-voxel-based cnns by 7.3 in miou. our code and trained models are available at https:// wang- ps.github.io/ octformer.",AB_0236
"adjusting the photo color to associate with some design elements is an essential way for a graphic design to effectively deliver its message and make it aesthetically pleasing. however, existing tools and previous works face a dilemma between the ease of use and level of expressiveness. to this end, we introduce an interactive language-based approach for photo recoloring, which provides an intuitive system that can assist both experts and novices on graphic design. given a graphic design containing a photo that needs to be recolored, our model can predict the source colors and the target regions, and then recolor the target regions with the source colors based on the given language-based instruction. the multi-granularity of the instruction allows diverse user intentions. the proposed novel task faces several unique challenges, including: 1) color accuracy for recoloring with exactly the same color from the target design element as specified by the user; 2) multi-granularity instructions for parsing instructions correctly to generate a specific result or multiple plausible ones; and 3) locality for recoloring in semantically meaningful local regions to preserve original image semantics. to address these challenges, we propose a model called langrecol with two main components: the language-based source color prediction module and the semantic-palette-based photo recoloring module. we also introduce an approach for generating a synthetic graphic design dataset with instructions to enable model training. we evaluate our model via extensive experiments and user studies. we also discuss several practical applications, showing the effectiveness and practicality of our approach. please find the code and data at https://zhenwwang.github.io/langrecol.",AB_0236
"we present a simple but effective technique to smooth out textures while preserving the prominent structures. our method is built upon a key observation the coarsest level in a gaussian pyramid often naturally eliminates textures and summarizes the main image structures. this inspires our central idea for texture filtering, which is to progressively upsample the very low-resolution coarsest gaussian pyramid level to a full-resolution texture smoothing result with well-preserved structures, under the guidance of each fine-scale gaussian pyramid level and its associated laplacian pyramid level. we show that our approach is effective to separate structure from texture of different scales, local contrasts, and forms, without degrading structures or introducing visual artifacts. we also demonstrate the applicability of our method on various applications including detail enhancement, image abstraction, hdr tone mapping, inverse halftoning, and ldr image enhancement. code is available at https://rewindl.github.io/pyramid_texture_filtering/.",AB_0236
"graph-contrastive learning has aided the development of unsupervised graph representation learning, comparable to supervised models in terms of performance. however, the robustness of the graph contrastive learning model still has a bottleneck problem, most of the current adversarial attacks are supervised, and the acquisition of labels cannot be guaranteed when attacking unsupervised graph contrastive learning models. we propose an unsupervised attack method for graph contrastive learning because the traditional supervised graph adversarial attack method is unsuitable for the attack graph contrastive learning model. it combines the graph inject attack with the poison feature matrix and uses gradients in different contrast views of the poison adjacency matrix. extensive experiments are conducted on various datasets and our method shows notable superiority among relevant methods, even compared to supervised ones. the code is publicly available at https://github. com/lizehaodashuaibi/paper. & copy; 2023 elsevier b.v. all rights reserved.",AB_0236
"autoscanning of an unknown environment is the key to many ar/vr and robotic applications. however, autonomous reconstruction with both high efficiency and quality remains a challenging problem. in this work, we propose a reconstruction-oriented autoscanning approach, called scanbot, which utilizes hierarchical deep reinforcement learning techniques for global region-of-interest (roi) planning to improve the scanning efficiency and local next-best-view (nbv) planning to enhance the reconstruction quality. given the partially reconstructed scene, the global policy designates an roi with insufficient exploration or reconstruction. the local policy is then applied to refine the reconstruction quality of objects in this region by planning and scanning a series of nbvs. a novel mixed 2d-3d representation is designed for these policies, where a 2d quality map with tailored quality channels encoding the scanning progress is consumed by the global policy, and a coarse-to-fine 3d volumetric representation that embodies both local environment and object completeness is fed to the local policy. these two policies iterate until the whole scene has been completely explored and scanned. to speed up the learning of complex environmental dynamics and enhance the agent's memory for spatial-temporal inference, we further introduce two novel auxiliary learning tasks to guide the training of our global policy. thorough evaluations and comparisons are carried out to show the feasibility of our proposed approach and its advantages over previous methods. code and data are available at https://github.com/hezhicao/scanbot.",AB_0236
"we present a neural rendering-based method called nero for reconstructing the geometry and the brdf of reflective objects from multiview images captured in an unknown environment. multiview reconstruction of reflective objects is extremely challenging because specular reflections are view-dependent and thus violate the multiview consistency, which is the cornerstone for most multiview reconstruction methods. recent neural rendering techniques can model the interaction between environment lights and the object surfaces to fit the view-dependent reflections, thus making it possible to reconstruct reflective objects from multiview images. however, accurately modeling environment lights in the neural rendering is intractable, especially when the geometry is unknown. most existing neural rendering methods, which can model environment lights, only consider direct lights and rely on object masks to reconstruct objects with weak specular reflections. therefore, these methods fail to reconstruct reflective objects, especially when the object mask is not available and the object is illuminated by indirect lights. we propose a two-step approach to tackle this problem. first, by applying the split-sum approximation and the integrated directional encoding to approximate the shading effects of both direct and indirect lights, we are able to accurately reconstruct the geometry of reflective objects without any object masks. then, with the object geometry fixed, we use more accurate sampling to recover the environment lights and the brdf of the object. extensive experiments demonstrate that our method is capable of accurately reconstructing the geometry and the brdf of reflective objects from only posed rgb images without knowing the environment lights and the object masks. codes and datasets are available at https://github.com/liuyuan-pal/nero.",AB_0236
"we present a novel vectorized indoor modeling approach that converts point clouds into building information models (bim) with concise and semantically segmented polygonal meshes. existing methods detect planar shapes and connect them to complete the scene. some focus on floor plan reconstruction as a simplified problem to better analyze connectivity between planes of floors and walls. however, the connectivity analysis is still challenging and ill-posed with incomplete point clouds as input. we propose arrangementnet to estimate scene arrangements from an incomplete point cloud, which we can easily convert into a bim model. arrangementnet is a novel graph neural network that consumes noisy over-partitioned initial arrangements extracted through non-learning techniques and outputs high-quality scene arrangement. the core of arrangementnet is an extended graph convolution that leverages co-linear and co-face relationships in the arrangement and improves the quality of prediction in complex scenes. we apply arrangementnet to improve floor plan and ceiling arrangements and enrich them with semantic objects as scene arrangements for scene generation. our approach faithfully models challenging scenes obtained from laser scans or multiview stereo and shows significant improvement in bim model reconstruction compared to the state-of-the-art. our code is available at https://github.com/zssjh/arrangementnet.",AB_0236
"human and environment sensing are two important topics in computer vision and graphics. human motion is often captured by inertial sensors, while the environment is mostly reconstructed using cameras. we integrate the two techniques together in egolocate, a system that simultaneously performs human motion capture (mocap), localization, and mapping in real time from sparse body-mounted sensors, including 6 inertial measurement units (imus) and a monocular phone camera. on one hand, inertial mocap suffers from large translation drift due to the lack of the global positioning signal. egolocate leverages image-based simultaneous localization and mapping (slam) techniques to locate the human in the reconstructed scene. on the other hand, slam often fails when the visual feature is poor. egolocate involves inertial mocap to provide a strong prior for the camera motion. experiments show that localization, a key challenge for both two fields, is largely improved by our technique, compared with the state of the art of the two fields. our codes are available for research at https://xinyu-yi.github.io/egolocate/.",AB_0236
