AB,NO
"the summary: the united resisdue (unres) package for coarse-grained simulations, which has recently been optimized to treat large protein systems, has been implemented on graphical processor units (gpus). an over 100-time speed-up of the gpu code (run on an nvidia a100) with respect to the sequential code and an 8.5 speed-up with respect to the parallel open multi-processing (openmp) code (run on 32 cores of 2 amd epyc 7313 central processor units (cpus)) has been achieved for large proteins (with size over 10 000 residues). due to the averaging over the fine-grain degrees of freedom, 1 time unit of unres simulations is equivalent to about 1000 time units of laboratory time; therefore, millisecond time scale of large protein systems can be reached with the unres-gpu code. availability and implementation: the source code of unres-gpu along with the benchmarks used for tests is available at https://projects.task.gda.pl/eurohpcpl-public/unres.",AB_0535
"underwater pressure sensors were deployed near-continuously at various locations of the nearshore (8-23m depth) hornsund fjord, svalbard, between july 2013 and february 2021. raw pressure measurements at 1 hz were used to derive mean water levels, wave spectra and bulk wave parameters for 1024 s bursts at hourly intervals. the procedure included subtracting atmospheric pressure, depth calculation, fast fourier transform, correction for the decrease of the wave orbital motion with depth and adding a high-frequency tail. the dataset adds to the sparse in situ measurements of wind waves and water levels in the arctic, and it can be used, for example, for analysing seasonal wind wave conditions and inter-annual trends and calibrating/validating wave models. the dataset is stored in the pangaea repository (https://doi.org/10.1594/pangaea.954020; swirad et al., 2023).",AB_0535
"introduction acute angle between dome and ala causes alar concavity/pinch deformity. breathing problems may accompany pinching. here, pinch deformities were classified according to their severity and treatment modalities discussed. materials and methods rhinoplasty patients with pinch deformities were included in study. pinching without external nasal valve blockage (envb) was classified mild, pinching with envb was classified moderate, and extreme pinching and envb were classified severe deformity. in mild deformity, cephalic resection of ala was performed or cephalic resection was combined with onlay graft over ala. in moderate deformity, cephalic part was bent and sutured over lower ala. in severe deformity, cephalic part was bent, and lateral strut graft was inserted between lower and cephalic ala. in pinch deformities combined with hypertrophic lower lateral cartilage (llc), medial crural overlay preceded above-mentioned treatment modalities. results thirty-eight patients (22 female, 16 male) with pinch deformities underwent rhinoplasty between january 2017 and december 2022. mean age was 27 years. mean follow-up was 32 months. fifteen patients had mild deformities. cephalic resection was enough in four patients. camouflage grafts were settled over ala in eleven patients. twenty patients had moderate deformities; cephalic ala was bent over lower part and sutured. two patients had severe deformities; lateral strut graft was settled between lower and bent cephalic alar parts. one patient had llc hypertrophy/pinch deformity. llc hypertrophy was corrected by medial crural overlay, and concavity was corrected with cephalic resection. satisfactory shape, better valve passage obtained in all cases. conclusion pinch deformity could be classified according to its severity and appropriate treatment options could be determined for each class. level of evidence iv this journal requires that authors assign a level of evidence to each article. for a full description of these evidence-based medicine ratings, please refer to the table of contents or the online instructions to authors https://www.springer.com/journal/00266.",AB_0535
"phasicflow is an open-source code for dem simulations developed in c++, licensed under gpl v-3.0 [www.github .com /phasicflow /phasicflow]. it is parallelized based on shared-memory model and can be executed on multi-core cpus or gpus. the philosophy of its development is to provide an efficient, easy-to-use, and easy-to-extend tool for a wide range of hardware including cheap to high-end desktop computers. here, the code structure, the most important features, its performance for simulating small to large problems (250,000 to 80,000,000 particles) on various hardware are presented. in all cases, the execution time varied linearly with number of particles that is ideal for dem. the computation load per particle was almost constant for all problem sizes except for small simulations (higher). for cpu simulations, a good scaling was observed with number of cores, and scaling was closer to ideal state for larger simulations. the performance of phasicflow was compared with similar tools, wherever possible. it was shown that the performance of phasicflow is much higher than the similar tools in cpu execution mode and it is around 10-15% faster than similar tools in gpu execution mode. this new tool broadens the application of computer simulation to larger problems and allows researchers to perform dem simulations on a wider range of hardware.program summaryprogram title: phasicflowcpc library link to program files: https://doi .org /10 .17632 /fs7w7jb8mr.1 developer's repository link: www.github .com /phasicflow /phasicflowlicensing provisions: gnu general public license 3 programming language: c++nature of problem: the code covers the simulation of granular matter in powder, gas-solid and liquid-solid flows. the formulation is based on the soft-sphere discrete element method (dem) which is a powerful tool for simulating solid powder flows. solid particles are considered distinct bodies that may interact with solid surfaces, other particles and fluid around them.this method can be purely used for investigating various flow problems or can be coupled with proper fluid flow simulation code to be used in multiphase flow systems. the main goal here is to provide a powerful tool for researchers to perform large-scale powder and multiphase flows on cheaper and more accessible hardware.solution method: the solution method is based on the high-order explicit integration of equations of motions for discrete particles over time to obtain their linear and rotational position and velocity. triangulation is used for representing solid walls and various motion models are implemented, which cover a wide range of processing units in the real-life application. particle-particle and particle-wall interactions are modeled using linear and non-linear models accompanied with friction models. the code is designed to be executed fast, to efficiently use computational resources, and to be easily understood, altered and extended for specific uses. it is multi-architecture and can be executed in parallel on both cpu and gpu, depending on the available hardware. it uses kokkos as the backend code for managing memory resources and dispatching parallel work on computational resources. accordingly, a single code can be compiled for various hardware types. the parallelization is performed on high level and with negligible overhead costs, so the developers of the code are not facing hardware-specific programming models when parallelizing the computations workloads. additional comments including restrictions and unusual features: the simulation setup in the code is based on human readable text-based files that are supplied by the user. to use the code, no knowledge of programming is required. the github repository contains a wiki page that provides a full set of tutorials to show researchers how to simulate various granular flows. a step-by-step procedure for building the code is also provided in the repository that has been tested on various version of ubuntu, including versions 18.04 lts, 20.04 lts and 22.04 lts.& copy; 2023 elsevier b.v. all rights reserved.",AB_0535
"recent improvements in deep learning models and their practical applications have raised concerns about the robustness of these models against adversarial examples. adversarial training (at) has been shown effective in reaching a robust model against the attack that is used during training. however, it usually fails against other attacks, i.e., the model over fits to the training attack scheme. in this paper, we propose a new method for generating adversarial perturbations during training that mitigates the mentioned issue. more specifically, we minimize the perturbation l(p) norm while maximizing the classification loss in the lagrangian form to craft adversarial examples. we argue that crafting adversarial examples based on this scheme results in enhanced attack generalization in the learned model. we compare our final model robust accuracy with the closely related state-of-the-art at methods against attacks that were not used during training. this comparison demonstrates that our average robust accuracy against unseen attacks is 5.9% higher in the cifar-10 dataset and 3.2% higher in the imagenet-100 dataset than corresponding state-of-the-art methods. we also demonstrate that our attack is faster than other attack schemes that are designed for unseen attack generalization and conclude that the proposed method is feasible for large datasets. our code is available at https://github.com/rohban-lab/lagrangian_unseen.",AB_0535
"background genomics-informed pathogen surveillance strengthens public health decision-making, playing an important role in infectious diseases' prevention and control. a pivotal outcome of genomics surveillance is the identification of pathogen genetic clusters and their characterization in terms of geotemporal spread or linkage to clinical and demographic data. this task often consists of the visual exploration of (large) phylogenetic trees and associated metadata, being time-consuming and difficult to reproduce.results we developed reportree, a flexible bioinformatics pipeline that allows diving into the complexity of pathogen diversity to rapidly identify genetic clusters at any (or all) distance threshold(s) or cluster stability regions and to generate surveillance-oriented reports based on the available metadata, such as timespan, geography, or vaccination/ clinical status. reportree is able to maintain cluster nomenclature in subsequent analyses and to generate a nomenclature code combining cluster information at different hierarchical levels, thus facilitating the active surveillance of clusters of interest. by handling several input formats and clustering methods, reportree is applicable to multiple pathogens, constituting a flexible resource that can be smoothly deployed in routine surveillance bioinformatics workflows with negligible computational and time costs. this is demonstrated through a comprehensive benchmarking of (i) the cg/wgmlst workflow with large datasets of four foodborne bacterial pathogens and (ii) the alignment based snp workflow with a large dataset of mycobacterium tuberculosis. to further validate this tool, we reproduced a previous large-scale study on neisseria gonorrhoeae, demonstrating how reportree is able to rapidly identify the main species genogroups and characterize them with key surveillance metadata, such as antibiotic resistance data. by providing examples for sars-cov-2 and the foodborne bacterial pathogen listeria monocytogenes, we show how this tool is currently a useful asset in genomics-informed routine surveillance and outbreak detection of a wide variety of species.conclusions in summary, reportree is a pan-pathogen tool for automated and reproducible identification and characterization of genetic clusters that contributes to a sustainable and efficient public health genomics-informed pathogen surveillance. reportree is implemented in python 3.8 and is freely available at https://github.com/insap athogenomics/reportree.",AB_0535
"existing out-of-distribution (ood) detection literature clearly defines semantic shift as a sign of ood but does not have a consensus over covariate shift. samples experiencing covariate shift but not semantic shift from the in-distribution (id) are either excluded from the test set or treated as ood, which contradicts the primary goal in machine learning-being able to generalize beyond the training distribution. in this paper, we take into account both shift types and introduce full-spectrum ood (f-ood) detection, a more realistic problem setting that considers both detecting semantic shift and being tolerant to covariate shift; and design three benchmarks. these new benchmarks have a more fine-grained categorization of distributions (i.elet@tokeneonedot, training id, covariate-shifted id, near-ood, and far-ood) for the purpose of more comprehensively evaluating the pros and cons of algorithms. to address the f-ood detection problem, we propose sem, a simple featurebased semantics score function. sem is mainly composed of two probability measures: one is based on high-level features containing both semantic and non-semantic information, while the other is based on low-level feature statistics only capturing non-semantic image styles. with a simple combination, the non-semantic part is canceled out, which leaves only semantic information in sem that can better handle f-ood detection. extensive experiments on the three new benchmarks show that sem significantly outperforms current state-of-the-art methods. our code and benchmarks are released in https://github.com/ jingkang50/openood.",AB_0535
"this study proposes a new deep learning-based method that demonstrates high performance in detecting covid-19 disease from cough, breath, and voice signals. this impressive method, named covidcoughnet, consists of a deep feature extraction network (inceptionfirenet) and a prediction network (deepconvnet). the inceptionfirenet ar-chitecture, based on inception and fire modules, was designed to extract important feature maps. the deepconvnet architecture, which is made up of convolutional neural network blocks, was developed to predict the feature vectors obtained from the inceptionfirenet architecture. the coughvid dataset containing cough data and the coswara dataset containing cough, breath, and voice signals were used as the data sets. the pitch-shifting technique was used to data augmentation the signal data, which significantly contributed to improving performance. additionally, chroma features (cf), root mean square energy (rmse), spectral centroid (sc), spectral bandwidth (sb), spectral rolloff (sr), zero crossing rate (zcr), and mel frequency cepstral coefficients (mfcc) feature extraction techniques were used to extract important features from voice signals. experimental studies have shown that using the pitch-shifting technique improved performance by around 3% compared to raw signals. when the proposed model was used with the coughvid dataset (healthy, covid-19, and symptomatic), a high performance of 99.19% accuracy, 0.99 precision, 0.98 recall, 0.98 f1-score, 97.77% specificity, and 98.44% auc was achieved. similarly, when the voice data in the coswara dataset was used, higher performance was achieved compared to the cough and breath studies, with 99.63% accuracy, 100% precision, 0.99 recall, 0.99 f1-score, 99.24% specificity, and 99.24% auc. moreover, when compared with current studies in the literature, the proposed model was observed to exhibit highly successful performance. the codes and details of the experimental studies can be accessed from the relevant github page: (https://github.com/gaffaricelik/covidcoughnet).",AB_0535
"neutrophil extracellular traps (nets), pathogen-ensnaring structures formed by neutrophils by expelling their dna into the environment, are believed to play an important role in immunity and autoimmune diseases. in recent years, a growing attention has been put into developing software tools to quantify nets in fluorescent microscopy images. however, current solutions require large, manually-prepared training data sets, are difficult to use for users without background in computer science, or have limited capabilities. to overcome these problems, we developed trapalyzer, a computer program for automatic quantification of nets. trapalyzer analyzes fluorescent microscopy images of samples double-stained with a cell-permeable and a cell-impermeable dye, such as the popular combination of hoechst 33342 and sytoxtm green. the program is designed with emphasis on software ergonomy and accompanied with step-by-step tutorials to make its use easy and intuitive. the installation and configuration of the software takes less than half an hour for an untrained user. in addition to nets, trapalyzer detects, classifies and counts neutrophils at different stages of net formation, allowing for gaining a greater insight into this process. it is the first tool that makes this possible without large training data sets. at the same time, it attains a precision of classification on par with state-of-the-art machine learning algorithms. as an example application, we show how to use trapalyzer to study net release in a neutrophil-bacteria co-culture. here, after configuration, trapalyzer processed 121 images and detected and classified 16 000 rois in approximately three minutes on a personal computer. the software and usage tutorials are available at https://github.com/czaki/trapalyzer.",AB_0535
"ideally, visual learning algorithms should be generalizable, for dealing with any unseen domain shift when deployed in a new target environment; and data-efficient, for reducing development costs by using as little labels as possible. to this end, we study semi-supervised domain generalization (ssdg), which aims to learn a domain-generalizable model using multi-source, partially-labeled training data. we design two benchmarks that cover state-of-the-art methods developed in two related fields, i.e., domain generalization (dg) and semi-supervised learning (ssl). we find that the dg methods, which by design are unable to handle unlabeled data, perform poorly with limited labels in ssdg; the ssl methods, especially fixmatch, obtain much better results but are still far away from the basic vanilla model trained using full labels. we propose stylematch, a simple approach that extends fixmatch with a couple of new ingredients tailored for ssdg: (1) stochastic modeling for reducing overfitting in scarce labels, and (2) multi-view consistency learning for enhancing domain generalization. despite the concise designs, stylematch achieves significant improvements in ssdg. we hope our approach and the comprehensive benchmarks can pave the way for future research on generalizable and data-efficient learning systems. the source code is released at https://github.com/kaiyangzhou/ssdg-benchmark.",AB_0535
