AB,NO
"semantic segmentation is an essential task in medical imaging research. many powerful deep-learning-based approaches can be employed for this problem, but they are dependent on the availability of an expansive labeled dataset. in this work, we augment such supervised segmentation models to be suitable for learning from unlabeled data. our semi-supervised approach, termed error-correcting mean-teacher, uses an exponential moving average model like the original mean teacher but introduces our new paradigm of error correction. the original segmentation network is augmented to handle this secondary correction task. both tasks build upon the core feature extraction layers of the model. for the correction task, features detected in the input image are fused with features detected in the predicted segmentation and further processed with task-specific decoder layers. the combination of image and segmentation features allows the model to correct present mistakes in the given input pair. the correction task is trained jointly on the labeled data. on unlabeled data, the exponential moving average of the original network corrects the student's prediction. the combined outputs of the students' prediction with the teachers' correction form the basis for the semi-supervised update. we evaluate our method with the 2017 and 2018 robotic scene segmentation data, the isic 2017 and the brats 2020 challenges, a proprietary endoscopic submucosal dissection dataset, cityscapes, and pascal voc 2012. additionally, we analyze the impact of the individual components and examine the behavior when the amount of labeled data varies, with experiments performed on two distinct segmentation architectures. our method shows improvements in terms of the mean intersection over union over the supervised baseline and competing methods. code is available at https://github.com/clonerob/ecmt.",AB_0431
"archaea are a vast and unexplored cellular domain that thrive in a high diversity of environments, having central roles in processes mediating global carbon and nutrient fluxes. for these organisms to balance their metabolism, the appropriate regulation of their gene expression is essential. a key momentum in regulating genes responsible for the life maintenance of archaea is when transcription factor proteins bind to the promoter element. this dna segment is conserved, which enables its exploration by machine learning techniques. here, we trained and tested a support vector machine with 3935 known archaeal promoter sequences. all promoter sequences were coded into dna duplex stability. after, we performed a model interpretation task to map the decision pattern of the classification procedure. we also used a dataset of known-promoter sequences for validation. our results showed that an at rich region around position - 27 upstream (relative to the start tss) is the most conserved in the analyzed organisms. in addition, we were able to identify the bre element (- 33), the ppe (at - 10) and a position at + 3, that provides a more understandable picture of how promoters are organized in all the archaeal organisms. finally, we used the interpreted model to identify potential promoter sequences of 135 unannotated organisms, delivering regulatory regions annotation of archaea in a scale never accomplished before (https://pcyt. unam. mx/gene-.regulation/). we consider that this approach will be useful to understand how gene regulation is achieved in other organisms apart from the already established transcription factor binding sites.",AB_0431
"background: in the global effort to discover biomarkers for cancer prognosis, prediction tools have become essential resources. tcr (t cell receptor) repertoires contain important features that differentiate healthy controls from cancer patients or differentiate outcomes for patients being treated with different drugs. considering, tools that can easily and quickly generate and identify important features out of tcr repertoire data and build accurate classifiers to predict future outcomes are essential. results: this paper introduces gentle (generator of t cell receptor repertoire features for machine learning): an open-source, user-friendly web-application tool that allows tcr repertoire researchers to discover important features; to create classifier models and evaluate them with metrics; and to quickly generate visualizations for data interpretations. we performed a case study with repertoires of tregs (regulatory t cells) and tconvs (conventional t cells) from healthy controls versus patients with breast cancer. we showed that diversity features were able to distinguish between the groups. moreover, the classifiers built with these features could correctly classify samples ('healthy' or 'breast cancer')from the tregs repertoire when trained with the tconvs repertoire, and from the tconvs repertoire when trained with the tregs repertoire. conclusion: the paper walks through installing and using gentle and presents a case study and results to demonstrate the application's utility. gentle is geared towards any researcher working with tcr repertoire data and aims to discover predictive features from these data and build accurate classifiers. gentle is available on https://github.com/dhiego22/gentle and https://share.streamlit.io/dhiego22/gentle/main/gentle.py.",AB_0431
"we present a python package for the efficient generation of special quasi-random structures (sqs) for atomic-scale calculations of disordered systems. both, a monte-carlo approach or a systematic enumeration of structures can be used to carry out optimizations to ensure the best optimal configuration is found for given cell size and composition. we present a measure of randomness based on warren -cowley short-range order parameters allowing for fast analysis of atomic structures. hence, optimal structures are found in a reasonable time for several dozens or even hundreds of atoms. both sqs optimizations and analysis of structures can be carried out via a command-line interface or a python api. additional features, such as optimization towards partial ordering or independent sublattices allow the generation of atomistic models of modern complex materials. moreover, hybrid parallelization, as well as distribution of vacancies, are supported. the output data format is compatible with ase, pymatgen and pyiron packages to be easily embeddable in complex simulation workflows.program summary program title: sqsgenerator cpc library link to program files: https://doi .org /10 .17632 /m2sb3wzcvc .1 developer's repository link: https://github.com/dgehringer/sqsgenerator licensing provisions: mit programming language: python, c++ supplementary material: https://sqsgenerator.readthedocs.io nature of problem: many technological relevant materials, exhibit a crystalline disorder. within atomistic modelling approaches such as density functional theory (dft) or molecular dynamics, disorder is modelled with a cell containing a (small) finite set of atoms. such an atomic configuration is usually found by enumerating structures. however, since configurational space is growing exponentially efficient tools are needed to sample it properly. solution method: efficient quantification of disorder using a generalization of warren-cowley short range order (wc-sro) parameters [1,2]. by either a monte-carlo approach or systematic enumeration, optimal structures can be found. the software is distributed as a python package offering a command line interface. core parts are written in c++ and exhibit shared (openmp) and distributed (mpi) memory parallelism. for embedding into complex simulation workflows, the tool exposes a python api to integrate into popular packages such as ase [3], pymatgen [4] or pyiron [5].",AB_0431
"in spite of recent advances in computer vision, the classic problem of offline handwritten signature verification still remains challenging. the signature verification task has a high intra-class variability because a given user often shows high variability between its samples. besides, signature verification is harder in the presence of skilled forgeries. recently, in order to tackle these challenges, the research community has investigated deep learning methods for learning feature representations of handwritten signatures. when mapping signatures to a feature space, it is desired to obtain dense clusters of signature's representations, in order to deal with intra-class variability. besides, not only dense clusters are required but also a larger separation between different user's clusters in the feature space. finally, it is also desired to move away feature representations of skilled forgeries in relation to the respective dense cluster of genuine representations. this last property is hard to achieve in the real-world scenario because skilled forgeries are not readily available during training. in this work, we hypothesize that such properties can be achieved by means of a multi-task framework for learning handwritten signature feature representations based on deep contrastive learning. the proposed framework is composed of two objective-specific tasks. the first task aims to map signature examples of the same user closer within the feature space, while separating the feature representations of signatures of different users. the second task aims to adjust the skilled forgeries representations by adopting contrastive losses with the ability to perform hard negative mining. hard negatives are examples from different classes with some degree of similarity that can be applied for training. we evaluated models obtained with the proposed framework in terms of the equal error rate on gpdssynthetic, cedar and mcyt-75 datasets in writer-dependent and writer-independent verification approaches. using synthetic and real signature datasets, friedman tests with bonferroni-dunn post hoc tests were performed to compare the proposed multi-task contrastive models against the popular signet model as a baseline. experiments demonstrated an statistically significant improvement in signature verification with a multi-task contrastive model based on the triplet loss. implementation of the method is available for download at https://github.com/tallesbrito/contrastive_sigver.",AB_0431
"data set acquisition and curation are often the most difficult and time-consuming parts of a machine learning endeavor. this is especially true for proteomics-based liquid chromatography (lc) coupled to mass spectrometry (ms) data sets, due to the high levels of data reduction that occur between raw data and machine learning-ready data. since predictive proteomics is an emerging field, when predicting peptide behavior in lc-ms setups, each lab often uses unique and complex data processing pipelines in order to maximize performance, at the cost of accessibility and reproducibility. for this reason we introduce proteomicsml, an online resource for proteomics-based data sets and tutorials across most of the currently explored physicochemical peptide properties. this community-driven resource makes it simple to access data in easy-to-process formats, and contains easy-to-follow tutorials that allow new users to interact with even the most advanced algorithms in the field. proteomicsml provides data sets that are useful for comparing state-of-the-art machine learning algorithms, as well as providing introductory material for teachers and newcomers to the field alike. the platform is freely available at https://www.proteomicsml.org/, and we welcome the entire proteomics community to contribute to the project at https://github.com/proteomicsml/proteomicsml.",AB_0431
"background: bacteriocins are defined as thermolabile peptides produced by bacteria with biological activity against taxonomically related species. these antimicrobial peptides have a wide application including disease treatment, food conservation, and probiotics. however, even with a large industrial and biotechnological application potential, these peptides are still poorly studied and explored. badass is software with a user-friendly graphical interface applied to the search and analysis of bacteriocin diversity in whole-metagenome shotgun sequencing data. results: the search for bacteriocin sequences is performed with tools such as blast or diamond using the bagel4 database as a reference. the putative bacteriocin sequences identified are used to determine the abundance and richness of the three classes of bacteriocins. abundance is calculated by comparing the reads identified as bacteriocins to the reads identified as 16s rrna gene using silva database as a reference. badass has a complete pipeline that starts with the quality assessment of the raw data. at the end of the analysis, badass generates several plots of richness and abundance automatically as well as tabular files containing information about the main bacteriocins detected. the user is able to change the main parameters of the analysis in the graphical interface. to demonstrate how the software works, we used four datasets from wms studies using default parameters. lantibiotics were the most abundant bacteriocins in the four datasets. this class of bacteriocin is commonly produced by streptomyces sp. conclusions: with a user-friendly graphical interface and a complete pipeline, badass proved to be a powerful tool for prospecting bacteriocin sequences in wholemetagenome shotgun sequencing ( wms) data. this tool is publicly available at https:// sourceforge.net/projects/badass/.",AB_0431
"background: recent population studies are ever growing in number of samples to investigate the diversity of a population or species. these studies reveal new polymorphism that lead to important insights into the mechanisms of evolution, but are also important for the interpretation of these variations. nevertheless, while the full catalog of variations across entire species remains unknown, we can predict which regions harbor additional not yet detected variations and investigate their properties, thereby enhancing the analysis for potentially missed variants.results: to achieve this we developed svhound (https://github.com/lfpaulin/svhou nd), which based on a population level svs dataset can predict regions that harbor unseen sv alleles. we tested svhound using subsets of the 1000 genomes project data and showed that its correlation (average correlation of 2800 tests r = 0.7136) is high to the full data set. next, we utilized svhound to investigate potentially missed or understudied regions across 1kgp and ccdg. lastly we also apply svhound on a small and novel sv call set for rhesus macaque (macaca mulatta) and discuss the impact and choice of parameters for svhound. conclusions: svhound is a unique method to identify potential regions that harbor hidden diversity in model and non model organisms and can also be potentially used to ensure high quality of sv call sets.",AB_0431
"analysis of point clouds through deep convolutional neural networks is an active area of research due to their massive real-world applications including autonomous driving, indoor navigation, robotics, virtual/ augmented reality, unmanned aerial vehicles, and drone technology. however, capturing the fine-grained geometric and semantic properties for the underlying recognition task with raw unstructured point cloud is highly challenging due to the lack of explicit neighborhood relationship and sparsity among the points. in this paper, we have introduced a deep, hierarchical 3d point based architecture for object classification and part segmentation that is able to learn robust geometric features which remain invariant to both the geometry and orientation of the local patches. the proposed architecture consists of multiple layers of sampling, concentric annular convolution, pooling, and residual feature propagation blocks. in the skip connections of our deep residual design, we propose to use a combination of linear projection shortcut and nonlinear relu group normalization shortcut with batch normalization, to improve both the opti-mization landscape and the representational power. our network achieves on par or even better than state-of-the-art results on synthetic and real-world object classification (i.e., modelnet40 and scanobjectnn) and part segmentation (i.e., shapenet-part) benchmark datasets. the implementation and results have been made available at ... https://github.com/rabbia-hassan/deep_annular_residual_ feature_learning_for_3dpointcloudsgithub-link (c) 2023 elsevier b.v. all rights reserved.",AB_0431
"following its initial identification on december 31, 2019, covid-19 quickly spread around the world as a pandemic claiming more than six million lives. an early diagnosis with appropriate intervention can help prevent deaths and serious illness as the distinguishing symptoms that set covid-19 apart from pneumonia and influenza frequently don't show up until after the patient has already suffered significant damage. a chest x-ray (cxr), one of many imaging modalities that are useful for detection and one of the most used, offers a non-invasive method of detection. the cxr image analysis can also reveal additional disorders, such as pneumonia, which show up as anomalies in the lungs. thus these cxrs can be used for automated grading aiding the doctors in making a better diagnosis. in order to classify a cxr image into the negative for pneumonia, typical, indeterminate, and atypical, we used the publicly available cxr image competition dataset siim-fisabio-rsna covid19 from kaggle. the suggested architecture employed an ensemble of efficientnetv2-l for classification, which was trained via transfer learning from the initialised weights of imagenet21k on various subsets of data (code for the proposed methodology is available at: https://github.com/asadkhan1221/siim-covid19.git). to identify and localise opacities, an ensemble of yolo was combined using weighted boxes fusion (wbf). significant generalisability gains were made possible by the suggested technique's addition of classification auxiliary heads to the cnn backbone. the suggested method improved further by utilising test time augmentation for both classifiers and localizers. the results for mean average precision score show that the proposed deep learning model achieves 0.617 and 0.609 on public and private sets respectively and these are comparable to other techniques for the kaggle dataset.",AB_0431
