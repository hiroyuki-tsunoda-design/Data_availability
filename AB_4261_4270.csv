AB,NO
"introductionthe aim of this research is to investigate young cancer patients' cognitive functioning and the underlying neurobiological mechanisms when cognitive functions are impaired. the mybrain protocol is a multidisciplinary study that investigates cancer-related cognitive impairment in children, adolescents and young adults, combining neuropsychology, cognitive neuroscience and cellular neuroscience. the study is exploratory with a wide focus on trajectories of cognitive functions from diagnosis to the end of treatment and into survivorship.methods and analysisprospective longitudinal study including patients diagnosed with non-brain cancers at age 7-29 years. each patient is paired with a control matched on age and social circle.primary objectiveevaluation of neurocognitive function over time.secondary objectivesevaluation of self-perceived quality of life and fatigue, p300 in an electroencephalography (eeg) oddball paradigm, power spectrum in resting state eeg, serum and cerebrospinal fluid levels of biomarkers of neuronal damage, neuroplasticity, proinflammatory and anti-inflammatory markers and their association with cognitive function.ethics and disseminationthe study is approved by the regional ethics committee for the capital region of denmark (no. h-21028495), and the danish data protection agency (no. p-2021-473). results are expected to guide future interventions to prevent brain damage and support patients with cognitive difficulties.trial registration numberthe article is registered at clinicaltrials.gov nct05840575 (https://clinicaltrials.gov/ct2/show/nct05840575)",AB_0427
"background: the potential benefit of transcatheter aortic valve replacement (tavr) in patients with nonsevere aortic stenosis (as) and heart failure is controversial. this study aimed to assess outcomes of patients with nonsevere low-gradient as (lgas) and reduced left ventricular ejection fraction undergoing tavr or medical management. methods: patients undergoing tavr for lgas and reduced left ventricular ejection fraction (<50%) were included in a multinational registry. true-severe low-gradient as (ts-lgas) and pseudo-severe low-gradient as (ps-lgas) were classified according to computed tomography-derived aortic valve calcification thresholds. a medical control group with reduced left ventricular ejection fraction and moderate as or ps-lgas was used (medical-mod). adjusted outcomes between all groups were compared. among patients with nonsevere as (moderate or ps-lgas), outcomes after tavr and medical therapy were compared using propensity score-matching. results: a total of 706 lgas patients undergoing tavr (ts-lgas, n=527; ps-lgas, n=179) and 470 medical-mod patients were included. after adjustment, both tavr groups showed superior survival compared with medical-mod patients (all p<0.001), while no difference was found between ts-lgas and ps-lgas tavr patients (p=0.96). after propensity score-matching among patients with nonsevere as, ps-lgas tavr patients showed superior 2-year overall (65.4%) and cardiovascular survival (80.4%) compared with medical-mod patients (48.8% and 58.5%, both p <= 0.004). in a multivariable analysis including all patients with nonsevere as, tavr was an independent predictor of survival (hazard ratio, 0.39 [95% ci, 0.27-0.55]; p<0.0001). conclusions: among patients with nonsevere as and reduced left ventricular ejection fraction, tavr represents a major predictor of superior survival. these results reinforce the need for randomized-controlled trials comparing tavr versus medical management in heart failure patients with nonsevere as. registration: url: https://www.clinicaltrials.gov; unique identifier: nct04914481.",AB_0427
"backgroundpostoperative delirium (pod) has a negative impact on prognosis, length of stay and the burden of care. although its prediction and identification may improve postoperative care, this need is largely unmet in the brazilian public health system.objectiveto develop and validate a machine-learning prediction model and estimate the incidence of delirium. we hypothesised that an ensemble machine-learning prediction model that incorporates predisposing and precipitating features could accurately predict pod.designa secondary analysis nested in a cohort of high-risk surgical patients.settingan 800-bed, quaternary university-affiliated teaching hospital in southern brazil. we included patients operated on from september 2015 to february 2020.patientswe recruited 1453 inpatients with an all-cause postoperative 30-day mortality risk greater than 5% assessed preoperatively by the excare model.main outcome measurethe incidence of pod classified by the confusion assessment method, up to 7 days postoperatively. predictive model performance with different feature scenarios were compared with the area under the receiver operating characteristic curve.resultsthe cumulative incidence of delirium was 117, giving an absolute risk of 8.05/100 patients. we developed multiple machine-learning nested cross-validated ensemble models. we selected features through partial dependence plot analysis and theoretical framework. we treated the class imbalance with undersampling. different feature scenarios included: 52 preoperative, 60 postoperative and only three features (age, preoperative length of stay and the number of postoperative complications). the mean areas (95% confidence interval) under the curve ranged from 0.61 (0.59 to 0.63) to 0.74 (0.73 to 0.75).conclusiona predictive model composed of three indicative readily available features performed better than those with numerous perioperative features, pointing to its feasibility as a prognostic tool for pod. further research is required to test the generalisability of this model.trial registrationinstitutional review board registration number 04448018.8.0000.5327 (brazilian cep/conep system, available in https://plataformabrasil.saude.gov.br/).",AB_0427
"b-cell epitopes (bces) can identify and bind with receptor proteins (antigens) to initiate an immune response against pathogens. understanding antigen-antibody binding interactions has many applications in biotechnology and biomedicine, including designing antibodies, therapeutics, and vaccines. lab-based experimental identifi-cation of these proteins is time-consuming and challenging. computational techniques have been proposed to discover bces, but most lack of significant accomplishments. this work uses classical and deep learning models (dlms) with sequence-based features to predict immunity stimulator bces from proteomics sequences. the proposed convolutional neural network-based model outperforms other models with an accuracy (acc) of 0.878, an f-measure of 0.871, and an area under the receiver operating characteristic curve (auc) of 0.945. the proposed strategy achieves 58.7% better results on average than other state-of-the-art approaches based on the mathews correlation coefficient (mcc) results. the established model is accessible through a web application located at http://deeplbcepred.pythonanywhere.com.",AB_0427
"a noiseprint is a camera-related artifact that can be extracted from an image to serve as a powerful tool for several forensic tasks. the noiseprint is built with a deep learning data-driven approach that is trained to produce unique noise residuals with clear traces of camera-related artifacts. this data-driven approach results in a complex relationship that governs the noiseprint with the input image, making it challenging to attack. this article proposes a novel neural noiseprint transfer framework for noiseprint-based counter forensics. given an authentic image and a forged image, the proposed framework synthesizes a newly generated image that is visually imperceptible to the forged image, but its noiseprint is very close to the noiseprint of the authentic one, to make it appear as if it is authentic and thus renders the noiseprint-based forensics ineffective. based on deep content and noiseprint representations of the forged and authentic images, we implement the proposed framework in two different approaches. the first is an optimization-based approach that synthesizes the generated image by minimizing the difference between its content representation with the content representation of the forged image while, at the same time, minimizing the noiseprint representation difference from the authentic one. the second approach is a noiseprint injection-based approach, which first trains a novel neural noiseprint-injector network that can inject the noiseprint of an image into another one. then, the trained noiseprint-injector is used to inject the noiseprint from the authentic image into the forged one to produce the generated image. the proposed approaches are generic and do not require training for specific images or camera models. both approaches are evaluated on several datasets against two common forensic tasks: the forgery localization and camera source identification tasks. in the two tasks, the proposed approaches are able to significantly reduce several forensic accuracy scores compared with two noiseprint-based forensics methods while at the same time producing high-fidelity images. on the dso-1 dataset, the reduction in the forensic accuracy scores has an average of 75%, while the produced images have an average psnr of 31.5 db and ssim of 0.9. the source code of the proposed approaches is available on github (https://ithub.com/ahmed-elliethy/nnt).",AB_0427
"background: the gut microbiota has emerged as a potential therapeutic target to improve the management of obesity and its comorbidities. objective: we investigated the impact of a high fiber (similar to 38 g/d) plant-based diet, consumed ad libitum, with or without added inulin-type fructans (itf), on the gut microbiota composition and cardiometabolic outcomes in subjects with obesity. we also tested if baseline prevotella/bacteroides (p/b) ratio predicts weight loss outcomes. methods: this is a secondary exploratory analysis from the preventomics study, in which 100 subjects (82 completers) aged 18-65 years with body mass index 27-40 kg/m(2) were randomized to 10weeks of double-blinded treatment with a personalized or a generic plant-based diet. changes from baseline to endof-trial in gut microbiota composition (16s rrna gene amplicon sequencing), body composition, cardiometabolic health and inflammatory markers were evaluated in the whole cohort (n =82), and also compared in the subgroup of subjects who were supplemented with an additional 20g/d itf-prebiotics (n =21) or their controls (n=22). results: in response to the plant-based diet, all subjects lost weight (-3.2 [95% ci -3.9, -2.5] kg) and experienced significant improvements in body composition and cardiometabolic health indices. addition of itf to the plant-based diet reduced microbial diversity (shannon index) and selectively increased bifidobacterium and faecalibacterium (q <0.05). the change in the latter was significantly associated with higher values of insulin and homa-ir and lower hdl cholesterol. in addition, the ldl:hdl ratio and the concentrations of il-10, mcp-1 and tnfa were significantly elevated in the itf-subgroup. there was no relationship between baseline p/b ratio and changes in body weight (r=-0.07, p=0.53). conclusion: a plant-based diet consumed ad libitum modestly decreases body weight and has multiple health benefits in individuals with obesity. addition of itf-prebiotics on top this naturally fiber-rich background selectively changes gut microbiota composition and attenuates some of the realized cardiometabolic benefits. clinical trial registration: [https://clinicaltrials.gov/ct2/show/nct04590989], identifier [nct04590989].",AB_0427
"this paper presents a new system to detect changes between reference and target videos suitable for small-scale datasets. twin pre-trained resnet-50 features are processed using a learning-based pipeline that has a limited number of adjustable parameters, allowing end-to-end training even on relatively small databases. this is achieved with two innovative modules in tandem: a low-complexity dissimilarity module and a post -processing step using learnable morphological operations. both can be smoothly incorporated in optimization procedures that employ gradient-based algorithms. the pipeline ends with temporal consistency and change classification modules, and it is evaluated on the vdao dataset, a challenging database of videos recorded with moving cameras in a cluttered industrial environment. ablation studies show how each proposed module contributes to the final system performance, with a prominence role for the newly proposed ones. results indicate that the proposed system achieves a detection performance that is about 18% superior to the one of current state-of-the-art methods. software, results, and a pre-trained architecture of the proposed framework are available at https://github.com/rafaelpadilla/tcf-lmo.",AB_0427
"motivation the problem of model inference is of fundamental importance to systems biology. logical models (e.g. boolean networks; bns) represent a computationally attractive approach capable of handling large biological networks. the models are typically inferred from experimental data. however, even with a substantial amount of experimental data supported by some prior knowledge, existing inference methods often focus on a small sample of admissible candidate models only. results we propose boolean network sketches as a new formal instrument for the inference of boolean networks. a sketch integrates (typically partial) knowledge about the network ' s topology and the update logic (obtained through, e.g. a biological knowledge base or a literature search), as well as further assumptions about the properties of the network ' s transitions (e.g. the form of its attractor landscape), and additional restrictions on the model dynamics given by the measured experimental data. our new bns inference algorithm starts with an ' initial ' sketch, which is extended by adding restrictions representing experimental data to a ' data-informed ' sketch and subsequently computes all bns consistent with the data-informed sketch. our algorithm is based on a symbolic representation and coloured model-checking. our approach is unique in its ability to cover a broad spectrum of knowledge and efficiently produce a compact representation of all inferred bns. we evaluate the method on a non-trivial collection of real-world and simulated data. availability and implementation: all software and data are freely available as a reproducible artefact at https://doi. org/10.5281/zenodo.7688740.",AB_0427
"background & aim: for older adults, the dietary protein intake has shown to be skewed towards the evening meal. resultingly, the vital source of essential amino acids could be insufficient after some meals, while after the evening meal the dietary protein could be suboptimally utilized for protein synthesis. the present study explored if an even distribution of the protein intake could improve the dietary amino acid absorption and whole-body protein net-balance. methods: twenty-four healthy elderly males and females were included in a randomized controlled trial. ten days of habituation to either an even (n 1/4 12) or skewed (n 1/4 12) protein intake, was followed by a trial day. the total protein intake was controlled at 1.5 g/kg lbm, divided into 30% at each main meal in even, and into 15% at breakfast and lunch and 60% at dinner in skewed. snacks with 5% of the protein intake were served between meals. energy intake in the meals and snacks were equal in both groups. intrinsically labelled 2h5- phenylalanine minced meat was served as the dietary protein to assess the amino acid absorption. on the trial day, infusion of 2h8-phenylalanine and 2h2-tyrosine, and blood samples taken over 11 h were used to measure whole-body protein turnover. vastus lateralis muscle biopsies were taken to measure 9 h muscle protein fsr. results: amino acid absorption rates and concentrationswere greater in evencompared to skewedprotein intake. whole-body protein breakdown rates were lower with similar protein synthesis rates, and consequently the net-balance was greater in even after breakfast and lunch compared to skewed and were the same in both groups after dinner. muscle protein fsr were not different between even and skewed. conclusions: the whole-body protein net-balance was more positive in even compared to skewed for an extended time of the measured period, driven by a lower whole-body protein breakdown in even. clinical trials registration: nct03870425, https://clinicaltrials.gov/ct2/show/nct03870425. (c) 2023 the authors. published by elsevier ltd. this is an open access article under the cc by license",AB_0427
"we present the pyerrors python package for statistical error analysis of monte carlo data. linear error propagation using automatic differentiation in an object oriented framework is combined with the gamma-method for a reliable estimation of autocorrelation times. data from different sources can easily be combined, keeping the information on the origin of error components intact throughout the analysis. pyerrors can be smoothly integrated into the existing scientific python ecosystem which allows for efficient and compact analyses. program summary program title: pyerrors cpc library link to program files: https://doi.org/10.17632/7ncw242ymh.1 developer's repository link: https://github.com/fjosw/pyerrors licensing provisions: mit programming language: python nature of problem: data obtained from markov chain monte carlo simulations exhibits autocorrelations. these become particularly severe when approaching the continuum limit of lattice discretized quantum field theories which becomes more and more relevant in modern day large scale simulations. in order to obtain reliable error estimates these autocorrelations have to be taken into account in complex data analysis workflows. solution method: linear error propagation in combination with automatic differentiation is implemented in a new python data type which keeps track of statistical errors across multiple sources of uncertainty. operator overloading allows for a seamless integration into the scientific python ecosystem and into existing workflows. the gamma-method facilitates a controlled estimate of integrated autocorrelation times at any stage of the analysis and provides reliable error estimates without numerical overhead. (c) 2023 the author(s). published by elsevier b.v. this is an open access article under the cc by license ().",AB_0427
