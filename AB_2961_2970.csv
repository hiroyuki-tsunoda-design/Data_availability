AB,NO
"practical applications ask for object detection models that achieve high performance at low overhead. knowledge distillation demonstrates favorable potential in this case by transferring knowledge from a cumbersome teacher model to a lightweight student model. however, previous distillation methods are plagued with massive misleading background information in remote sensing images and ignore investigating the relationships between different instances. in this article, we propose an instance-aware distillation (insdist for short) method to derive efficient remote sensing object detectors. our insdist combines feature-based and relation-based knowledge distillation to make the most of instance-related information in the knowledge transfer from the teacher to the student. on one hand, we propose a parameter-free masking module to decouple instance-related foreground from instance-irrelevant background in multiscale features. on the other hand, we construct the relationships between different instances to enhance the learning of intraclass compactness and interclass dispersion. the student comprehensively imitates both features and relationships from the teacher, yielding considerable effectiveness in dealing with complex remote sensing images. in addition, our insdist can be easily built on mainstream object detectors with negligible extra cost. extensive experiments on two large-scale remote sensing object detection datasets, namely dior and dota, show that our insdist obtains noticeable gains over other distillation methods for both one-stage and two-stage, as well as both anchor-based and anchor-free detectors. the source code will be publicly available at https://github.com/swift1988/insdist",AB_0297
"convolutional neural networks (cnns) have been successfully employed in remote sensing image classification because of their robust feature representation for different visual tasks and powerful graphics processing units (gpus). the attendant problem is that high computational cost and high memory footprint hindering the application of cnns for remote sensing applications in resource- and time-sensitive situations. based on practical deployment requirements, we pioneer a pruning-quantization joint learning model compression method for remote sensing image classification, called co-compression via superior gene (cc-sg). an enhanced evolution algorithm (eea) is adopted as the agent to search a superior gene, and immediately following, a director receives the superior gene and gives a compression mask and a resource constraint feedback to the agent. the network is eventually compressed and fine-tuned according to the optimal compression mask. specifically, we introduce gene age and progressive shrinkage mutation rate to eea and design a fitness function that balances accuracy and resource constraints. as validated using the uc merced land-use and nwpu-resisc45 datasets, the proposed cc-sg demonstrated superiority over other compared model compression approaches. for example, cc-sg attained substantial bit operations (bops) compression ratio of 40.04 with 0.956% accuracy increase for vgg-16 on uc merced land-use dataset and 40.00 with 0.203% accuracy increase for resnet-56 on nwpu-resisc45 dataset. the code is available at https://github.com/fanxxxxyi/cc-sg.",AB_0297
"deep learning (dl)-based pansharpening methods have shown great advantages in extracting spectral-spatial features from multispectral (ms) and panchromatic (pan) images compared with traditional methods. however, most dl-based methods ignore the local inner connection between the source images and the high-resolution ms (hrms) image, which cannot fully extract spectral-spatial information and attempt to improve the quality of fusion by increasing the complexity of the network. to solve these problems, a lightweight network based on adaptive weighted feature learning network (awfln) is proposed for pansharpening. specifically, a novel detail extraction model is first built by exploring the local relationship between hrms and source images, thereby improving the accuracy of details and the interpretability of the network. guided by this model, we then design a residual multiple receptive-field structure to fully extract spectral-spatial features of source images. in this structure, an adaptive feature learning block based on spectral-spatial interleaving attention is proposed to adaptively learn the weights of features and improve the accuracy of the extracted details. finally, the pansharpened result is obtained by a detail injection model in awfln. numerous experiments are carried out to validate the effectiveness of the proposed method. compared to traditional and state-of-the-art methods, awfln performs the best both subjectively and objectively, with high efficiency. the code is available at https://github.com/yotick/awfln.",AB_0297
"as for hyperspectral images (hsis), the discrepancy of contiguous spectral information should be the main basis for the identification of ground objects. due to the difficulty of spectral sequence coding and the spectrum similarity between categories, successful deep-learning-based classification methods always attempt to capture the spatial information to improve the accuracy by convolutional neural networks (cnns) or other excellent spatial feature extractors. however, extracting spatial features is generally accompanied by the distortion of ground objects distribution and categories boundary. to effectively represent spectral features, the spectralformer based on transformer backbone can better capture the long-term dependence of the spectrum, which improves the performance of spectral feature methods significantly. however, it is still unable to compete with advanced spectral-spatial feature methods. to exploit the discriminative advantage of the spectrum fully, this letter introduces an efficient sparse-to-dense optical flow estimation method to track the spectrum variation in the hsi. then, we take such a variation as a spectrum motion feature to enhance the original spectrum. at last, we continue to use the spectralformer to encode the concatenated spectrum sequence for classification. extensive experiments show that the spectralformer enhanced by the spectrum motion feature (sf-smf) significantly improves the performance of spectral feature methods, even surpassing advanced spectral-spatial feature methods. sf-smf can avoid interference with additional spatial information to obtain exquisite whole-domain classification maps, showing its practical value. the codes will be public at https://github.com/sssssyf/sf-smf.",AB_0297
"the transformer framework has shown great potential in the field of hyperspectral image (hsi) classification due to its superior global modeling capabilities compared to convolutional neural networks (cnns). to utilize the transformer to model spatial-spectral information, a hybrid transformer that integrates multigranularity tokens and spatial-spectral attention (ssa) is proposed. specifically, a token generator is designed to embed the multigranularity semantic tokens, which contributes richer image features to the model by exploiting cnn's local representation capability. moreover, a transformer encoder with an ssa mechanism is proposed to capture the global dependencies between different tokens, enabling the model to focus on more differentiated channels and spatial locations to improve the classification accuracy. ultimately, adaptive weighted fusion is applied to different granularity transformer branches to boost hybridformer's classification performance. experiments were conducted on four new challenging datasets, and the results indicate that hybridformer achieves state-of-the-art results in terms of classification performance. the code of this work will be available at https://github.com/zhaolin6/hybridformer for the sake of reproducibility.",AB_0297
"road extraction from satellite imagery is vital in a broad range of applications. however, extracting complete roads is challenging due to road occlusions caused by the surroundings. this letter proposed an improved encoder-decoder network via extracting road context and integrating full-stage features from satellite imagery, dubbed as rcfsnet. a multiscale context extraction (msce) module is designed to enhance inference capabilities by introducing adequate road context. multiple full-stage feature fusion (fsff) modules in the skip connection are devised to provide accurate road structure information, and we devise a coordinate dual-attention mechanism (cdam) to strengthen the representation of road features. extensive experiments are carried out on two public datasets, and as a result, our rcfsnet outperforms other state-of-the-art methods. the results indicate that the road labels extracted by our method have preferable connectivity. the source code will be available at https://github.com/cver-yang/rcfsnet.",AB_0297
"the domain adaptation (da) approaches available to date are usually not well suited for practical da scenarios of remote sensing image classification since these methods (such as unsupervised da) rely on rich prior knowledge about the relationship between label sets of source and target domains, and source data are often not accessible due to privacy or confidentiality issues. to this end, we propose a practical universal da (unida) setting for remote sensing image scene classification that requires no prior knowledge on the label sets. furthermore, a novel unida method without source data is proposed for cases when the source data are unavailable. the architecture of the model is divided into two parts: the source data generation stage and the model adaptation stage. the first stage estimates the conditional distribution of source data from the pretrained model using the knowledge of class separability in the source domain and then synthesizes the source data. with this synthetic source data in hand, it becomes a unida task to classify a target sample correctly if it belongs to any category in the source label set or mark it as unknown otherwise. in the second stage, a novel transferable weight that distinguishes the shared and private label sets in each domain promotes the adaptation in the automatically discovered shared label set and recognizes the unknown samples successfully. empirical results show that the proposed model is effective and practical for remote sensing image scene classification, regardless of whether the source data are available or not. the code is available at https://github.com/zhu-xlab/unida.",AB_0297
"the extraction of building outline vectors is an essential task in supporting various applications. although the recent development of deep-learning-based techniques has made advancements in the automation of this task, the accuracy and precision are insufficient due to errors caused by abundant noise and obstruction around buildings in aerial images. to better address this issue, this letter presents a new approach called multi-task edge detection (mted) for building vectorization with the following characteristics. first, instead of detecting building corner points that are very sensitive to noise effects, a deep-learning-based rotated bounding box (rbb) detector is introduced for building edge detection to increase robustness to interference. second, a multi-task learning strategy is designed to integrate building segmentation inside the metd framework to closely guide edge detection using spatial context. third, a simple yet effective geometry-guided postprocessing method is designed to reconstruct vectorized building outlines based on the detected edges and learned building shape prior knowledge. the comparative experiments conducted on benchmark very-high-resolution optical aerial images indicate that the proposed approach can significantly outperform the state-of-the-art in terms of vertex-based building outline accuracy metrics. with a test time of 58 ms per building, this method enables efficient building polygon labeling in interactive mapping applications for building surveying and mapping. code is available at https://github.com/yifanthomaswu/mted_framework.",AB_0297
"the application of optical remote sensing images (orsis) is prevalent in many fields. accordingly, orsi-oriented salient object detection (sod) has attracted more attention in recent years. however, yet many previously proposed methods present appealing performance in natural scene images (nsis), they are difficult to be directly extended to remote sensing images due to the more complex scenes, such as blended backgrounds and diversiform topological shapes. most specifically designed models often fail to achieve satisfactory results due to the weak usage of edge information and the ignorance of attention loss. besides, computational inefficiency often causes poor applicability. to solve these problems, we propose a new model, namely, bidimensional attention and full-stage semantic guidance network (bafs-net), containing an edge guidance branch and a mainstream detection branch. concretely, edge guidance generates boundary information, in which supervision with border labels is imposed to highlight the salient regions and plays a complementary role on the main branch. the mainstream detection branch involves two important components, i.e., bidimensional attention modules (bams) and semantic-guided fusion modules (sgfms). between these two, bam uniformly assembles channel and spatial attention in an efficient and rational manner, addressing the open issue of dimensionwisely attention computation. sgfm hammers at the fusion of high-level features and low-level features. moreover, the semantic maps are employed to interact with sgfm in full stages. our approach surpasses most state-of-the-art rsi-sod methods proposed in recent years, with respect to the accuracy, parameter size, computational cost, and floating point operations per second (flops). the code is available at https://github.com/zhengjianwei2/bafs-net.",AB_0297
"as a key issue in orchestrating various biological processes and functions, protein post -translational modification (ptm) occurs widely in the mechanism of protein's function of animals and plants. glutarylation is a type of protein-translational modification that occurs at active epsilon-amino groups of specific lysine residues in proteins, which is associated with various human diseases, including diabetes, cancer, and glutaric aciduria type i. therefore, the issue of prediction for glutarylation sites is particularly important. this study developed a brand-new deep learning-based prediction model for glutarylation sites named deepdn_iglu via adopting attention residual learning method and densenet. the focal loss function is utilized in this study in place of the traditional cross-entropy loss function to address the issue of a substantial imbalance in the number of positive and negative samples. it can be noted that deepdn_iglu based on the deep learning model offers a greater potential for the glutarylation site prediction after employing the straightforward one hot encoding method, with sensitivity (sn), specificity (sp), accuracy (acc), mathews correlation coefficient (mcc), and area under curve (auc) of 89.29%, 61.97%, 65.15%, 0.33 and 0.80 accordingly on the independent test set. to the best of the authors' knowledge, this is the first time that densenet has been used for the prediction of glutarylation sites. deepdn_iglu has been deployed as a web server (https://bioinfo.wugenqiang.top/-smw/deepdn_iglu/) that is available to make glutarylation site prediction data more accessible.",AB_0297
