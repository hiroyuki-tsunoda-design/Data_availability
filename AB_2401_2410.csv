AB,NO
"this study proposes an adaptive image augmentation scheme using deep reinforcement learning (drl) to improve the performance of a deep learning-based automated optical inspection system. the study addresses the challenge of inconsistency in the performance of single image augmentation methods. it introduces a drl algorithm, dqn, to select the most suitable augmentation method for each image. the proposed approach extracts geometric and pixel indicators to form states, and uses deeplab-v3+ model to verify the augmented images and generate rewards. image augmentation methods are treated as actions, and the dqn algorithm selects the best methods based on the images and segmentation model. the study demonstrates that the proposed framework outperforms any single image augmentation method and achieves better segmentation performance than other semantic segmentation models. the framework has practical implications for developing more accurate and robust automated optical inspection systems, critical for ensuring product quality in various industries. future research can explore the generalizability and scalability of the proposed framework to other domains and applications. the code for this application is uploaded at https://github.com/lynnkobe/adaptive-image-augmentation.git.",AB_0241
"we address the problem of multi-modal object tracking in video and explore various options available for fusing the complementary information conveyed by the visible (rgb) and thermal infrared (tir) modalities, including pixel-level, feature-level and decision-level fusion. specifically, in contrast to the existing approaches, we propose and develop the paradigm for combining multi-modal information for image fusion at pixel level. at the feature level, two different kinds of fusion strategies are investigated for completeness, i.e., the attention based online fusion strategy and the offline-trained fusion block. at the decision level, a novel fusion strategy is put forward, inspired by the success of the simple averaging configuration which has shown so much promise. the effectiveness of the proposed decision-level fusion strategy owes to a number of innovative contributions, including a dynamic weighting of the rgb and tir contributions and a linear template update operation. a variant of the proposed decision fusion method produced the winning tracker at the visual object tracking challenge 2020 (vot-rgbt2020). a comprehensive comparison of the innovative pixel and feature-level fusion strategies with the proposed decision-level fusion method highlights the advantages fusing multimodal information at the decision score level. extensive experimental results on five challenging datasets, i.e., gtot, vot-rgbt2019, rgbt234, lasher and vot-rgbt2020, demonstrate the effectiveness and robustness of the proposed method, compared to the state-of-the-art approaches. the code is available at https://github.com/zhangyong-tang/dfat.",AB_0241
"due to the high diversity and complexity of body shapes, it is challenging to directly estimate the human geometry from a single image with the various clothing styles. most of the model-based approaches are limited to predict the shape and pose of a minimally clothed body with over-smoothing surface. while capturing the fine detailed geometries, the model-free methods are lack of the fixed mesh topology. to address these issues, we propose a novel topology-preserved human reconstruction approach by bridging the gap between model-based and model-free human reconstruction. we present an end-to-end neural network that simultaneously predicts the pixel-aligned implicit surface and an explicit mesh model built by graph convolutional neural network. experiments on deephuman and our collected dataset showed that our approach is effective. the code will be made publicly available at https://github.com/l1346792580123/sdfgcn.",AB_0241
"a gene family refers to a group of genes that share a common ancestry and encode proteins or rna molecules with similar functions or structural features. gene families play a crucial role in determining the traits of plants and can be utilized to develop new crop varieties. therefore, a comprehensive database of gene family is significant for gaining deep insight into crops. to address this need, we have developed cropgf (https://bis.zju.edu.cn/cropgf), a comprehensive visual platform that encompasses six important crops (rice, wheat, maize, barley, sorghum and foxtail millet) and one model plant (arabidopsis), as well as genomics, transcriptomics and proteomics data for gene family mining and analysis, covering a total of 314611 genes and 4399 types of domains. cropgf provides a versatile search system that allows for the identification of gene families and their members in a single crop or multiple crops. users can customize their search based on gene family domains and/or homology using keywords or blast. to enhance usability, we have collected the corresponding id information from various public databases for both genes and domains. furthermore, cropgf comprises numerous downstream analysis modules, such as ka/ks analysis, phylogenetic tree construction, subcellular localization analysis and more. these visually-displayed modules provide intuitive insights into gene expression patterns, gene family expansion and functional relationships across different molecular levels and different species. we believe that cropgf will be a valuable resource for deep mining and analysis in future studies of crop gene families. database url: https://bis.zju.edu.cn/cropgf",AB_0241
"configurable software systems can be tuned for better performance. leveraging on some pareto optimizers, recent work has shifted from tuning for a single, time-related performance objective to two intrinsically different objectives that assess distinct performance aspects of the system, each with varying aspirations to be satisfied, e.g., the latency is less than 10s while the memory usage is no more than 1gb. before we design better optimizers, a crucial engineering decision to make therein is how to handle the performance requirements with clear aspirations in the tuning process. for this, the community takes two alternative optimization models: either quantifying and incorporating the aspirations into the search objectives that guide the tuning, or not considering the aspirations during the search but purely using them in the later decision-making process only. however, despite being a crucial decision that determines how an optimizer can be designed and tailored, there is a rather limited understanding of which optimization model should be chosen under what particular circumstance, and why. in this article, we seek to close this gap. firstly, we do that through a review of over 426 articles in the literature and 14 real-world requirements datasets, from which we summarize four performance requirement patterns that quantify the aspirations in the configuration tuning. drawing on these, we then conduct a comprehensive empirical study that covers 15 combinations of the state-of-the-art performance requirement patterns, four types of aspiration space, three pareto optimizers, and eight real-world systems/environments, leading to 1,296 cases of investigation. our findings reveal that (1) the realism of aspirations is the key factor that determines whether they should be used to guide the tuning; (2) the given patterns and the position of the realistic aspirations in the objective landscape are less important for the choice, but they do matter to the extents of improvement; (3) the available tuning budget can also influence the choice for unrealistic aspirations but it is insignificant under realistic ones. to promote open science practice, we make our code and dataset publicly available at: https://github.com/ideas-labo/aspiration-study.",AB_0241
"the collection and curation of large-scale medical datasets from multiple institutions is essential for training accurate deep learning models, but privacy concerns often hinder data sharing. federated learning (fl) is a promising solution that enables privacy-preserving collaborative learning among different institutions, but it generally suffers from performance deterioration due to heterogeneous data distributions and a lack of quality labeled data. in this paper, we present a robust and label-efficient self-supervised fl framework for medical image analysis. our method introduces a novel transformer-based self-supervised pre-training paradigm that pre-trains models directly on decentralized target task datasets using masked image modeling, to facilitate more robust representation learning on heterogeneous data and effective knowledge transfer to downstream models. extensive empirical results on simulated and real-world medical imaging non-iid federated datasets show that masked image modeling with transformers significantly improves the robustness of models against various degrees of data heterogeneity. notably, under severe data heterogeneity, our method, without relying on any additional pre-training data, achieves an improvement of 5.06%, 1.53% and 4.58% in test accuracy on retinal, dermatology and chest x-ray classification compared to the supervised baseline with imagenet pre-training. in addition, we show that our federated self-supervised pre-training methods yield models that generalize better to out-of-distribution data and perform more effectively when fine-tuning with limited labeled data, compared to existing fl algorithms. the code is available at https://github.com/rui-yan/ssl-fl.",AB_0241
"the purpose of federated learning is to enable multiple clients to jointly train a machine learning model without sharing data. however, the existing methods for training an image segmentation model have been based on an unrealistic assumption that the training set for each local client is annotated in a similar fashion and thus follows the same image supervision level. to relax this assumption, in this work, we propose a label-agnostic unified federated learning framework, named fedmix, for medical image segmentation based on mixed image labels. in fedmix, each client updates the federated model by integrating and effectively making use of all available labeled data ranging from strong pixel-level labels, weak bounding box labels, to weakest image-level class labels. based on these local models, we further propose an adaptive weight assignment procedure across local clients, where each client learns an aggregation weight during the global model update. compared to the existing methods, fedmix not only breaks through the constraint of a single level of image supervision but also can dynamically adjust the aggregation weight of each local client, achieving rich yet discriminative feature representations. experimental results on multiple publicly-available datasets validate that the proposed fedmix outperforms the state-of-the-art methods by a large margin. in addition, we demonstrate through experiments that fedmix is extendable to multi-class medical image segmentation and much more feasible in clinical scenarios. the code is available at: https://github.com/jwicaksana/fedmix.",AB_0241
"achieving subjective and objective quality assessment of underwater images is of high significance in underwater visual perception and image/video processing. however, the development of underwater image quality assessment (uiqa) is limited for the lack of publicly available underwater image datasets with human subjective scores and reliable objective uiqa metrics. to address this issue, we establish a large-scale underwater image dataset, dubbed uid2021, for evaluating no-reference (nr) uiqa metrics. the constructed dataset contains 60 multiply degraded underwater images collected from various sources, covering six common underwater scenes (i.e., bluish scene, blue-green scene, greenish scene, hazy scene, low-light scene, and turbid scene), and their corresponding 900 quality improved versions are generated by employing 15 state-of-the-art underwater image enhancement and restoration algorithms. mean opinion scores with 52 observers for each image of uid2021 are also obtained by using the pairwise comparison sorting method. both in-air and underwater-specific nr iqa algorithms are tested on our constructed dataset to fairly compare their performance and analyze their strengths and weaknesses. our proposed uid2021 dataset enables ones to evaluate nr uiqa algorithms comprehensively and paves the way for further research on uiqa. the dataset is available at https://github.com/hou-guojia/uid2021.",AB_0241
"large training datasets are important for deep learning-based methods. for medical image segmentation, it could be however difficult to obtain large number of labeled training images solely from one center. distributed learning, such as swarm learning, has the potential to use multi-center data without breaching data privacy. however, data distributions across centers can vary a lot due to the diverse imaging protocols and vendors (known as feature skew). also, the regions of interest to be segmented could be different, leading to inhomogeneous label distributions (referred to as label skew). with such non-independently and identically distributed (non-iid) data, the distributed learning could result in degraded models. in this work, we propose a novel swarm learning approach, which assembles local knowledge from each center while at the same time overcomes forgetting of global knowledge during local training. specifically, the approach first leverages a label skew-awared loss to preserve the global label knowledge, and then aligns local feature distributions to consolidate global knowledge against local feature skew. we validated our method in three non-iid scenarios using four public datasets, including the multi-centre, multi-vendor and multi-disease cardiac segmentation (m & ms) dataset, the federated tumor segmentation (fets) dataset, the multi-modality whole heart segmentation (mmwhs) dataset and the multi-site prostate t2-weighted mri segmentation (msprosmri) dataset. results show that our method could achieve superior performance over existing methods. code will be released via https://zmiclab.github.io/projects.html once the paper gets accepted.",AB_0241
"image inpainting that completes large free-form missing regions in images is a promising yet challenging task. state-of-the-art approaches have achieved significant progress by taking advantage of generative adversarial networks (gan). however, these approaches can suffer from generating distorted structures and blurry textures in high-resolution images (e.g., $512\times 512$512x512). the challenges mainly drive from (1) image content reasoning from distant contexts, and (2) fine-grained texture synthesis for a large missing region. to overcome these two challenges, we propose an enhanced gan-based model, named aggregated contextual-transformation gan (aot-gan), for high-resolution image inpainting. specifically, to enhance context reasoning, we construct the generator of aot-gan by stacking multiple layers of a proposed aot block. the aot blocks aggregate contextual transformations from various receptive fields, allowing to capture both informative distant image contexts and rich patterns of interest for context reasoning. for improving texture synthesis, we enhance the discriminator of aot-gan by training it with a tailored mask-prediction task. such a training objective forces the discriminator to distinguish the detailed appearances of real and synthesized patches, and in turn facilitates the generator to synthesize clear textures. extensive comparisons on places2, the most challenging benchmark with 1.8 million high-resolution images of 365 complex scenes, show that our model outperforms the state-of-the-art. a user study including more than 30 subjects further validates the superiority of aot-gan. we further evaluate the proposed aot-gan in practical applications, e.g., logo removal, face editing, and object removal. results show that our model achieves promising completions in the real world. we release codes and models in https://github.com/researchmm/aot-gan-for-inpainting.",AB_0241
