AB,NO
"change detection (cd) is a hot research topic in the remote-sensing (rs) community. with the increasing availability of high-resolution (hr) rs images, there is a growing demand for cd models with high detection accuracy and generalization ability. in other words, the cd models are expected to work well for various hrrs images. convolutional neural networks (cnns) have been dominated in hrrs image cd due to their excellent information extraction and nonlinear fitting capabilities. however, they are not skilled in modeling long-range contexts hidden in hrrs images, which limits their performance in cd tasks more or less. recently, the transformer, which is good at extracting global context dependencies, has become popular in the rs community. nevertheless, detailed local knowledge receives insufficient emphasis in common transformers. considering the above discussion, we combine cnns and transformers and propose a new w-shaped dual-siamese branch hierarchical network for hrrs image cd named w-shaped hierarchical network (wnet). wnet first incorporates a siamese cnn and a siamese transformer into a dual-branch encoder to extract multilevel local fine-grained features and global long-range contextual dependencies. also, we introduce deformable ideas into the siamese cnn and transformer to make wnet understand the critical and irregular areas within hrrs images. second, the difference enhancement module (dem) is developed and embedded into the encoder to produce the difference feature maps at different levels. using simple pixel-wise subtraction and channel-wise concatenation, the changes of interest and irrelevant changes can be highlighted and suppressed in a learnable manner. next, the multilevel difference feature maps are fused stage by stage by cnn-transformer fusion modules (ctfms), which are the basic units of the decoder in wnet. in ctfms, the local, global, and cross-scale clues are taken into account to ensure the integrity of information. finally, a simple classifier is constructed and added at the top of the decoder to predict the change maps. positive experimental results counted on four public datasets demonstrate that the proposed wnet is helpful in hrrs image cd tasks. our source codes are available at https://github.com/tangxu-group/remote-sensing-image-change-detection/tree/main/wnet.",AB_0282
"deep hashing has been intensively studied and successfully applied in large-scale image retrieval systems due to its efficiency and effectiveness. recent studies have recognized that the existence of adversarial examples poses a security threat to deep hashing models, that is, adversarial vulnerability. notably, it is challenging to efficiently distill reliable semantic representatives for deep hashing to guide adversarial learning, and thereby it hinders the enhancement of adversarial robustness of deep hashing-based retrieval models. moreover, current researches on adversarial training for deep hashing are hard to be formalized into a unified minimax structure. in this paper, we explore semantic-aware adversarial training (saat) for improving the adversarial robustness of deep hashing models. specifically, we conceive a discriminative mainstay features learning (dmfl) scheme to construct semantic representatives for guiding adversarial learning in deep hashing. particularly, our dmfl with the strict theoretical guarantee is adaptively optimized in a discriminative learning manner, where both discriminative and semantic properties are jointly considered. moreover, adversarial examples are fabricated by maximizing the hamming distance between the hash codes of adversarial samples and mainstay features, the efficacy of which is validated in the adversarial attack trials. further, we, for the first time, formulate the formalized adversarial training of deep hashing into a unified minimax optimization under the guidance of the generated mainstay codes. extensive experiments on benchmark datasets show superb attack performance against the state-of-the-art algorithms, meanwhile, the proposed adversarial training can effectively eliminate adversarial perturbations for trustworthy deep hashing-based retrieval. our code is available at https://github.com/xandery-geek/saat.",AB_0282
"cloud detection plays a significant role in remote sensing (rs) image applications. existing deep learning-based cloud detection methods rely on massive precise pixelwise annotations, which are time-consuming and expensive. to alleviate this problem, we propose a weakly supervised cloud detection framework that leverages physical rules to generate weak supervision for cloud detection in rs images. specifically, a rule-based adaptive pseudo labeling (rapl) algorithm is devised to adaptively annotate potential cloud pixels based on cloud spectral properties without manual intervention. unlike existing physical annotations using fixed thresholds, rapl employs the bidirectional threshold segmentation and adaptive gating mechanism to annotate cloud and boundary masks with more explicit semantic categories and spatial structures separately. subsequently, these pseudo masks are treated as weak supervision to optimize the heuristic cloud detection network for pixelwise segmentation. considering that clouds appear as complex geometric structures and nonuniform spectral reflectance, a deformable boundary refining module is designed to enhance the modeling ability of spatial transformation and activate sharp boundaries from translucent cloud regions. moreover, a harmonic loss is employed to recognize clouds with nonuniform spectral reflectance and suppress the interference of bright backgrounds. extensive experiments on the gf-1, l8 biome, and weakly supervised cloud detection (wdcd) datasets demonstrate that the proposed method achieves state-of-the-art results. a public reference implementation of this work in pytorch is available at https://github.com/nian-creator/heuristicclouddetection.",AB_0282
"spatiotemporal fusion (stf) can produce synthetic remote sensing data with both high spatial and high temporal resolution by blending the temporally sparse fine resolution data and temporally dense coarse resolution data, for which it has been adopted in many fields, such as agriculture, environment, and ecology. in the literature, moderate resolution imaging spectroradiometer (modis) images are the most frequently employed as the coarse resolution images in stf due to their high quality and availability. however, the modis images contain significant strip noise sometimes, which will lead to unfavorable fusion results. in this letter, a modis strip noise removal strategy for stf (nrstf) is suggested. specifically, the nrstf first removes the strip noise of modis image of the base time utilizing the bands relationship from the base fine resolution image, then reconstructs that of the predicted time via an adaptive convolution-based noise estimation approach. finally, the reconstructed, noise-free modis images are fused with the base fine resolution image to predict the missing fine resolution images. four wildly used stf approaches, including spatial and temporal adaptive reflectance fusion method (starfm), flexible spatiotemporal data fusion (fsdaf) method, fit-fc, and reliable and adaptive spatiotemporal data fusion (rasdf) are selected to test the nrstf on an actual landsat-modis dataset, whose modis images suffers from serious strip noise in two short wave infrared (swir) bands. the experiment results demonstrate that the proposed nrstf can not only effectively eliminate strip noise, but also significantly enhance the accuracy and applicability of the fusion results. the source code of nrstf is available at https://github.com/lijli26/nrstf.git.",AB_0282
"superpixel segmentation has emerged as a prominent approach for simultaneous extraction of spatial-spectral features in hyperspectral imagery, exhibiting considerable efficacy in this domain. although effective in spatial spectrum feature extraction, the existing feature extraction algorithms typically perform superpixel segmentation on a single band, failing to utilize the rich spectral and spatial information available across more bands. moreover, current superpixel feature extraction methods lack scientific guidance for determining optimal multiscale parameters, which can lead to suboptimal segmentation and increased the complexity of hyperspectral analysis. to overcome these limitations, this article presents a novel band-by-band adaptive multiscale superpixel feature extraction (bams-fe) method. the method comprises of two key components: a band-by-band superpixelbased feature extraction method and an adaptive optimal superpixel multiscale determination method. first, the bandby-band superpixel-based feature extraction method performs superpixel segmentation for each band of hyperspectral images (hsis), thereby extracting joint spatial and spectral features. second, the adaptive optimal superpixel multiscale determination method uses an unsupervised approach to determine the optimal multiscale superpixel segmentation parameters. finally, the bams algorithm is obtained by combining the above two algorithms. the proposed algorithm is evaluated on five different datasets, and the results demonstrate its excellent precision and stability. with the top 99% principal components post-pca transformation or with raw, unprocessed hyperspectral datasets, stable and satisfactory classification performance is achieved by bams. additionally, we compared its performance with several other state-of-the-art algorithms and found that it outperformed them in terms of accuracy. our code is publicly available at https://github.com/upcgit/bams-fe.",AB_0282
"although remarkable advances have been achieved in salient object detection (sod) for natural scene images (nsis), sod for optical remote sensing images (rsis) still remains a big challenge due to the unique imaging conditions and various scene patterns. to enable effective sod for rsis, this letter proposes a novel end-to-end network, called attention-aware three-branch network (aatbnet). first, an attention feature encoding (afe) branch is constructed for learning more discriminative features. then, a hierarchical feature decoding (hfd) branch, equipped with three streams, i.e., a decoding stream, a dilated reverse attention (dra) stream, and a fusion dense up-sampling convolution (fduc) stream, is proposed to effectively and robustly compute saliency maps and salient edge maps. third, a two losses computation (tlc) branch is designed to further boost sod performance. comprehensive evaluations of two well-known rsi benchmarks, as well as comparisons with 20 state-of-the-art technologies validate the superiority of our aatbnet. the code of our method is publicly available at: https://github.com/wangxin81/aatbnet",AB_0282
"automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogrammetry and remote sensing, yet has an important role in forest management. in this article, we propose the first semi-supervised transformer-based framework for tree counting which reduces the expensive tree annotations for remote sensing images. our method, termed as treeformer, first develops a pyramid tree representation module based on transformer blocks to extract multiscale features during the encoding stage. contextual attention-based feature fusion (caff) and tree density regressor (tdr) modules are further designed to utilize the robust features from the encoder to estimate tree density maps in the decoder. moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranking losses to utilize unlabeled images in the training process. finally, the tree counter token (tct) is introduced to regulate the network by computing the global tree counts for both labeled and unlabeled images. our model was evaluated on two benchmark tree counting datasets, jiangsu and yosemite, as well as a new dataset, kcl-london, created by ourselves. our treeformer outperforms the state-of-the-art semi-supervised methods under the same setting and exceeds the fully-supervised methods using the same number of labeled images. the codes and datasets are available at https://github.com/haaclassic/treeformer.",AB_0282
"multimodal change detection (mcd) is an increasingly interesting but very challenging topic in remote sensing, which is due to the unavailability of detecting changes by directly comparing multimodal images from different domains. in this article, we first analyze the structural asymmetry between multitemporal images and show their negative impact on the previous mcd methods using image structures. specifically, when there is a structural asymmetry, previous structure-based methods can only complete a structure comparison or image regression in one direction and fail in the other direction; that is, they cannot transform or convert from complex structural images (with more categories) to simple structural images (with fewer categories). to reduce the influence of structural asymmetry, we propose a structural regression fusion (srf)-based method that simultaneously transforms the pre-event and post-event images into the image domain of each other, calculating the forward and backward changed images, respectively. noteworthy, different from previous late fusion methods that fuse the forward and backward changed images in the postprocessing stage, srf incorporates fusion into the regression process, which can fully explore the connection between changed images and, thus, improve image transformation performance and obtain better changed images. specifically, srf yields three types of constraints to perform the fused image transformation: structure consistency-based regression term, change smoothness and alignment-based fusion term, and prior sparsity-based penalty term. finally, the changes can be extracted by comparing the transformed and original images. the proposed srf is verified on six real datasets by comparing with some state-of-the-art (sota) methods. source code of the proposed method will be made available at https://github.com/yulisun/srf.",AB_0282
"studies on 3-d change detection (3dcd) become a research hotspot with the development of 3-d sensors. however, most of the 3dcd works are focused on remote sensing data. in the field of street-level 3-d data, the related works are under investigation. the two main challenges are the lack of pointwise annotated datasets and a universal detection framework. in this article, we proposed an end-to-end point-based network named 3dcdnet and a new dataset named street-level point cloud change detection (slpccd) dataset to deliver street-level 3dcd task. to structure the proposed 3dcdnet, a local feature aggregation (lfa) module and a nearest feature difference (nfd) module are introduced. the lfa is capable of extracting point features and aggregating local information, which is an effective neural module for embedding point cloud features. by stacking multiple lfa blocks, the proposed network can be good at establishing relationships between different points and embedding semantically rich features. different from the cd in images, another crucial point in 3dcd is how to identify changes since point clouds are unstructured data. in order to deliver this challenge, the nfd module is introduced to identify change results using the nearest query operation. extensive experiments were implemented on the manually annotated slpccd dataset as well as another benchmark called urb3dcd to validate the effectiveness and efficiency of the introduced network. it demonstrates that the proposed network outperforms popular existing approaches. the source code and the annotated dataset are available at: https://github.com/wangle53/3dcdnet.",AB_0282
"infrared small target detection (irstd) plays an important role in many military and civilian applications. despite the great advances made by irstd studies in recent years, most of the existing methods have difficulty in balancing detection probabilities and false alarms. moreover, there are only a few public datasets for infrared small targets, which limits the development of irstd research. to address the abovementioned issues, in this article, we propose a robust irstd method that joins multiple pieces of information and noise predictions, named minp-net. specifically, we first design a gradient and contextual information extraction module to extract multiscale features from an input infrared image. second, we construct a noise prediction network to model the background noise. third, we plan a regional positioning branch to provide a coarse target location to decrease the false alarm ratio. in addition, we build a new irstd benchmark to advance the research in this field, named the nchu-seg dataset. to the best of the authors' knowledge, the nchu-seg dataset is the largest real-world scene dataset for evaluating infrared small target segmentation methods. for a comprehensive evaluation, we compare our method with some of the state-of-the-art methods on both the well-known nuaa-sirst dataset and our nchu-seg dataset. the experimental results demonstrate that the proposed minp-net method performs better in terms of detection effectiveness and segmentation accuracy and effectively balances the detection probabilities and false alarms with complex backgrounds. (the code and dataset are available at https://github.com/pcwenyue.)",AB_0282
