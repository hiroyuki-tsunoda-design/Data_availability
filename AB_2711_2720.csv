AB,NO
"land cover change detection (lccd) with bitemporal remote sensing images has been widely used in practical applications. however, when the bitemporal images are multimodal remote sensing images (mrsis) which are acquired with different sensors, the change detection performance may be unsatisfactory, because mrsis cannot be compared directly to generate a change magnitude and obtain a change detection map. here a novel approach is proposed to overcome this problem, i.e., the enhanced unet (e-unet) which learns deep shared features from mrsis to achieve change detection with mrsis. first, a preevent image to postevent image (p2p) transformation module based on classical cycle-consistent generation adversarial network (cgan) is suggested to embed at the head of the proposed e-unet to translate the preevent image to a p2p one. then, multiscale convolutions are added at each encoding layer to capture the various shapes and sizes of ground targets. finally, a polarized self-attention (psa) module is employed before beginning the decoding progress of e-unet with an aim to pay extra attention to changed areas. compared with five typical state-of-the-art methods, experimental results based on two pairs of mrsis well demonstrated the feasibility and advantages of the proposed e-unet for lccd with mrsis in terms of visual observations and quantitative evaluations. for example, the improvement is 4.19% and 4.75% in terms of the overall accuracy for the sardinia dataset and california dataset, respectively. the code of the proposed approach can be found at https://github.com/imgscigroup/e-unet.",AB_0272
"recently, various view synthesis distortion estimation models have been studied to better serve 3-d video coding. however, they can hardly model the relationship quantitatively among different levels of depth changes, texture degeneration, and view synthesis distortion (vsd), which is crucial for rate-distortion optimization and rate allocation. in this paper, an auto-weighted layer representation based view synthesis distortion estimation model is developed. firstly, sub-vsd (s-vsd) is defined according to the level of depth changes and their associated texture degeneration. after that, a set of theoretical derivations demonstrate that the vsd can be approximately decomposed into the s-vsds multiplied by their associated weights. to obtain the s-vsds efficiently, a layer-based representation method is developed, where all the pixels with the same level of depth changes are represented with a layer. it enables the s-vsd calculation at the layer level. meanwhile, a nonlinear mapping function is learnt to accurately represent the relationship between the vsd and s-vsds, automatically providing weights for the s-vsds during vsd estimation. to learn such a function, a dataset of the vsd and its associated s-vsds are built, termed as vsdset. experimental results show that the vsd can be accurately estimated with the weights learnt by the nonlinear mapping function once its associated s-vsds are available. the proposed method outperforms the relevant state-of-the-art methods in both accuracy and efficiency. the vsdset and source code of the proposed method will be available at https://github.com/jianjin008/.",AB_0272
"point cloud segmentation is fundamental in understanding 3d environments. however, most existing methods usually perform poorly on identifying boundaries of touching objects and large surfaces of objects. planes in a scene usually act as supporting surfaces to separate touching objects and provide geometry priors to group points on a large surface as shown in fig. 1. besides, planes can roughly represent the structure of a scene, and are more efficient to encode holistic scene contexts than large scale point clouds. in light of the above advantages, we advise a plane-assisted module, coined 3d-pam, to enhance semantic segmentation of touching objects and large surface objects. 3d-pam consists of a plane separation network (ps-net) and a plane relation network (pr-net). ps-net focuses on learning features that can robustly separate touching objects, e.g., a chair on a floor, as well as capture plane-based geometry priors to group points on a large plane, e.g., points of a desk. pr-net encodes mutual plane relations as a proxy of a scene structure to capture holistic contexts. 3d-pam is designed as a plug-and-play module so that it can be easily plugged into any off-the-shelf semantic segmentation network. extensive experiments demonstrate that the method achieves large segmentation improvements on several backbones, and accomplishes superior results on most categories when using a randla-net backbone (11/13 categories on s3dis dataset and 15/20 categories on scannetv2 dataset). the project is available at github https://github.com/windmillknight/ context- aware- 3dpoint- cloud- semantic-segmentation-with- plane- guidance",AB_0272
"deep learning (dl) networks have demonstrated promising performance in high-resolution remote sensing (rs) image change detection (cd). the transformer can enhance the features and capture the global semantic relations, which has been used to solve the cd problem for high-resolution remote sensing images with good results. however, the depth of the transformer is limited, and the extracted features are not representative, which makes the performance of the cd model unsatisfied. to fix this problem, we propose a siamese network based on multiple attention and multilayer transformers (smarts) for cd in this article. it is a siamese network containing three different modules, which can process bitemporal images in parallel and extract enhanced features at different levels. the first is the feature extraction module. it expresses the features as a certain number of high-order semantic features through the spatial attention module (spam), followed by the calculation of the semantic relations between these high-order semantic features using the transformer encoder, which greatly improves the computational efficiency. the second is the feature enhancement module. it computes global semantic relations with a self-attention module (sfam). the multilayer encoder gets the enhanced features at different levels by computing the relationship between features at each layer. the multilayer decoder refines the bitemporal features of each layer and projects them back to the original space. the third is the fusion module. it uses the ensemble channel attention module (ecam) to elaborate the feature differences at different levels. the proposed smart model has been compared with some state-of-the-art cd methods in three publicly available datasets. the results confirm that smart outperforms state-of-the-art cd methods on several evaluation metrics. our code is available at https://github.com/twj-igg/smart",AB_0272
"although convolutional neural networks (cnns) have shown superior performance to traditional machine learning algorithms for hyperspectral image (hsi) classification tasks, the ability of traditional cnns to model remote dependencies in the spatial orientation of hsis is still limited, and they always extract similar low-level features, leading to feature redundancy. to cope with this limitation, this article proposes a novel multiorder statistical representation-guided graph convolution and continuous context threshold-aware network for the classification of hsis with limited training samples. initially, the spectral-spatial information is separately modeled using first-order features and second-order pooling operators. secondly, we graph-structured the patch features, and by employing a random walk transition probability matrix, the graph-structured convolution can mine more discriminative directional features. in addition, a continuous context threshold-aware network is designed to model multidimensional spatial relationships, which enhances the feature representation of graph features. specifically, the cross-attention mechanism is used to calculate the attention weights in the vertical and horizontal directions, and the features are divided into two levels-important and secondary-by solving the cosine distance between feature vectors; the former is retained and the latter is punished. extensive experiments on multiple hsi datasets demonstrated that the proposed method delivers competitive performance. the code will be available at https://github.com/vivitsai/gsc-ccta.",AB_0272
"automatic extraction of vector polygons of buildings from remotely sensed images is an important but difficult task. recent existing methods based on deep learning usually adopt a multistage solution of semantic segmentation, contour detection, and polygon simplification. such a long processing chain may lead to unreliable results as the boundary regularization and optimization processes are ultimately completed using low-level features, which ignores the potential of deep features in polygon generation. in this article, we present an algorithm for directly extracting simplified polygons of buildings in remotely sensed images. the key of this task is the encoding of the polygon structure. polymapper uses a recurrent neural network (rnn) to produce vertices of a polygon sequentially. due to the limitation of rnn, this approach is unstable and difficult to deal with objects with complex shapes. in this work, we encode the polygon into a tensor representation and use a nonrecurrent manner to recover the polygon structure. in our algorithm, two types of points are used, i.e., the corner point and the connecting point. corner points are used to delineate the building outlines and form the vertices of the final polygon. meanwhile, connecting points are sampled from the edges of the buildings for the assistance of the connection of the corner points. furthermore, we predict the forward and backward directions of each keypoint in a polygon and propose a bidirectional tracing (bd-tracing) strategy for the polygon structure recovery. our approach is simple, effective, and robust. experiments on public datasets demonstrate the superiority of the proposed algorithm. the code is made publicly available at https://github.com/sz94/bldvec.",AB_0272
"recently, the u-shaped networks have been widely explored in remote sensing image dehazing and obtained promising performance. however, most of the existing dehazing methods based on u-shaped framework lack the reconstruction constraints of haze areas, which is particularly important to restore haze-free images. moreover, their encoding and decoding layers cannot effectively fuse multiscale features, resulting in deviations in the color and texture of the dehazing image. to address these issues, in this article, we propose a partial siamese with multiscale bi-codec dehazing network (psmb-net), which is mainly composed of a partial siamese framework (psf) and a multiscale bi-codec information fusion (mbif) module. specifically, the psf is proposed to create dehazing prior information to guide the network to build siamese constraints and achieve improved dehazing results. furthermore, we design an mbif module which can enhance feature extraction (fe), and the multiscale information is used to improve the reconstruction ability of the network for the color and texture of the dehazing image. experimental results on challenging benchmark datasets demonstrate the superiority of our psmb-net over state-of-the-art image dehazing methods. the source code is available at https://github.com/thislzm/psmb-net.",AB_0272
"infrared small-target detection is a challenging task for deep learning-based methods, because targets tend to disappear in the deep layers. to handle this problem, the existing deep neural networks usually apply various dense and skip connections for feature maintenance. although these well-designed networks have achieved good detection performance, the complex network structures reduce their efficiency. in this article, we propose a simple yet efficient network (repisd-net) for infrared small-target detection. the core of our repisd-net is to use different network architectures but equivalent model parameters for training and inference, respectively. specifically, in the training phase, we design a parallel multibranch edge compensation block (ecb) to enhance the local salient features and capture finer contour characteristic of infrared small targets. in the inference phase, the multibranch topology structures are merged into a single branch with only cascaded 3 x 3 convolutions for fast inference. we conduct extensive experiments on several public datasets to validate the effectiveness of our method. experimental results demonstrate that our repisd-net can achieve comparable or even better detection performance with significant acceleration in inference speed as compared with state-of-the-art infrared small-target detection methods. our code is available at https://github.com/dalinlin-wu/repisd-net.",AB_0272
"person re-identification is still a challenging task when moving objects or another person occludes the probe person. mainstream methods based on even partitioning apply an off-the-shelf human semantic parsing to highlight the non-collusion part. in this paper, we apply an attention branch to learn the human semantic partition to avoid misalignment introduced by even partitioning. in detail, we propose a semantic attention branch to learn 5 human semantic maps. we also note that some accessories or belongings, such as a hat, bag, may provide more informative clues to improve the person re-id. human semantic parsing, however, usually treats non-human parts as distractions and discards them. to fetch the missing clues, we design a branch to capture the salient non-human parts. finally, we merge the semantic and saliency attention to build an end-to-end network, named as s-2-net. specifically, to further improve re-id, we develop a trade-off weighting scheme between semantic and saliency attention and set the right weight with the actual scene. the extensive experiments show that s-2-net gets the competitive performance. s-2-net achieves 87.4% map on market1501 and obtains 79.3%/56.1% rank-1/map on msmt17 without semantic supervision. the source codes are available at https://github.com/upgirlnana/s2net.",AB_0272
"alzheimer's disease (ad) is an irreversible neurodegenerative disease, and its incidence increases yearly. because ad patients will have cognitive impairment and personality changes, it has caused a heavy burden on the family and society. image genetics takes the structure and function of the brain as a phenotype and studies the influence of genetic variation on the structure and function of the brain. based on the structural magnetic resonance imaging data and transcriptome data of ad and healthy control samples in the alzheimer's disease neuroimaging disease database, this paper proposed the use of an orthogonal structured sparse canonical correlation analysis for diagnostic information fusion algorithm. the algorithm added structural constraints to the region of interest (roi) of the brain. integrating the diagnostic information of samples can improve the correlation performance between samples. the results showed that the algorithm could extract the correlation between the two modal data and discovered the brain regions most affected by multiple risk genes and their biological significance. in addition, we also verified the diagnostic significance of risk rois and risk genes for ad. the code of the proposed algorithm is available at https://github.com/wanguangyu111/osscca-dif.",AB_0272
