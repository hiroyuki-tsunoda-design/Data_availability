AB,NO
"we focus on multi-step ahead time series forecasting with the multi-output strategy. from the perspective of multi-task learning (mtl), we recognize imbalanced uncertainties between prediction tasks of different future time steps. unexpectedly, trained by the standard summed mean squared error (mse) loss, existing multi-output forecasting models may suffer from performance drops due to the inconsistency between the loss function and the imbalance structure. to address this problem, we reformulate each prediction task as a distinct gaussian mixture model (gmm) and derive a multi-level gaussian mixture loss function to better fit imbalanced uncertainties in multi-output time series forecasting. instead of using the two-step expectation-maximization (em) algorithm, we apply the self-attention mechanism on the task-specific parameters to learn the correlations between different prediction tasks and generate the weight distribution for each gmm component. in this way, our method jointly optimizes the parameters of the forecasting model and the mixture model simultaneously in an end-to-end fashion, avoiding the need of two-step optimization. experiments on three real-world datasets demonstrate the effectiveness of our multi-level gaussian mixture loss compared to models trained with the standard summed mse loss function. all the experimental data and source code are available at https://github.com/smallgum/gmm-fnn.",AB_0237
"unsupervised deformable image registration benefits from progressive network structures such as pyramid and cascade. however, existing progressive networks only consider the single-scale deformation field in each level or stage and ignore the long-term connection across non-adjacent levels or stages. in this paper, we present a novel unsupervised learning approach named self-distilled hierarchical network (sdhnet). by decomposing the registration procedure into several iterations, sdhnet generates hierarchical deformation fields (hdfs) simultaneously in each iteration and connects different iterations utilizing the learned hidden state. specifically, hierarchical features are extracted to generate hdfs through several parallel gated recurrent units, and hdfs are then fused adaptively conditioned on themselves as well as contextual features from the input image. furthermore, different from common unsupervised methods that only apply similarity loss and regularization loss, sdhnet introduces a novel self-deformation distillation scheme. this scheme distills the final deformation field as the teacher guidance, which adds constraints for intermediate deformation fields on deformation-value and deformation-gradient spaces respectively. experiments on five benchmark datasets, including brain mri and liver ct, demonstrate the superior performance of sdhnet over state-of-the-art methods with a faster inference speed and a smaller gpu memory. code is available at https://github.com/blcony/sdhnet.",AB_0237
"increased pericardial adipose tissue (peat) is associated with a series of cardiovascular diseases (cvds) and metabolic syndromes. quantitative analysis of peat by means of image segmentation is of great significance. although cardiovascular magnetic resonance (cmr) has been utilized as a routine method for non-invasive and non-radioactive cvd diagnosis, segmentation of peat in cmr images is challenging and laborious. in practice, no public cmr datasets are available for validating peat automatic segmentation. therefore, we first release a benchmark cmr dataset, mrpeat, which consists of cardiac short axis (sa) cmr images from 50 hypertrophic cardiomyopathy (hcm), 50 acute myocardial infarction (ami), and 50 normal control (nc) subjects. we then propose a deep learning model, named as 3sunet, to segment peat on mrpeat to tackle the challenges that peat is relatively small and diverse and its intensities are hard to distinguish from the background. the 3sunet is a triple-stage network, of which the backbones are all unet. one unet is used to extract a region of interest (roi) for any given image with ventricles and peat being contained completely using a multi-task continual learning strategy. another unet is adopted to segment peat in roi-cropped images. the third unet is utilized to refine peat segmentation accuracy guided by an image adaptive probability map. the proposed model is qualitatively and quantitatively compared with the state-of-the-art models on the dataset. we obtain the peat segmentation results through 3sunet, assess the robustness of 3sunet under different pathological conditions, and identify the imaging indications of peat in cvds. the dataset and all source codes are available at https://dflag-neu.github.io/member/csz/research/.",AB_0237
"the exploration of self-supervised information mining of heterogeneous datasets has gained significant traction in recent years. heterogeneous graph neural networks (hgnns) have emerged as a highly promising method for handling heterogeneous information networks (hins) due to their superior performance. these networks leverage aggregation functions to convert pairwise relations-based features from raw heterogeneous graphs into embedding vectors. however, real-world hins contain valuable higher-order relations that are often overlooked but can provide complementary information. to address this issue, we propose a novel method called self-supervised nodes-hyperedges embedding (snhe), which leverages hypergraph structures to incorporate higher-order information into the embedding process of hins. our method decomposes the raw graph structure into snapshots based on various meta-paths, which are then transformed into hypergraphs to aggregate high-order information within the data and generate embedding representations. given the complexity of hins, we develop a dual self-supervised structure that maximizes mutual information in the enhanced graph data space, guides the overall model update, and reduces redundancy and noise. we evaluate our proposed method on various real-world datasets for node classification and clustering tasks, and compare it against state-of-the-art methods. the experimental results demonstrate the efficacy of our method. our code is available at https://github.com/limengran98/snhe.",AB_0237
"3d semi-supervised medical image segmentation is extremely essential in computer-aided diagnosis, which can reduce the time-consuming task of performing annotation. the challenges with current 3d semi-supervised segmentation algorithms includes the methods, limited attention to volume-wise context information, their inability to generate accurate pseudo labels and a failure to capture important details during data augmentation. this article proposes a dual uncertainty-guided mixing consistency network for accurate 3d semi-supervised segmentation, which can solve the above challenges. the proposed network consists of a contrastive training module which improves the quality of augmented images by retaining the invariance of data augmentation between original data and their augmentations. the dual uncertainty strategy calculates dual uncertainty between two different models to select a more confident area for subsequent segmentation. the mixing volume consistency module that guides the consistency between mixing before and after segmentation for final segmentation, uses dual uncertainty and can fully learn volume-wise context information. results from evaluative experiments on brain tumor and left atrial segmentation shows that the proposed method outperforms state-of-the-art 3d semi-supervised methods as confirmed by quantitative and qualitative analysis on datasets. this effectively demonstrates that this study has the potential to become a medical tool for accurate segmentation. code is available at: https://github.com/yang6277/dumc.",AB_0237
"cancer survival prediction requires exploiting related multimodal information (e.g., pathological, clinical and genomic features, etc.) and it is even more challenging in clinical practices due to the incompleteness of patient's multimodal data. furthermore, existing methods lack sufficient intra- and inter-modal interactions, and suffer from significant performance degradation caused by missing modalities. this manuscript proposes a novel hybrid graph convolutional network, entitled hgcn, which is equipped with an online masked autoencoder paradigm for robust multimodal cancer survival prediction. particularly, we pioneer modeling the patient's multimodal data into flexible and interpretable multimodal graphs with modality-specific preprocessing. hgcn integrates the advantages of graph convolutional networks (gcns) and a hypergraph convolutional network (hcn) through node message passing and a hyperedge mixing mechanism to facilitate intra-modal and inter-modal interactions between multimodal graphs. with hgcn, the potential for multimodal data to create more reliable predictions of patient's survival risk is dramatically increased compared to prior methods. most importantly, to compensate for missing patient modalities in clinical scenarios, we incorporated an online masked autoencoder paradigm into hgcn, which can effectively capture intrinsic dependence between modalities and seamlessly generate missing hyperedges for model inference. extensive experiments and analysis on six cancer cohorts from tcga show that our method significantly outperforms the state-of-the-arts in both complete and missing modal settings. our codes are made available at https://github.com/lin-lcx/hgcn.",AB_0237
"vision transformers have recently set off a new wave in the field of medical image analysis due to their remarkable performance on various computer vision tasks. however, recent hybrid-/transformer-based approaches mainly focus on the benefits of transformers in capturing long-range dependency while ignoring the issues of their daunting computational complexity, high training costs, and redundant dependency. in this paper, we propose to employ adaptive pruning to transformers for medical image segmentation and propose a lightweight and effective hybrid network apformer. to our best knowledge, this is the first work on transformer pruning for medical image analysis tasks. the key features of apformer are self-regularized self-attention (ssa) to improve the convergence of dependency establishment, gaussian-prior relative position embedding (grpe) to foster the learning of position information, and adaptive pruning to eliminate redundant computations and perception information. specifically, ssa and grpe consider the well-converged dependency distribution and the gaussian heatmap distribution separately as the prior knowledge of self-attention and position embedding to ease the training of transformers and lay a solid foundation for the following pruning operation. then, adaptive transformer pruning, both query-wise and dependency-wise, is performed by adjusting the gate control parameters for both complexity reduction and performance improvement. extensive experiments on two widely-used datasets demonstrate the prominent segmentation performance of apformer against the state-of-the-art methods with much fewer parameters and lower gflops. more importantly, we prove, through ablation studies, that adaptive pruning can work as a plug-n-play module for performance improvement on other hybrid-/transformer-based methods. code is available at https://github.com/xianlin7/apformer.",AB_0237
"semi-supervised learning (ssl) has demonstrated remarkable advances on medical image classification, by harvesting beneficial knowledge from abundant unlabeled samples. the pseudo labeling dominates current ssl approaches, however, it suffers from intrinsic biases within the process. in this paper, we retrospect the pseudo labeling and identify three hierarchical biases: perception bias, selection bias and confirmation bias, at feature extraction, pseudo label selection and momentum optimization stages, respectively. in this regard, we propose a hierarchical bias mitigation (habit) framework to amend these biases, which consists of three customized modules including mutual reconciliation network (mrnet), recalibrated feature compensation (rfc) and consistency-aware momentum heredity (cmh). firstly, in the feature extraction, mrnet is devised to jointly utilize convolution and permutator-based paths with a mutual information transfer module to exchanges features and reconcile spatial perception bias for better representations. to address pseudo label selection bias, rfc adaptively recalibrates the strong and weak augmented distributions to be a rational discrepancy and augments features for minority categories to achieve the balanced training. finally, in the momentum optimization stage, in order to reduce the confirmation bias, cmh models the consistency among different sample augmentations into network updating process to improve the dependability of the model. extensive experiments on three semi-supervised medical image classification datasets demonstrate that habit mitigates three biases and achieves state-of-the-art performance. our codes are available at https://github.com/cityu-aim-group/habit.",AB_0237
"we present an unsupervised domain adaptation method for image segmentation which aligns high-order statistics, computed for the source and target domains, encoding domain-invariant spatial relationships between segmentation classes. our method first estimates the joint distribution of predictions for pairs of pixels whose relative position corresponds to a given spatial displacement. domain adaptation is then achieved by aligning the joint distributions of source and target images, computed for a set of displacements. two enhancements of this method are proposed. the first one uses an efficient multi-scale strategy that enables capturing long-range relationships in the statistics. the second one extends the joint distribution alignment loss to features in intermediate layers of the network by computing their cross-correlation. we test our method on the task of unpaired multi-modal cardiac segmentation using the multi-modality whole heart segmentation challenge dataset and prostate segmentation task where images from two datasets are taken as data in different domains. our results show the advantages of our method compared to recent approaches for cross-domain image segmentation. code is available at https://github.com/wangping521/domain_adaptation_shape_prior.",AB_0237
"3d point clouds have found a wide variety of applications in multimedia processing, remote sensing, and scientific computing. although most point cloud processing systems are developed to improve viewer experiences, little work has been dedicated to perceptual quality assessment of 3d point clouds. in this work, we build a new 3d point cloud database, namely the waterloo point cloud (wpc) database. in contrast to existing datasets consisting of small-scale and low-quality source content of constrained viewing angles, the wpc database contains 20 high quality, realistic, and omni-directional source point clouds and 740 diversely distorted point clouds. we carry out a subjective quality assessment experiment over the database in a controlled lab environment. our statistical analysis suggests that existing objective point cloud quality assessment (pcqa) models only achieve limited success in predicting subjective quality ratings. we propose a novel objective pcqa model based on an attention mechanism and a variant of information content-weighted structural similarity, which significantly outperforms existing pcqa models. the database has been made publicly available at https://github.com/qdushl/waterloo-point-cloud-database.",AB_0237
