AB,NO
"automatic segmentation of intrahepatic bile ducts and common bile ducts plays an essential role in interventional surgery for cholangiocarcinoma, directly related to the success rate of the operation. however, the large shape and appearance variances make it challenging to segment bile ducts, especially for 3d ct images. in this study, we propose a transformer-based axial guided network, dubbed tagnet, to automatically segment intrahepatic and common bile ducts by exploiting intraand inter-slice context modeling. the pivot is to take advantage of cnn-transformer hybrid architecture to simultaneously explore local and global contextual information from multiple adjacent slices. especially a novel slice-axial-attention transformer module is imposed at multi-scales concurrently to capture the intraand inter-slice feature representations along each direction, boosting long-distance contextual modeling while limiting the computation cost. moreover, a slice guided consistency loss function is advanced to enforce anatomical prior consistency among adjacent slices in a semi-supervised manner, thus enhancing the spatial topology of bile duct segmentation. extensive experimental results on an in-house bile ducts ct dataset demonstrate that our method is capable of achieving promising performance, which achieves at least a 4.5% improvement in dice and a reduction of 1.5 in hd95 than other state-of-the-art methods, indicating its potential for automated intrahepatic and common bile duct segmentation. we have made our code publicly available via https://github.com/zephyrize/tagnet.",AB_0240
"unsupervised feature selection is an important machine learning task and thus attracts increasingly more attention. however, due to the absence of labels, unsupervised feature selection often suffers from stability and robustness problems. to tackle these problems, some works try to ensemble multiple feature selection results to obtain a consensus result. most of the existing methods do the ensemble on the feature level, i.e., they directly ensemble feature selection results by feature ranking or voting aggregation, without paying any attention to the following downstream tasks. in this paper, we take clustering as the downstream task and wish to ensemble the base results to select features which are appropriate for clustering. to this end, we propose a novel bi-level feature selection ensemble method, which ensembles on two levels: the feature level and the clustering level. together with feature level ensemble, we also learn a consensus clustering result from base feature selection results with self-paced learning. then, we apply the consensus clustering result to guide the feature selection in turn. extensive experiments are conducted to demonstrate that the proposed method outperforms other state-of-the-art feature selection and feature selection ensemble methods in the clustering task. the codes of this paper are released in https://doctor-nobody.github.io/codes/blfse.zip.",AB_0240
"major depressive disorder (mdd) is a severe mental disorder, in recent years, in order to help psychologists in clinical diagnosis, many precision medicine methods based on artificial intelligence have been proposed. this paper proposes a depression detection method based on image data that introduces frequency attention. in response to the problem of existing methods that use auxiliary information to improve performance but significantly increase computational costs, this method first applies adaptive gamma correction and deblurring data enhancement to the original data based on three universal characteristics of image data (information sparsity, inter-domain difference, and infinite granularity). next, in accordance with the frequency richness characteristic unique to depression images, a transformer network with frequency attention is introduced for training. finally, a pre-trained fine-tuning training strategy is used to address the problem of overfitting due to the small size of the dataset. this method achieved an rmse of 7.36 and an mae of 5.97 on the avec2013 dataset and 7.23 rmse and 5.85 mae on the avec2014 dataset. sufficient experiments have shown that the combination of data enhancement and frequency attention in image-based depression detection is effective and promising. code for our work can be accessed at https://github.com/daofaziran911/work01.",AB_0240
"exploring reliable correspondences in a given putative set is a fundamental task in twoview geometry estimation. the random sample consensus (ransac) method is a widely used estimator. it typically searches inliers within the putative correspondences initialized by the local similarity of the descriptors. however, ransac may be inefficient when actual inliers are heavily contaminated by mismatches. in this study, we attempt to identify true inliers from heavily contaminated two-view correspondences and propose a parallel core sample consensus (csac) method based on gradient difference. csac employs the gradient difference between two images as a globalmetric to compensate for the locality of the typical initialization. first, a pool of errors is constructed in parallel based on the gradient differences of the pixels between a pair of correspondences. for four keypoints of two correspondences, the gradients of the pixels on the line between two keypoints in each image are calculated. the error of the two correspondences is the average difference between the two resulting gradient serials. second, a core set is constructed using the correspondences with the topk smallest errors in the pool. subsequently, csac searches the inliers in the input set via parallel testing of the minimal sets sampled in the core set. finally, post-processing refines the resulting inliers based on neighborhood preservation. experiments comparing seven state-ofthe-art methods are conducted on eight publicly available datasets. the experimental results indicate that csac outperforms the other competing methods in terms of inlier precision and model accuracy. the source code is available at https://github.com/xintaoding/ csac.",AB_0240
"to tackle the challenge of vehicle re-identification (re-id) in complex lighting environments and diverse scenes, multi-spectral sources like visible and infrared information are taken into consideration due to their excellent complementary advantages. however, multi-spectral vehicle re-id suffers cross-modality discrepancy caused by heterogeneous properties of different modalities as well as a big challenge of the diverse appearance with different views in each identity. meanwhile, diverse environmental interference leads to heavy sample distributional discrepancy in each modality. in this work, we propose a novel cross-directional consistency network (ccnet) to simultaneously overcome the discrepancies from both modality and sample aspects. in particular, we design a new cross-directional center loss (������������������������) to pull the modality centers of each identity close to mitigate cross-modality discrepancy, while the sample centers of each identity close to alleviate the sample discrepancy. such a strategy can generate discriminative multi-spectral feature representations for vehicle re id. in addition, we design an adaptive layer normalization unit (alnu) to dynamically adjust individual feature distribution to handle distributional discrepancy of intra-modality features for robust learning. to provide a comprehensive evaluation platform, we create a high-quality rgb-nir-tir multi-spectral vehicle re-id benchmark (msvr310), including 310 different vehicles from a broad range of viewpoints, time spans and environmental complexities. comprehensive experiments on both created and public datasets demonstrate the effectiveness of the proposed approach comparing to the state-of-the-art methods. the dataset and code will be released for free academic usage at https://github.com/superlollipop123/cross-directional-center-networkand-msvr310.",AB_0240
"semantic segmentation for extracting buildings and roads from uncrewed aerial vehicle (uav) remote sensing images by deep learning becomes a more efficient and convenient method than traditional manual segmentation in surveying and mapping fields. in order to make the model lightweight and improve the model accuracy, a lightweight network using object attention (loanet) for buildings and roads from uav aerial remote sensing images is proposed. the proposed network adopts an encoder-decoder architecture in which a lightweight densely connected network (ldcnet) is developed as the encoder. in the decoder part, the dual multi-scale context modules which consist of the atrous spatial pyramid pooling module (aspp) and the object attention module (oam) are designed to capture more context information from feature maps of uav remote sensing images. between aspp and oam, a feature pyramid network (fpn) module is used to fuse multi-scale features extracted from aspp. a private dataset of remote sensing images taken by uav which contains 2431 training sets, 945 validation sets, and 475 test sets is constructed. the proposed basic model performs well on this dataset, with only 1.4m parameters and 5.48g floating point operations (flops), achieving excellent mean intersection-over-union (miou). further experiments on the publicly available loveda and city-osm datasets have been conducted to further validate the effectiveness of the proposed basic and large model, and outstanding miou results have been achieved. all codes are available on https://github.com/gtlinyer/loanet.",AB_0240
"medical images are generally acquired with limited field-of-view (fov), which could lead to incomplete regions of interest (roi), and thus impose a great challenge on medical image analysis. this is particularly evident for the learning-based multi-target landmark detection, where algorithms could be misleading to learn primarily the variation of background due to the varying fov, failing the detection of targets. based on learning a navigation policy, instead of predicting targets directly, reinforcement learning (rl)-based methods have the potential to tackle this challenge in an efficient manner. inspired by this, in this work we propose a multi -agent rl framework for simultaneous multi-target landmark detection. this framework is aimed to learn from incomplete or (and) complete images to form an implicit knowledge of global structure, which is consolidated during the training stage for the detection of targets from either complete or incomplete test images. to further explicitly exploit the global structural information from incomplete images, we propose to embed a shape model into the rl process. with this prior knowledge, the proposed rl model can not only localize dozens of targets simultaneously, but also work effectively and robustly in the presence of incomplete images. we validated the applicability and efficacy of the proposed method on various multi-target detection tasks with incomplete images from practical clinics, using body dual-energy x-ray absorptiometry (dxa), cardiac mri and head ct datasets. results showed that our method could predict whole set of landmarks with incomplete training images up to 80% missing proportion (average distance error 2.29 cm on body dxa), and could detect unseen landmarks in regions with missing image information outside fov of target images (average distance error 6.84 mm on 3d half-head ct). our code will be released via https://zmiclab.github.io/projects.html.",AB_0240
"the task-related component analysis (trca) is a data-driven method for extracting reproducible components across multiple data segments from multivariate data. trca has been proven effective in enhancing the signal-to-noise ratio of neuroimaging data in previous studies. however, its original form requires a computational cost of o(n2) to compute the sum of cross-covariance matrices, indicating that the computational time increases rapidly as the number of data segments n increases. this study aims to empirically validate that the reformulated form of trca can reduce its computational complexity. this study estimated that the reformulation can reduce the computational complexity from o(n2) to o(n) without distorting its outputs by reducing the number of matrix multiplications in computing the sum of cross-covariance matrices. a series of computer simulations using computer-generated synthetic, functional near-infrared spectroscopy (fnirs), and electroencephalogram (eeg) data were conducted to verify the theoretical computation complexity and consistency of the outputs between the original and reformulated trca. the computation reduction was validated through an experimental comparison of the original and reformulated trca with synthetic and real data of various sizes (e.g., data segments, dimensions, and lengths). in addition, the two algorithms output almost exactly identical results with only floating-point errors. the reformulation would considerably benefit practical applications, especially when applying the trca to large-scale computations. the code is available at https://github.com/mnakanishi/trca-ssvep.",AB_0240
"background & aims: gut immaturity leads to feeding difficulties in very preterm infants (<32 weeks gestation at birth). maternal milk (mm) is the optimal diet but often absent or insufficient. we hypothesized that bovine colostrum (bc), rich in protein and bioactive components, improves enteral feeding progression, relative to preterm formula (pf), when supplemented to mm. aim of the study is to determine whether bc supplementation to mm during the first 14 days of life shortens the time to full enteral feeding (120 ml/kg/d, tff120).methods: this was a multicenter, randomized, controlled trial at seven hospitals in south china without access to human donor milk and with slow feeding progression. infants were randomly assigned to receive bc or pf when mm was insufficient. volume of bc was restricted by recommended protein intake (4-4.5 g/kg/d). primary outcome was tff120. feeding intolerance, growth, morbidities and blood parameters were recorded to assess safety. results: a total of 350 infants were recruited. bc supplementation had no effect on tff120 in intentionto-treat analysis [n (bc) = 171, n (pf) = 179; adjusted hazard ratio, ahr: 0.82 (95% ci: 0.64, 1.06); p = 0.13]. body growth and morbidities did not differ, but more cases of periventricular leukomalacia were detected in the infants fed bc (5/155 vs. 0/181, p = 0.06). blood chemistry and hematology data were similar between the intervention groups.conclusions: bc supplementation during the first two weeks of life did not reduce tff120 and had only marginal effects on clinical variables. clinical effects of bc supplementation on very preterm infants in the first weeks of life may depend on feeding regimen and remaining milk diet. trial registration: http:// www.clinicaltrials.gov: nct03085277. & copy; 2023 the author(s). published by elsevier ltd. this is an open access article under the cc by license ().",AB_0240
"studies have shown that the mechanism of action of many drugs is related to mirna. in-depth research on the relationship between mirna and drugs can provide theoretical foundations and practical approaches for various areas, such as drug target discovery, drug repositioning and biomarker research. traditional biological experiments to test mirna-drug susceptibility are costly and time-consuming. thus, sequence-or topology-based deep learning methods are recognized in this field for their efficiency and accuracy. however, these methods have limitations in dealing with sparse topologies and higher-order information of mirna (drug) feature. in this work, we propose gcfmcl, a model for multi-view contrastive learning based on graph collaborative filtering. to the best of our knowledge, this is the first attempt that incorporates contrastive learning strategy into the graph collaborative filtering framework to predict the sensitivity relationships between mirna and drug. the proposed multi-view contrastive learning method is divided into topological contrastive objective and feature contrastive objective: (1) for the homogeneous neighbors of the topological graph, we propose a novel topological contrastive learning method via constructing the contrastive target through the topological neighborhood information of nodes. (2) the proposed model obtains feature contrastive targets from high-order feature information according to the correlation of node features, and mines potential neighborhood relationships in the feature space. the proposed multi-view comparative learning effectively alleviates the impact of heterogeneous node noise and graph data sparsity in graph collaborative filtering, and significantly enhances the performance of the model. our study employs a dataset derived from the noncorna and ncdr databases, encompassing 2049 experimentally validated mirna-drug sensitivity associations. five-fold cross-validation shows that the area under the curve (auc), area under the precision-recall curve (aupr) and f1-score (f1) of gcfmcl reach 95.28%, 95.66% and 89.77%, which outperforms the state-of-the-art (sota) method by the margin of 2.73%, 3.42% and 4.96%, respectively. our code and data can be accessed at https://github.com/kkkayle/gcfmcl.",AB_0240
