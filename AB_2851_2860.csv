AB,NO
"recently, pansharpening methods based on deep learning (dl) have achieved state-of-the-art results. however, current existing dl-based pansharpening methods need to be trained repetitively for different satellite sensors to obtain satisfactory fusion performance and therefore require a large number of training images for each satellite. to deal with these issues, in this article, we propose a unified two-stage spatial and spectral network (utsn) for pansharpening. a branch of networks is constructed for each different satellite, in which the spatial enhancement network (sen) is shared to improve the spatial details in the fused images from different satellites. a spectral adjustment network (san) is employed to capture the spectral characteristics of the specific satellite. through san, the spectral information in the intermediate image from sen is refined to produce the final fusion results. such a framework can integrate the datasets from different satellites together for sufficient training of sen. the proposed method is able to achieve promising pansharpening results also for a new satellite with limited training images by only learning a new san on the few-shot datasets due to the simple but efficient structure of san. the experimental results show that the proposed method can produce state-of-the-art fusion results in both the standard and few-shot cases. the source code is publicly available at https://github.com/rsmagneto/utsn.",AB_0286
"a precise and robust classification of land cover is crucial for land use estimation. a robust model that can provide rich semantic information is imperative for the challenging task of land cover classification (lcc) in foggy conditions. we propose semantic representation enhancement (sre) and semantic representation aggregation (sra) modules for the fusion of semantic representation. the dense depthwise separable atrous spatial pyramid pooling (dds-aspp) module in sre possesses a large receptive field, which covers an extensive scale range. an enhanced asymmetric convolution module (eacm) in sre focuses on features of various directions. dds-aspp and eacm generate the class-based and pixel-based representation, respectively. by means of sra and dual representations, we model the relationship between global context and coarse class regions to capture long-range correlation. moreover, evaluated on potsdam, vaihingen, and custom real-world datasets under fog, we demonstrate that our work is competitive with state-of-the-art models in terms of robustness. code will be available at https://github.com/bowenroom/robust-land-cover-classification.",AB_0286
"oriented object detectors have suffered severely from the discontinuous boundary problem for a long time. in this work, we ingeniously avoid this problem by relating regression outputs to regression target orientations. the core idea of our method is to build a contour function which imports orientations and outputs the corresponding distance predictions. inspired by fourier transformations, we assume this function can be represented as a linear combination of trigonometric functions and fourier series. we replace the final 4-d layer in the regression branch of fully convolutional one-stage object detector (fcos) with a fourier series transformation (fst) module and term this new network fcosf. by this unique design, the regression outputs in fcosf can adaptively vary according to the regression target orientations. thus, the discontinuous boundary has no impact on our fcosf. more importantly, fcosf avoids building complicated oriented box representations, which usually cause extra computations and ambiguities. with only flipping augmentation and single-scale training and testing, fcosf with resnet-50 achieves 73.64% mean average precision (map) on the dota-v1.0 dataset with up to 23.6-frames/s speed, surpassing all one-stage oriented object detectors. on the more challenging dota-v2.0 dataset, fcosf also achieves the highest results of 51.75% map among one-stage detectors. more experiments on dior-r and hrsc2016 are also conducted to verify the robustness of fcosf. code and models will be available at https://github.com/ddgrcf/fcosf.",AB_0286
"for people who ardently love painting but unfortunately have visual impairments, holding a paintbrush to create a work is a very difficult task. people in this special group are eager to pick up the paintbrush, like leonardo da vinci, to create and make full use of their own talents. therefore, to maximally bridge this gap, we propose a painting navigation system called angle's eyes to assist blind people in artistic creation. the proposed system is composed of cognitive system and guidance system. the system adopts drawing board positioning based on qr code, brush navigation based on target detection and bush real-time positioning. meanwhile, we design a simple yet efficient position information coding rule to remind the user of the current brush tip position. in addition, we design a criterion to efficiently judge whether the brush reaches the target or not. the numerous experiments are conducted to optimize and test the performance of the system. the results of real-world scenario experiments demonstrate that the developed system has great potential to help blind people with painting. this work also demonstrates that it is practicable for the blind people to feel the world through the brush in their hands. in the future, we plan to deploy angle's eyes on the phone to make it more portable. the demo video of the proposed painting navigation system is available at https://doi.org/10.6084/m9.figshare.9760004.v1.",AB_0286
"cross-view geo-localization is a critical task in various applications, such as smart city management and disaster monitoring. current methods typically divide a satellite image into patches and use these patches to identify the geographic location of a query image. however, these methods can only provide the location of an image rather than the location of a specific object of interest. this makes it difficult to link these methods to geodatabases to obtain detailed information about a target object, such as its name and construction time. to overcome this limitation, we propose a novel problem of cross-view object geo-localization (cvogl) in a local region with high-resolution satellite images. this problem includes two main challenges: accurately identifying the location of an object and distinguishing the target object from others in satellite images. to address these challenges, we present a new detection-based geo-localization method called detgeo, which consists of an object detection-based framework with a two-branch encoder and a query-aware cross-view fusion module. detgeo uses cross-view images as input to the detector to provide object-level geo-localization. the fusion module employs cross-view spatial attention to focus on relevant areas of target objects during cross-view feature fusion. to evaluate our method, we constructed a new cvogl dataset, which comprises ground- or drone-view images as query images and satellite-view images as geo-tagged reference images. comprehensive experiments are conducted to demonstrate the effectiveness of our method on cvogl (https://github.com/sunyuxi/detgeo).",AB_0286
"in recent years, deep-learning-based hyperspectral image (hsi) classification methods have achieved significant development and gradually become widely applied. the existing advanced methods can achieve near-saturation performance with sufficient labels in a closed-set environment (cse); i.e., training set and test set are all known categories of ground objects. however, the real world is usually open because of the diversity of land covers; i.e., test set exists unknown categories that are not labeled in the training set. therefore, the prevalent advanced cse methods still cannot effectively and robustly handle unknown categories of ground objects in an open-set environment (ose). therefore, we propose a spectral-spatial multiple layer perceptron (mlp)-like network with reciprocal points learning (ssmlp-rpl) to improve the performance of open-set hsi classification. first, a feature learning framework based on rpl is constructed to model the extra-category space and reduce the risk of open space. the learned feature space enables to enlarge the distance between the known and unknown categories. besides, we further propose to utilize a learnable dynamic threshold of each known category to effectively distinguish the unknown categories and improve open performance of the model. second, to enhance the capacity of feature learning, an ssmlp-like network is designed to capture the spectral-spatial feature merely with a series of fully connected (fc) layers, which mainly involve spefc and spafc two modules. among them, the spafc module enables to model spacial semantics, and the spefc module enables to model long-distance spectral dependence. extensive experiments on three benchmark hsis show that ssmlp-rpl has a competitive performance both in cse and ose and even surpasses currently advanced closed-set and open-set hsi classification methods. as an end-to-end hsi classification framework of mlp backbone, ssmlp network can compete with the advanced works based on cnn and transformer. the code will be open at https://github.com/sssssyf/ssmlp-rpl.",AB_0286
"deep learning (dl) based methods, such as the representative vision transformer (vit) and convolutional neural network (cnn) structures, can characterize spatial-spectral features of hyperspectral images (hsis) well and achieve outstanding classification performance. nevertheless, when land cover is complex, the intraclass spectral consistency may be weak and difficult to express effectively in the original data space, leading to potential bias regarding the validity of spatial-spectral information utilization. we propose a new method gspformer that first constructs a global spectral projection space (gsps) to generate land cover more robust representations and enhance the spectral consistency in local neighborhoods. after that, a space aggregation idea is introduced to obtain the central pixel's more abundant spectral feature expression for better classification by fusing all spectral features in the local neighborhood. extensive experiments are conducted on various hsi datasets for evaluating the classification performance of gspformer and other state-of-the-art networks. comparison results indicate the superiority of the proposed method not only in classification accuracy but also in the number of parameters and convergence. the code of gspformer will be found at https://github.com/preston-dong/ gspformer.",AB_0286
"3d perception of objects is critical for many real-world applications, such as autonomous cars and robots. among them, most state-of-the-art (sota) 3d perception systems are based on deep learning models. recently, the research community found that 3d object classifiers on point cloud based on deep learning are easily fooled by adversarial point cloud craft by attackers. to overcome this, adversarial defenses are considered the most effective ways to improve the robustness of deep learning models, and most adversarial defenses on point cloud are focused on input transformation. however, all previous defense methods decrease the natural accuracy, and the nature of the point cloud classifiers itself has been overlooked. to this end, in this paper, we propose a novel adversarial defense for 3d point cloud classifiers that makes full use of the nature of the point cloud classifiers. due to the disorder of point cloud, all point cloud classifiers have the property of permutation invariant to the input point cloud. based on this nature, we design invariant transformations defense (it-defense). we show that, even after accounting for obfuscated gradients, our it-defense is a resilient defense against sota 3d attacks. moreover, it-defense does not hurt clean accuracy compared to previous sota 3d defenses. our code will be available at: https://github.com/cuge1995/ it-defense.",AB_0286
"unsupervised person re-identification (reid), including fully unsupervised reid and unsupervised domain adaptive reid, remains a challenge for the fields of biometrics and computer vision due to its difficulty in learning with unlabeled target domain data. existing state-of-the-art methods, most of which generate pseudo-labels via unsupervised clustering for model optimization, are inevitably hampered by the under-explored problem of pseudo-label noise. motivated by this, we propose a novel joint framework termed pseudo-label noise prevention, suppression, and softening (npss) for unsupervised person re-identification. instead of refining generated label noise after clustering as many existing methods do, we start solving this issue from the source of pseudo-label noise by proposing a new dynamic camera-adaptive clustering (dcac), which dynamically involves camera information to prevent noise caused by cross-camera variance, thus improving their quality during clustering. moreover, we propose an online domain union (odu) mechanism for the classification model learning on the target domain via involving source domain data with their ground-truth labels, which effectively suppresses the indelible noisy pseudo-labels. furthermore, we present the self-consistency constraint (scc) to soften the label noise in a single model with reduced computation and network parameter cost, which achieves intra-sample knowledge ensembling with our global-local scc and cross-sample knowledge ensembling with our inter-instance scc. experiments demonstrate the effectiveness of our method as it surpasses state-of-the-art methods by a large margin on market-1501, dukemtmc-reid, and msmt17 benchmarks. the code is available at https://github.com/hjwang-824/npss.",AB_0286
"fine-grained oriented object recognition (fgo(2)r) is a practical need for intellectually interpreting remote sensing images. it aims at realizing fine-grained classification and precise localization with oriented bounding boxes, simultaneously. our considerations for the task are general but decisive: 1) the extraction of subtle differences carries a big weight in differentiating fine-grained classes and 2) oriented localization prefers rotation-sensitive features. in this article, we propose a network with separate feature refinement (sfrnet), in which two transformer-based branches are designed to perform function specific feature refinement for fine-grained classification and oriented localization, separately. to highlight the discriminative information advantageous to fine-grained classification, we propose a spatial and channel transformer (sc-former) to capture both the long-range spatial interactions and the key correlations hidden in the feature channels. besides, we design a multi region of interest (roi) loss (mrl) following the protocol of deep metric learning to enhance the separability of finegrained classes further. for oriented localization, we integrate the oriented response convolution with the transformer structure (namely, or-former) to assist in encoding rotation information during regression. extensive experimental results validate the effectiveness and robustness of our sfrnet. without bells and whistles, our sfrnet achieves the state-of-the-art performance on the large-scale fair1m datasets (fair1m-1.0 and fair1m-2.0). code will be available at https://github.com/ranchosky/sfrnet.",AB_0286
