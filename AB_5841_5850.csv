AB,NO
"robots operating in everyday environments need to effectively perceive, model, and infer semantic properties of objects. existing knowledge reasoning frameworks only model binary relations between an object's class label and its semantic properties, unable to collectively reason about object properties detected by different perception algorithms and grounded in diverse sensory modalities. we bridge the gap between multimodal perception and knowledge reasoning by introducing an n-ary representation that models complex, inter-related object properties. to tackle the problem of collecting n-ary semantic knowledge at scale, we propose transformer neural networks that generalize knowledge from observations of object instances by learning to predict single missing properties or predict joint probabilities of all properties. the learned models can reason at different levels of abstraction, effectively predicting unknown properties of objects in different environmental contexts given different amounts of observed information. we quantitatively validate our approach against prior methods on link, a unique dataset we contribute that contains 1457 object instances in different situations, amounting to 15multimodal properties types and 200 total properties. compared to the top-performing baseline, amarkov logic network, our models obtain a 10% improvement in predicting unknown properties of novel object instances while reducing training and inference time by more than 150 times. additionally, we apply our work to a mobile manipulation robot, demonstrating its ability to leverage n-ary reasoning to retrieve objects and actively detect object properties. the code and data are available at https://github.com/ wliu88/ link.",AB_0585
"in high-dimensional data analysis, the bi-level (or the sparse group) variable selection can simultaneously conduct penalization on the group level and within groups, which has been developed for continuous, binary, and survival responses in the literature. zhou et al. (2022) (pmid: 35766061) has further extended it under the longitudinal response by proposing a quadratic inference function-based penalization method in gene-environment interaction studies. this study introduces springer, an r package implementing the bi-level variable selection within the qif framework developed in zhou et al. (2022). in addition, r package springer has also implemented the generalized estimating equation-based sparse group penalization method. alternative methods focusing only on the group level or individual level have also been provided by the package. in this study, we have systematically introduced the longitudinal penalization methods implemented in the springer package. we demonstrate the usage of the core and supporting functions, which is followed by the numerical examples and discussions. r package springer is available at https://cran.r-project.org/ package=springer.",AB_0585
"measuring microbial diversity is traditionally based on microbe taxonomy. here, in contrast, we aimed to quantify heterogeneity in microbial gene content across 14,183 metagenomic samples spanning 17 ecologies, including 6 human associated, 7 nonhuman host associated, and 4 in other nonhuman host environments. in total, we identified 117,629,181 nonredundant genes. the vast majority of genes (66%) occurred in only one sample (i.e., singletons). in contrast, we found 1,864 sequences present in every metagenome, but not necessarily every bacterial genome. additionally, we report data sets of other ecology-associated genes (e.g., abundant in only gut ecosystems) and simultaneously demonstrated that prior microbiome gene catalogs are both incomplete and inaccurately cluster microbial genetic life (e.g., at gene sequence identities that are too restrictive). we provide our results and the sets of environmentally differentiating genes described above at http://www.microbial-genes.bio.",AB_0585
"in this article, we demonstrate the benefits of imposing stability on data-driven koopman operators. the data-driven identification of stable koopman operators (disko) is implemented using an algorithm [1] that computes the nearest stable matrix solution to a least-squares reconstruction error. as a first result, we derive a formula that describes the prediction error of koopman representations for an arbitrary number of time steps, and which shows that stability constraints can improve the predictive accuracy over long horizons. as a second result, we determine formal conditions on basis functions of koopman operators needed to satisfy the stability properties of an underlying nonlinear system. as a third result, we derive formal conditions for constructing lyapunov functions for nonlinear systems out of stable data-driven koopman operators, which we use to verify stabilizing control from data. finally, we demonstrate the benefits of disko in prediction and control with simulations using a pendulum and a quadrotor and experiments with a pusher-slider system. the paper is complemented with a video: https://sites.google.com/view/learning-stable-koopman.",AB_0585
"phase-field modeling is a popular front-tracking approach used to model solidification. its time-evolution equations are often coupled to alloy composition and/or thermal diffusion in high-resolution multiphysics approaches. materials thermodynamic properties tabulated in calphad databases can be used for phase-field modeling to parameterize bulk energies of alloys. in addition, they can be naturally integrated into models such as the kim-kim-suzuki (kks) model where driving forces depend on the differences between chemical potentials of co-existing phases. in that case, a small system of coupled nonlinear equations needs to be solved at every point in space where the phase-field order parameter is to be updated and evolved in time. we present thermo4pfm, a solver for the kks equations for binary and ternary alloys, with two or three phases, and parameterized with calphad models. thermo4pfm is open source, written in c++, and can take advantage of graphics processing units (gpu) accelerators. using openmp offload capabilities for c++ classes, an excellent performance is demonstrated on gpu using the llvm compiler. calphad data is read from simple json files using an open source parser from the boost library.program summaryprogram title: thermo4pfm cpc library link to program files: https://doi .org /10 .17632 /8j3ntp5c7k.1developer's repository link: https://github .com /ornl /thermo4pfmlicensing provisions: bsd 3-clauseprogramming language: c++/openmpnature of problem: accurate modeling of solidification in metallic alloys requires thermodynamic data associated with possible material phases. that data can be used in phase-field modeling (pfm) to evaluate the driving force responsible for phase changes and solidification front motion. integrating that data into pfm requires the solution of a small system of coupled nonlinear differential equations - the kim-kim -suzuki equations - that needs to be solved at every point of a discretization mesh. solution method: thermo4pfm implements a newton-based solver for the kim-kim-suzuki equations for a few special cases (binary and ternary alloys with two or three phases) for calphad-based models of the bulk energy of the phases. the software is written in c++ and uses the curiously recurring template pattern and openmp offload capabilities to take advantage of gpu accelerators, when available, on modern high performance computing resources. it also uses the boost property tree library to parse input calphad data.(c) 2023 elsevier b.v. all rights reserved.",AB_0585
"we present three deep learning sequence-based prediction models for peptide properties including hemolysis, solubility, and resistance to nonspecific interactions that achieve comparable results to the state-of-the-art models. our sequence-based solubility predictor, mahlool, outperforms the current state-of-the-art methods for short peptides. these models are implemented as a static website without the use of a dedicated server or cloud computing. web-based models like this allow for accessible and effective reproducibility. most existing approaches rely on third-party servers that typically require upkeep and maintenance. our predictive models do not require servers, require no installation of dependencies, and work across a range of devices. the specific architecture is bidirectional recurrent neural networks. this serverless approach is a demonstration of edge machine learning that removes the dependence on cloud providers. the code and models are accessible at https://github.com/ur-whitelab/peptide-dashboard.",AB_0585
"background: skeletal muscle is a large and clinically relevant body component that has been difficult and impractical to quantify outside of specialized facilities. advances in smartphone technology now provide the opportunity to quantify multiple body surface dimensions such as circumferences, lengths, surface areas, and volumes.objectives: this study aimed to test the hypothesis that anthropometric body measurements acquired with a smartphone application can be used to accurately estimate an adult's level of muscularity.methods: appendicular lean mass (alm) measured by dxa served as the reference for muscularity in a sample of 322 adults. participants also had digital anthropometric dimensions (circumferences, lengths, and regional and total body surface areas and volumes) quantified with a 20-camera 3d imaging system. least absolute shrinkage and selection operator (lasso) regression procedures were used to develop the alm prediction equations in a portion of the sample, and these models were tested in the remainder of the sample. then, the accuracy of the prediction models was cross-validated in a second independent sample of 53 adults who underwent alm estimation by dxa and the same digital anthropometric estimates acquired with a smartphone application. results: lasso models included multiple significant demographic and 3d digital anthropometric predictor variables. evaluation of the models in the testing sample indicated respective rmses in women and men of 1.56 kg and 1.53 kg and r2's of 0.74 and 0.90, respectively. cross-validation of the lasso models in the smartphone application group yielded rmses in women and men of 1.78 kg and 1.50 kg and r2's of 0.79 and 0.95; no significant differences or bias between measured and predicted alm values were observed.conclusions: smartphone image capture capabilities combined with device software applications can now provide accurate renditions of the adult muscularity phenotype outside of specialized laboratory facilities. am j clin nutr 2023;x:xx. this trial was registered at clinicaltrials.gov as nct03637855 (https://clinicaltrials.gov/ct2/show/nct03637855), nct05217524 (https://clinicaltr ials.gov/ct2/show/nct05217524), and nct03771417 (https://clinicaltrials.gov/ct2/show/nct03771417).",AB_0585
"background: recent 3-dimensional optical (3do) imaging advancements have provided more accessible, affordable, and self-operating opportunities for assessing body composition. 3do is accurate and precise in clinical measures made by dxa. however, the sensitivity for monitoring body composition change over time with 3do body shape imaging is unknown. objectives: this study aimed to evaluate the ability of 3do in monitoring body composition changes across multiple intervention studies. methods: a retrospective analysis was performed using intervention studies on healthy adults that were complimentary to the cross-sectional study, shape up! adults. each participant received a dxa (hologic discovery/a system) and 3do (fit3d proscanner) scan at the baseline and follow-up. 3do meshes were digitally registered and reposed using meshcapade to standardize the vertices and pose. using an established statistical shape model, each 3do mesh was transformed into principal components, which were used to predict whole-body and regional body composition values using published equations. body composition changes (follow-up minus the baseline) were compared with those of dxa using a linear regression analysis. results: the analysis included 133 participants (45 females) in 6 studies. the mean (sd) length of follow-up was 13 (5) wk (range: 3-23 wk). agreement between 3do and dxa (r2) for changes in total fm, total ffm, and appendicular lean mass were 0.86, 0.73, and 0.70, with root mean squared errors (rmses) of 1.98 kg, 1.58 kg, and 0.37 kg, in females and 0.75, 0.75, and 0.52 with rmses of 2.31 kg, 1.77 kg, and 0.52 kg, in males, respectively. further adjustment with demographic descriptors improved the 3do change agreement to changes observed with dxa. conclusions: compared with dxa, 3do was highly sensitive in detecting body shape changes over time. the 3do method was sensitive enough to detect even small changes in body composition during intervention studies. the safety and accessibility of 3do allows users to self-monitor on a frequent basis throughout interventions. this trial was registered at clinicaltrials.gov as nct03637855 (shape up! adults; https://clinicaltrials.gov/ct2/show/nct03637855); nct03394664 (macronutrients and body fat accumulation: a mechanistic feeding study; https://clinicaltrials.gov/ct2/show/nct03394664); nct03771417 (resis-tance exercise and low-intensity physical activity breaks in sedentary time to improve muscle and cardiometabolic health; https://clinicaltrials.gov/c t2/show/nct03771417); nct03393195 (time restricted eating on weight loss; https://clinicaltrials.gov/ct2/show/nct03393195), and nct04120363 (trial of testosterone undecanoate for optimizing performance during military operations; https://clinicaltrials.gov/ct2/show/nct04120363).",AB_0585
"detecting semantically similar binary functions - a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics - requires understanding function behaviors and intentions. this task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. most existing approaches match functions based on syntactic features without understanding the functions' execution semantics. we present trex, a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functions' traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. while it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ml model approximate execution semantics of diverse instructions and their compositions. we thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. we then finetune the pretrained model to match semantically similar functions. we evaluate trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, arm, and mips), and with 4 optimizations (o0-o3) and 5 obfuscations. trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8x faster. ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. our case studies demonstrate the practical use-cases of trex - on 180 real-world firmware images, trex uncovers 14 vulnerabilities not disclosed by previous studies. we release the code and dataset of trex at https://github.com/cumlsec/trex.",AB_0585
"context: machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc.). motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. while those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. objective: we aim to better detect and mitigate algorithmic discrimination in machine learning software problems. method: here we propose fairmask, a model-based extrapolation method that is capable of both mitigating bias and explaining the cause. in our fairmask approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). we then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. results: the experiments of this paper show that, without compromising (original) model performance, fairmask can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. moreover, compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. conclusion: algorithmic decision bias can be removed via extrapolation that corrects the misleading latent correlation between the protected attributes and other non-protected ones. as evidence for this, our proposed fairmask is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms. reproduction package: in order to better support open science, all scripts and data used in this study are available online at https://github.com/anonymous12138/biasmitigation.",AB_0585
