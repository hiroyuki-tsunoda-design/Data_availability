AB,NO
"with the continuous development of deep learning in the field of image generation models, a large number of vivid forged faces have been generated and spread on the internet. these high-authenticity artifacts could grow into a threat to society security. existing face forgery detection methods directly utilize the obtained public shared or centralized data for training but ignore the personal privacy and security issues when personal data couldn't be centralizedly shared in real-world scenarios. additionally, different distributions caused by diverse artifact types would further bring adverse influences on the forgery detection task. to solve the mentioned problems, the paper proposes a novel generalized residual federated learning for face forgery detection (fedforgery). the designed variational autoencoder aims to learn robust discriminative residual feature maps to detect forgery faces (with diverse or even unknown artifact types). furthermore, the general federated learning strategy is introduced to construct distributed detection model trained collaboratively with multiple local decentralized devices, which could further boost the representation generalization. experiments conducted on publicly available face forgery detection datasets prove the superior performance of the proposed fedforgery. the designed novel generalized face forgery detection protocols and source code would be publicly available at https://github.com/gang370/fedforgery.",AB_0283
"human perception systems can integrate audio and visual information automatically to obtain a profound understanding of real-world events. accordingly, fusing audio and visual contents is important to solve the audio-visual event (ave) localization problem. although most existing works have fused audio and visual modalities to explore their relationship with attention-based networks, we can delve into their relationship more deeply to improve the fusion capability of the two modalities. in this paper, we propose a dense modality interaction network (dmin) to elegantly leverage audio and visual information by integrating two novel modules, namely, the audio-guided triplet attention (agta) module and the dense inter-modality attention (dima) module. the agta module enables audio information to guide the network to pay more attention to event-relevant visual regions. this guidance is conducted in the channel, temporal, and spatial dimensions, which emphasize informative features, temporal relationships and spatial regions, to boost the capacity of representations. furthermore, the dima module establishes the dense-relationship between audio and visual modalities. specifically, the dima module leverages the information of all channel pairs of audio and visual features to formulate the cross-modality attention weight, which is superior to the multi-head attention module that uses limited information. moreover, a novel unimodal discrimination loss (udl) is introduced to exploit the unimodal and fused features together for more exact ave localization. the experimental results show that our method is remarkably superior to the state-of-the-art methods in fully- and weakly-supervised ave settings. to further evaluate the model's ability to build audio-visual connections, we design a dense cross modality relation network (dcmr) to solve the cross-modality localization task. dcmr is a simple deformation of a dmin, and the experimental results further illustrate that dima can explore denser relationships between the two modalities. code is available at https://github.com/weizequan/dmin.git.",AB_0283
"despite much research progress in synthetic aperture radar (sar) object detection, the performance of sar object detection has encountered a bottleneck limited by the imaging mechanism of sar. in this work, we investigate how to perform robust sar object detection by distilling the category knowledge from optical images in the training stage. to this end, we propose a novel knowledge distillation (kd) method called category-oriented localization distillation (cold), which employs the optical object detection network as the teacher to guide the sar object detection network. to introduce the category prior knowledge of the teacher network in the localization knowledge transferring, a category-oriented partition module (cpm) is designed in cold to decouple candidate bounding boxes into target and nontarget ones according to the category information in optical images. through box decoupling, the accuracy and efficiency of sar object detection can be significantly improved. moreover, an intersection over union (iou)-based weighting module (iwm) is introduced in cold to guide the student network focusing more on high-quality candidate boxes by adaptively changing the weight of each candidate bounding box based on the corresponding iou score in the teacher network. in addition, a unified benchmark dataset is created for the evaluation of optical information-guided sar object detection, which consists of 14 665 optical and sar image pairs in the training set and 3666 sar images in the testing set. extensive experiments on the dataset demonstrate the effectiveness of our cold against state-of-the-art methods. the dataset is available at: https://github.com/mmic-lcl/datasets-and-benchmark-code.",AB_0283
"the hyperspectral image (hsi) classification aims to assign each pixel to a land-cover category. it is receiving increasing attention from both industry and academia. the main challenge lies in capturing reliable and informative spatial and spectral dependencies concealed in the hsi for each class. to address the challenge, we propose a spatial-spectral 1dswin (ss1dswin) transformer with groupwise feature tokenization for hsi classification. specifically, we reveal local and hierarchical spatial-spectral relationships from two different perspectives. it mainly consists of a groupwise feature tokenization module (gftm) and a 1dswin transformer with cross-block normalized connection module (tcncm). for gftm, we reorganize an image patch into overlapping cubes and further generate groupwise token embeddings with multihead self-attention (msa) to learn the local spatial-spectral relationship along the spatial dimension. for tcncm, we adopt the shifted windowing strategy when acquiring the hierarchical spatial-spectral relationship along the spectral dimension with 1-d window-based msa (1dw-msa) and 1-d shifted window-based msa (1dsw-msa) and leverage cross-block normalized connection (cnc) to adaptively fuse the feature maps from different blocks. in ss1dswin, we apply these two modules in order and predict the class label for each pixel. to test the effectiveness of the proposed method, extensive experiments are conducted on four hsi datasets, and the results indicate that ss1dswin outperforms several current state-of-the-art methods. the source code of the proposed method is available at https://github.com/minato252/ss1dswin.",AB_0283
"recent efforts have witnessed remarkable progress in satellite video super-resolution (svsr). however, most svsr methods usually assume the degradation is fixed and known, e.g., bicubic downsampling, which makes them vulnerable in real-world scenes with multiple and unknown degradations. to alleviate this issue, blind sr has, thus, become a research hotspot. nevertheless, the existing approaches are mainly engaged in blur kernel estimation while losing sight of another critical aspect for vsr tasks: temporal compensation, especially compensating for blurry and smooth pixels with vital sharpness from severely degraded satellite videos. therefore, this article proposes a practical blind svsr algorithm (bsvsr) to explore more sharp cues by considering the pixelwise blur levels in a coarse-to-fine manner. specifically, we employed multiscale deformable (msd) convolution to coarsely aggregate the temporal redundancy into adjacent frames by window-slid progressive fusion. then, the adjacent features are finely merged into mid-feature using deformable attention (da), which measures the blur levels of pixels and assigns more weights to the informative pixels, thus inspiring the representation of sharpness. moreover, we devise a pyramid spatial transformation (pst) module to adjust the solution space of sharp mid-feature, resulting in flexible feature adaptation in multilevel domains. quantitative and qualitative evaluations on both simulated and real-world satellite videos demonstrate that our bsvsr performs favorably against state-of-the-art nonblind and blind sr models. code will be available at https://github.com/xy-boy/blind-satellite-vsr.",AB_0283
"optical and synthetic aperture radar (sar) image keypoint detection is an important foundation for multimodal remote sensing image matching. the influence of nonlinear radiometric differences and geometric deformation between optical and sar images leads to low repeatability of existing keypoint detection methods. to address the problem that existing keypoint detection methods cannot provide the required homonymous points for heterogeneous image matching, we propose a keypoint detection method [multilevel attention siamese network for keypoint detection in optical and sar images (skd-net)] and improve it in terms of both network structure and network optimization. first, we propose a multilevel attention siamese network, which is composed of multiple convolutional modules and transformer modules with shared weights to extract common features at different levels for keypoint detection. we introduce a transformer module in the keypoint detection pipeline and fuse shallow and deep features to obtain more spatial and rich semantic information to facilitate heterogeneous image keypoint detection. then, to ensure that the detected keypoints have more homonymous points and localization accuracy, we propose a position-consistent loss. unlike previous loss functions, our designed position-consistent loss function takes the differences between heterogeneous image score maps into account, and it autonomously selects the optimized correct point pairs to enable the network to perform correct learning. finally, extensive experiments show that our detection method outperforms the current state-of-the-art keypoint detection methods in terms of repeatability, localization accuracy, and matching performance. our source code is available at https://github.com/zhangschen/skd-net.",AB_0283
"in recent years, hyperspectral image change detection (hsi-cd) based on deep learning has achieved high detection accuracy, but these methods obtain excellent detection results usually rely on having sufficient labeled samples to train the network. however, the production of hsi labels is difficult, costly, and inefficient. in practical tasks, often only a limited number of labeled samples can be obtained due to the limitation of timeliness. to address this problem, a cross-heterogeneous domain few-shot learning framework based on bidirectional generation (big-fslf) is proposed for hsi-cd, which aims to solve the few-shot problem of hsi-cd by few-shot learning (fsl) and to assist hsi-fsl to perform better by obtaining learnable changed information (i.e., empirical knowledge) from another remote sensing data. specifically, a multitask generation encoder (mlgene) is designed to take on both the tasks of fsl and domain adaptation to achieve hsi-cd under the condition of cross-heterogeneous domain few-shot. first, we take any pair of image data in a very high-resolution image (vhri) cd dataset as the source domain and hsi is used as the target domain, using sufficient labeled samples in the source domain and a small number of labeled samples in the target domain for fsl. meanwhile, a bidirectional generation domain adaptation (bigda) method based on a generative adversarial strategy is proposed to achieve adaptive alignment of the two heterogeneous domains (source and target domains) feature distributions, to mitigate the impact of the domain shift problem inherent to cross-domain data on fsl. abundant experiments with only five training samples on the publicly available popular hsi-cd datasets confirm that the proposed method can show great detection performance. the source code of the proposed framework will be released at https://github.com/lsylnnu/big-fslf.",AB_0283
"high spatial resolution (hsr) remote sensing (rs) images inevitably pose the challenge of multiscale transformation, as small objects, such as cars and helicopters (hcs), may occupy only a few pixel points. this incurs a significant hurdle for global context modeling, particularly in backbone networks with large downsampling coefficients. simple summation or concatenation techniques, such as skip connections, fail to address semantic gaps and even impose negative impacts on multiscale feature fusion. meanwhile, due to the complexity of foreground objects, the boundary details of hsr rs images are easy to lose in sampling operations. to overcome these challenges, we propose a multiscale channelwise cross attention network (mccanet) assisted by boundary supervision (bs). technically, mcca captures the channel attention (ca) with various scales, which allows dynamic and adaptive feature fusion in a contextual scale-aware manner and focuses on both large and small objects distributed throughout the inputs. besides, a channel and context strainer (ccs) module is proposed and embedded in mcca, filtering channels and contexts for the mitigation of intraclass differences. in addition, we apply a bs module to recover boundary contour, avoiding the blurring effect during the construction of contextual information. the refined boundary allows for the effective recognition of surrounding pixels, ensuring a better segmentation performance. extensive experiments on the instance segmentation in aerial images dataset (isaid), international society for photogrammetry and remote sensing (isprs) potsdam, and land-cover domain adaptive (loveda) datasets demonstrate that our proposed mccanet achieves a good balance of high accuracy and efficiency. code will be available at: https://github.com/zhengjianwei2/mccanet.",AB_0283
"land cover mapping based on multispectral images can, in principle, be considered an application of semantic segmentation, but land cover mapping inputs include near-infrared (nir) data in addition to rgb data. it has been experimentally found that lulc mapping performance based on rgb data alone is better than that based on rgb and nir data when using some established single-branch encoder-decoder models. to address this issue, we propose a dual-branch encoder-decoder (dbed) framework that can be applied to existing encoder-decoder models for semantic segmentation. first, the multispectral input data is divided into two parts: rgb and nir and fed to the respective branch for encoding. the dual-branch structure facilitates cross-modal complementary information encoding without deteriorating the original rgb modality-specific feature extraction. second, an attention module named the multispectral attention module (msam) is proposed to mine the contextual correlation between the multispectral feature maps, leading to further performance boosting. we apply this framework to three mainstream semantic segmentation models and validate it on the gaofen image dataset (gid). experimental results show that this structure brings performance improvements. the source code of dbed is publicly available at https://github.com/malignuscn/dbed.",AB_0283
"lightweight object detection and recognition models are extremely crucial for in-orbit applications, which is the most critical factor for whether deep learning-based object detection and recognition algorithms can be applied to remote sensing satellites for real-time or near real-time processing. global information is extremely important for object detection and recognition of remote sensing images. however, due to the high computational cost, the existing cnn-based lightweight models over-emphasize the extraction of local information, while ignoring the global information. for this reason, we propose a lightweight object detection and recognition model lightweight global-local detection (lgldet) based on the especially light global modeling structure. in lgldet, a light global-local module (lglm) is proposed to extract the global and local information. the lglm consists of point2patch non-local (p2pnl), local branch, and skip connection. specifically, p2pnl is proposed to reduce the computation of global long-range dependency modeling. in addition, the feature fusion part and detection head are also designed in a lightweight way. in the experiments, the proposed method can achieve optimal performance with fewer parameters and lower computational complexity than existing cnn-based lightweight models and transformer-based lightweight models with similar parameters or computational complexity. the code will be released on the site of https://github.com/dyl96/lgldet.",AB_0283
