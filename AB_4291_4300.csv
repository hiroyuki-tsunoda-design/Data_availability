AB,NO
"we present the software modinterv as an informatics tool to monitor, in an automated and userfriendly manner, the evolution and trend of covid-19 epidemic curves, both for cases and deaths. the modinterv software uses parametric generalized growth models, together with lowess regression analysis, to fit epidemic curves with multiple waves of infections for countries around the world as well as for states and cities in brazil and the usa. the software automatically accesses publicly available covid-19 databases maintained by the johns hopkins university (for countries as well as states and cities in the usa) and the federal university of vicosa (for states and cities in brazil). the richness of the implemented models lies in the possibility of quantitatively and reliably detecting the distinct acceleration regimes of the disease. we describe the backend structure of software as well as its practical use. the software helps the user not only to understand the current stage of the epidemic in a chosen location but also to make short term predictions as to how the curves may evolve. the app is freely available on the internet (http://fisica.ufpr.br/modinterv), thus making a sophisticated mathematical analysis of epidemic data readily accessible to any interested user. (c) 2023 elsevier b.v. all rights reserved.",AB_0430
"splicing is one of the most important post-transcriptional pro-cessing systems and is responsible for the generation of tran-scriptome diversity in all living eukaryotes. splicing is regulated by the spliceosome machinery, which is responsible for each step of primary rna processing. however, current molecules and stages involved in rna splicing are still spread over different studies. thus, a curated atlas of spliceosome-related molecules and all involved stages during rna processing can provide all researchers with a reliable resource to better investigate this important mechanism. here, we present iara (website access: https://pucpr-bioinformatics.github.io/atlas/), an extensively curated and constantly updated catalog of molecules involved in spliceosome machinery. iara has a map of the steps involved in the human splicing mechanism, and it allows a detailed overview of the molecules involved throughout the distinct steps of splicing.",AB_0430
"fast charging is considered a promising protocol for raising the charging efficiency of electric vehicles. however, high currents applied to lithium-ion (li-ion) batteries inevitably accelerate the degradation and shorten their lifetime. this work designs a multi-step fast-charging method to extend the lifetime of lini0.5co0.2mn0.3o2 (nmc)/graphite li-ion batteries based on the studies of half cells and investigates the aging mechanisms for different charging methods. the degradation has been studied from both full cell behaviour and materials perspectives through a combination of non-destructive diagnostic methods and post-mortem analysis. in the proposed multi-step charging protocol, the state-of-charge (soc) profile is subdivided into five ranges, and the charging current is set differently for different soc ranges. one of the designed multi-step fast charging protocols is shown to allow for a 200 full equivalent cycles longer lifetime as compared to the standard charging method, while the charging time is reduced by 20%. from the incremental capacity analysis and electrical impedance spectroscopy, the loss of active materials and lithium inventory on the electrodes, as well as an increase in internal resistance for the designed multi-step constant-current-constant-voltage (mcccv) protocol have been found to be significantly lower than for the standard charging method. post-mortem analysis shows that cells aged by the designed mcccv fast charging protocol exhibit less graphite exfoliation and crystallization damage, as well as a reduced solid electrolyte interphase (sei) layer growth on the anode, leading to a lower rsei resistance and extended lifetime.(c) 2023 science press and dalian institute of chemical physics, chinese academy of sciences. published by elsevier b.v. and science press. this is an open access article under the cc by license (http://creati-vecommons.org/licenses/by/4.0/).",AB_0430
"the correct evaluation of ligand binding free energies by computational methods is still a very challenging active area of research. the most employed methods for these calculations can be roughly classified into four groups: (i) the fastest and less accurate methods, such as molecular docking, designed to sample a large number of mole-cules and rapidly rank them according to the potential binding energy; (ii) the second class of methods use a thermodynamic ensemble, typically generated by molecular dynamics, to analyze the endpoints of the ther-modynamic cycle for binding and extract differences, in the so-called 'end-point' methods; (iii) the third class of methods is based on the zwanzig relationship and computes the free energy difference after a chemical change of the system (alchemical methods); and (iv) methods based on biased simulations, such as metadynamics, for example. these methods require increased computational power and as expected, result in increased accuracy for the determination of the strength of binding. here, we describe an intermediate approach, based on the monte carlo recursion (mcr) method first developed by harold scheraga. in this method, the system is sampled at increasing effective temperatures, and the free energy of the system is assessed from a series of terms w(b, t), computed from monte carlo (mc) averages at each iteration. we show the application of the mcr for ligand binding with datasets of guest-hosts systems (n = 75) and we observed that a good correlation is obtained be-tween experimental data and the binding energies computed with mcr. we also compared the experimental data with an end-point calculation from equilibrium monte carlo calculations that allowed us to conclude that the lower-energy (lower-temperature) terms in the calculation are the most relevant to the estimation of the binding energies, resulting in similar correlations between mcr and mc data and the experimental values. on the other hand, the mcr method provides a reasonable view of the binding energy funnel, with possible connections with the ligand binding kinetics, as well. the codes developed for this analysis are publicly available on github as a part of the libela/mclibela project (https://github.com/alessandronascimento/libela).",AB_0430
"background: to our knowledge, there is no parkinson's disease (pd) gait biomechanics data sets available to the public.objective: this study aimed to create a public data set of 26 idiopathic individuals with pd who walked overground on on and off medication.materials and methods: their upper extremity, trunk, lower extremity, and pelvis kinematics were measured using a three-dimensional motion-capture system (raptor-4; motion analysis). the external forces were collected using force plates. the results include raw and processed kinematic and kinetic data in c3d and ascii files in different file formats. in addition, a metadata file containing demographic, anthropometric, and clinical data is provided. the following clinical scales were employed: unified parkinson's disease rating scale motor aspects of experiences of daily living and motor score, hoehn & yahr, new freezing of gait questionnaire, montreal cognitive assessment, mini balance evaluation systems tests, fall efficacy scale-international-fes-i, stroop test, and trail making test a and b.results: all data are available at figshare (https://figshare.com/articles/dataset/ a_dataset_of_overground_walking_full-body_kinematics_and_kinetics_in_ individuals_with_parkinson_s_disease/14896881).conclusion: this is the first public data set containing a three-dimensional full body gait analysis of individuals with pd under the on and off medication. it is expected to contribute so that different research groups worldwide have access to reference data and a better understanding of the effects of medication on gait.",AB_0430
"spreadsheets are widely used for data exploration. since spreadsheet systems have limited capabilities, users often need to load spreadsheets to other data science environments to perform advanced analytics. however, current approaches for spreadsheet loading suffer from either high runtime or memory usage, which hinders data exploration on commodity systems. to make spreadsheet loading practical on commodity systems, we introduce a novel parser that minimizes memory usage by tightly coupling decompression and parsing. furthermore, to reduce the runtime, we introduce optimized spreadsheet-specific parsing routines and employ parallelism. to evaluate our approach, we implement prototypes for loading excel spreadsheets into r and python environments. our evaluation shows that our novel approach is up to 3x faster while consuming up to 40x less memory than state-of-the-art approaches.artifact availability: the source code is available at https://github.com/fhenz/sheetreader-r. (c) 2023 elsevier ltd. all rights reserved.",AB_0430
"the determination of accurate equilibrium dissociation constants, kd, of protein-small molecule complexes is important but challenging as all established methods have inherent sources of inaccuracy. accurate constant via transient incomplete separation (actis) is a new method for kd determination using transient incomplete separation of the complex from the unbound small molecule in a pressure-driven flow inside a capillary. actis is accurate, and its accuracy is invariant to variations in geometries of both the fluidic system and the flow. furthermore, actis is implemented using a simple fluidic system supporting its accuracy and providing a simple-to-follow/copy template for instrumentation. despite the simple and robust instrumentation/acquisition, the current data processing workflow is cumbersome, time consuming, and prone to hard-to-trace human errors therefore hindering actis' ability to become a practical reference method for kd determination. this technical note describes a streamlined workflow for processing actis data; the workflow is implemented as a set of open-source software tools called practised (https://github.com/practisedprogram/practised). these tools allow all steps of data processing to be performed in a fast and straightforward fashion. these practical software tools complement the simple instrumentation serving both developers and users of actis.",AB_0430
"electrochemical impedance spectroscopy (eis) is an effective technique for lithium-ion battery state of health diagnosis, and the impedance spectrum prediction by battery charging curve is expected to enable battery impedance testing during vehicle operation. however, the mechanistic relationship between charging curves and impedance spectrum remains unclear, which hinders the development as well as optimization of eis-based prediction techniques. in this paper, we predicted the impedance spectrum by the battery charging voltage curve and optimized the input based on electrochemical mechanistic analysis and machine learning. the internal electrochemical relationships between the charging curve, incremental capacity curve, and the impedance spectrum are explored, which improves the physical interpretability for this prediction and helps define the proper partial voltage range for the input for machine learning models. different machine learning algorithms have been adopted for the verification of the proposed framework based on the sequence-to-sequence predictions. in addition, the predictions with different partial voltage ranges, at different state of charge, and with different training data ratio are evaluated to prove the proposed method have high generalization and robustness. the experimental results show that the proper partial voltage range has high accuracy and converges to the findings of the electrochemical analysis. the predicted errors for impedance spectrum are less than 1.9 mo with the proper partial voltage range selected by the corelative analysis of the electrochemical reactions inside the batteries. even with the voltage range reduced to 3.65-3.75 v, the predictions are still reliable with most rmses less than 4 mo.(c) 2023 science press and dalian institute of chemical physics, chinese academy of sciences. published by elsevier b.v. and science press. this is an open access article under the cc by license (http://creati-vecommons.org/licenses/by/4.0/).",AB_0430
"calorimeters play an important role in high-energy physics experiments. their design includes electronic instrumentation, signal processing chain, computing infrastructure, and also a good understanding of their response to particle showers produced by the interaction of incoming particles. this is usually supported by full simulation frameworks developed for specific experiments so that their access is restricted to the collaboration members only. such restrictions limit the general-purpose developments that aim to propose innovative approaches to signal processing, which may include machine learning and advanced stochastic signal processing models. this work presents the lorenzetti showers, a general-purpose framework that mainly targets supporting novel signal reconstruction and triggering strategies using segmented calorimeter information. this framework fully incorporates developments down to the signal processing chain level (signal shaping, energy estimation, and noise mitigation techniques) to allow advanced signal processing approaches in modern calorimetry and triggering systems. the developed framework is flexible enough to be extended in different directions. for instance, it can become a tool for the phenomenology community to go beyond the usual detector design and physics process generation approaches. program summary program title: lorenzetti showers cpc library link to program files: https://doi .org /10 .17632 /sy64367452 .1 developer's repository link: https://github .com /lorenzetti -hep /lorenzetti licensing provisions: gplv3 programming language: python, c++. nature of problem: in experimental high-energy physics, simulation is essential for experiment preparation, design and interpretations of ongoing acquisitions. especially for calorimeters, an accurate simulation that can describe detector geometry, behavior to different physics processes and signal generation close to the readout electronics and data acquisition levels is required to properly develop signal processing and computational methods. such detectors may face very challenging demands arising from the new designs, such as pileup mitigation and noise reduction tasks under unprecedented levels. in this sense, simulation requirements continuously increase in complexity and performance, because new physics searches require large datasets and accurate modeling to experimental effects. solution method: the lorenzetti showers is an integrated software framework that provides complete calorimeter information close enough to the electronic readout chain. thus, the proposed framework allows users to access cell readout values, configurable sensor pulse-shapes, crosstalk modeling, and different energy estimation methods. it aims at supporting designs that target low or high pileup operation conditions in an easy-to-use modular structure. the developed framework is based on pythia 8 (particle generation) and geant4 (interactions with the calorimeter technique under analysis). an efficient data recording structure was used to allow full access to the lorenzetti showers outputs. in summary, the lorenzetti showers tool provides to the scientific community a user-friendly, flexible, user-oriented, and low-level calorimeter simulation framework. additional comments including restrictions and unusual features: the framework current version provides the implementation of a generic segmented calorimeter (electromagnetic and hadronic sections), which may be modified by the user, if desired. it allows the generation of particles interactions using pythia 8 (native) or any generator compatible with the hepmc format (which may be integrated using an external input file) and propagation through a user-configurable calorimeter using geant4.(c) 2023 elsevier b.v. all rights reserved.",AB_0430
"microfluidics is a technology that enables moving analytic processes from expensive and bulky laboratory equipment to small-scale devices. microfluidic devices, usually in the form of labs-on-a-chips (locs), have found many great applications in medicine, biology, and chemistry. in particular, locs that utilize channels to transport fluids or droplets between different components on the chip are a promising technology. however, the design process of such channel-based locs is in need of further automation efforts since the underlying design steps are still rather complex and conducted mainly by hand. an important task in microfluidic design automation is the so-called channel routing, where components on locs are connected by microfluidic channels. methods that aim to automate this routing task must factor in the specific demands of microfluidic devices. common requirements for microfluidic routing layouts are to prevent sharp channel bends and to realize a particular length of channels. unfortunately, most of the available routing algorithms address these requirements only partly and insufficiently. in this work, we propose a router that is able to overcome these shortcomings and allows automatic channel routing with a minimal bending radius as well as a desired length. in order to make the router accessible to users with little to no design automation expertise, the solution is implemented as an online tool with a user-friendly and intuitive interface. the resulting tool can be accessed at https://www.cda.cit.tum.de/research/microfluidics/channel_router/.",AB_0430
