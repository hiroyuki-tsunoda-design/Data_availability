AB,NO
"the goal of this work is to reconstruct 3d dogs from monocular images. we take a model-based approach, where we estimate the shape and pose parameters of a 3d articulated shape model for dogs. we consider dogs as they constitute a challenging problem, given they are highly articulated and come in a variety of shapes and appearances. recent work has considered a similar task using the multi-animal smal model, with additional limb scale parameters, obtaining reconstructions that are limited in terms of realism. like previous work, we observe that the original smal model is not expressive enough to represent dogs of many different breeds. moreover, we make the hypothesis that the supervision signal used to train the network, that is 2d keypoints and silhouettes, is not sufficient to learn a regressor that can distinguish between the large variety of dog breeds. we therefore go beyond previous work in two important ways. first, we modify the smal shape space to be more appropriate for representing dog shape. second, we formulate novel losses that exploit information about dog breeds. in particular, we exploit the fact that dogs of the same breed have similar body shapes. we formulate a novel breed similarity loss, consisting of two parts: one term is a triplet loss, that encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. the second one is a breed classification loss. with our approach we obtain 3d dogs that, compared to previous work, are quantitatively better in terms of 2d reconstruction, and significantly better according to subjective and quantitative 3d evaluations. our work shows that a-priori side information about similarity of shape and appearance, as provided by breed labels, can help to compensate for the lack of 3d training data. this concept may be applicable to other animal species or groups of species. we call our method barc (breed-augmented regression using classification). our code is publicly available for research purposes at https:// barc.is.tue.mpg.de/.",AB_0495
"conservation biology faces the challenge of safeguarding the ecosystem functions and ecological processes (the water cycle, nutrients, energy flow, and community dynamics) that sustain the multiple facets of biodiversity. characterization and evaluation of these processes and functions can be carried out through functional attributes or traits related to the exchanges of matter and energy between vegetation and the atmosphere. based on this principle, satellite imagery can provide integrative spatiotemporal characterizations of ecosystem functions at local to global scales. here, we provide a multitemporal dataset at protected-area level that characterizes the spatial patterns and temporal dynamics of ecosystem functioning in the biosphere reserve of the sierra nevada (spain), captured through the spectral enhanced vegetation index (evi, using product mod13q1.006 from the modis sensor) from 2001 to 2018. the database contains, at the annual scale, a synthetic map of ecosystem functional type (eft) classes from three ecosystem functional attributes (efas): (i) descriptors of annual primary production, (ii) seasonality, and (iii) phenology of carbon gains. it also includes two ecosystem functional-diversity indices derived from the above datasets: (i) eft richness and (ii) eft rarity. finally, it provides interannual summaries for all previously mentioned variables, i.e., their long-term means and interannual variability. the datasets are available at two open-source sites (pangaea: ; cazorla et al., 2020a; interannual summaries at http://obsnev.es/apps/efts_sn.html, last access: 17 april 2023). this dataset provides scientists, environmental managers, and the public in general with valuable information on the first characterization of ecosystem functional diversity based on primary production developed in the sierra nevada, a biodiversity hotspot in the mediterranean basin and an exceptional natural laboratory for ecological research within the long-term social-ecological research (lter) network.",AB_0495
"e-rna is a collection of web-servers for the predic-tion and visualisation of rna secondary structures and their functional features, including in particular rna-rna interactions. in this updated version, we have added novel tools for rna secondary struc-ture prediction and have significantly updated the visualisation functionality. the new method cobold can identify transient rna structure features and their potential functional effects on a known rna structure during co-transcriptional structure forma-tion. new tool shapesorter can predict evolutionarily conserved rna secondary structure features while simultaneously taking experimental shape probing evidence into account. the web-server r-chie which visualises rna secondary structure information in terms of arc diagrams, can now be used to also visu-alise and intuitively compare rna-rna, rna-dna and dna-dna interactions alongside multiple se-quence alignments and quantitative information. the prediction generated by any method in e-rna can be readily visualised on the web-server. for completed tasks, users can download their results and read-ily visualise them later on with r-chie without hav-ing to re-run the predictions. e-rna can be found at http://www.e-rna.org.",AB_0495
"background: artificial intelligence (ai) programs that train on large datasets require powerful compute infrastructure consisting of several cpu cores and gpus. jupyterlab provides an excellent framework for developing ai programs, but it needs to be hosted on such an infrastructure to enable faster training of ai programs using parallel computing. findings: an open-source, docker-based, and gpu-enabled jupyterlab infrastructure is developed that runs on the public compute infrastructure of galaxy europe consisting of thousands of cpu cores, many gpus, and several petabytes of storage to rapidly prototype and develop end-to-end ai projects. using a jupyterlab notebook, long-running ai model training programs can also be executed remotely to create trained models, represented in open neural network exchange (onnx) format, and other output datasets in galaxy. other features include git integration for version control, the option of creating and executing pipelines of notebooks, and multiple dashboards and packages for monitoring compute resources and visualization, respectively. conclusions: these features make jupyterlab in galaxy europe highly suitable for creating and managing ai projects. a recent scientific publication that predicts infected regions in covid-19 computed tomography scan images is reproduced using various features of jupyterlab on galaxy europe. in addition, colabfold, a faster implementation of alphafold2, is accessed in jupyterlab to predict the 3-dimensional structure of protein sequences. jupyterlab is accessible in 2 ways-one as an interactive galaxy tool and the other by running the underlying docker container. in both ways, long-running training can be executed on galaxy's compute infrastructure. scripts to create the docker container are available under mit license at https://github.com/usegalaxy-eu/gpu-jupyterlab-docker.",AB_0495
"image semantic segmentation, a fundamental computer vision task, performs the pixel-wise classification of an image seeking to group pixels that share some semantic content. one of the main issues in semantic segmentation is the creation of fully annotated datasets where each image has one label per pixel. these annotations are highly time-consuming and, the more the labelling increases, the higher the percentage of human-entered errors grows. segmentation methods based on less supervision can reduce both labelling time and noisy labels. however, when dealing with real-world applications, it is far from trivial to establish a method that minimizes labelling time while maximizing performance. our main contribution is to present the first comprehensive study of state-of-the-art methods based on different levels of supervision. image processing baselines, unsupervised, weakly supervised and supervised approaches have been evaluated. we aim to guide anyone approaching a new real-world use case by providing a trade-off between performance and supervision complexity on datasets from different domains, such as street scenes (camvid), microscopy (metaldam), satellite (floodnet) and medical images (nucls). our experimental results suggest that: (i) unsupervised and weak learning perform well on majority classes, which helps to speed up labelling; (ii) weakly supervised can outperform fully supervised methods on minority classes; (iii) not all weak learning methods are robust to the nature of the dataset, especially those based on image-level annotations; and (iv) among all weakly supervised methods, point-based are the best-performing ones, even competing with fully supervised methods. the code is available at https://github.com/martafdezmam/lessen_ supervision.",AB_0495
"this work presents a novel deep-learning-based pipeline for the inverse problem of image deblurring, leveraging augmentation and pre-training with synthetic data. our results build on our winning submission to the recent helsinki deblur challenge 2021, whose goal was to explore the limits of stateof-the-art deblurring algorithms in a real-world data setting. the task of the challenge was to deblur out-of-focus images of random text, thereby in a downstream task, maximizing an optical-character-recognition-based score function. a key step of our solution is the data-driven estimation of the physical forward model describing the blur process. this enables a stream of synthetic data, generating pairs of ground-truth and blurry images on-the-fly, which is used for an extensive augmentation of the small amount of challenge data provided. the actual deblurring pipeline consists of an approximate inversion of the radial lens distortion (determined by the estimated forward model) and a u-net architecture, which is trained end-to-end. our algorithm was the only one passing the hardest challenge level, achieving over 70% character recognition accuracy. our findings are well in line with the paradigm of data-centric machine learning, and we demonstrate its effectiveness in the context of inverse problems. apart from a detailed presentation of our methodology, we also analyze the importance of several design choices in a series of ablation studies. the code of our challenge submission is available under https://github.com/theophil-trippe/hdc_tuberlin_version_1.",AB_0495
"background: genome sequencing efforts for individuals with rare mendelian disease have increased the research focus on the noncoding genome and the clinical need for methods that prioritize potentially disease causal noncoding variants. some tools for assessment of variant pathogenicity as well as annotations are not available for the current human genome build (grch38), for which the adoption in databases, software, and pipelines was slow. results: here, we present an updated version of the regulatory mendelian mutation (remm) score, retrained on features and variants derived from the grch38 genome build. like its grch37 version, it achieves good performance on its highly imbalanced data. to improve accessibility and provide users with a toolbox to score their variant files and look up scores in the genome, we developed a website and api for easy score lookup. conclusions: scores of the grch38 genome build are highly correlated to the prior release with a performance increase due to the better coverage of features. for prioritization of noncoding mutations in imbalanced datasets, the remm score performed much better than other variation scores. prescored whole-genome files of grch37 and grch38 genome builds are cited in the article and the website; ucsc genome browser tracks, and an api are available at https://remm.bihealth.org.",AB_0495
"the detection of communities in graph datasets provides insight about a graph's underlying structure and is an important tool for various domains such as social sciences, marketing, traffic forecast, and drug discovery. while most existing algorithms provide fast approaches for community detection, their results usually contain strictly separated communities. however, most datasets would semantically allow for or even require overlapping communities that can only be determined at much higher computational cost. we build on an efficient algorithm, fox, that detects such overlapping communities. fox measures the closeness of a node to a community by approximating the count of triangles which that node forms with that community. we propose lazyfox, a multi-threaded adaptation of the fox algorithm, which provides even faster detection without an impact on community quality. this allows for the analyses of significantly larger and more complex datasets. lazyfox enables overlapping community detection on complex graph datasets with millions of nodes and billions of edges in days instead of weeks. as part of this work, lazyfox's implementation was published and is available as a tool under an mit licence at https://github.com/timgarrels/lazyfox.",AB_0495
"rna-binding proteins (rbps) form highly diverse and dynamic ribonucleoprotein complexes, whose functions determine the molecular fate of the bound rna. in the model organism sacchromyces cerevisiae, the number of proteins identified as rbps has greatly increased over the last decade. however, the cellular function of most of these novel rbps remains largely unexplored. we used mass spectrometry-based quantitative proteomics to systematically identify protein-protein interactions (ppis) and rna-dependent interactions (rdis) to create a novel dataset for 40 rbps that are associated with the mrna life cycle. domain, functional and pathway enrichment analyses revealed an over-representation of rna functionalities among the enriched interactors. using our extensive ppi and rdi networks, we revealed putative new members of rna-associated pathways, and highlighted potential new roles for several rbps. our rbp interactome resource is available through an online interactive platform as a community tool to guide further in-depth functional studies and rbp network analysis (https://www.butterlab.org/rine). [graphics] .",AB_0495
"a vast number of microarray datasets have been produced as a way to identify differentially expressed genes and gene expression signatures. a better understanding of these biological processes can help in the diagnosis and prognosis of diseases, as well as in the therapeutic response to drugs. however, most of the available datasets are composed of a reduced number of samples, leading to low statistical, predictive and generalization power. one way to overcome this problem is by merging several microarray datasets into a single dataset, which is typically a challenging task. statistical methods or supervised machine learning algorithms are usually used to determine gene expression signatures. nevertheless, statistical methods require an arbitrary threshold to be defined, and supervised machine learning methods can be ineffective when applied to high-dimensional datasets like microarrays. we propose a methodology to identify gene expression signatures by merging microarray datasets. this methodology uses statistical methods to obtain several sets of differentially expressed genes and uses supervised machine learning algorithms to select the gene expression signature. this methodology was validated using two distinct research applications: one using heart failure and the other using autism spectrum disorder microarray datasets. for the first, we obtained a gene expression signature composed of 117 genes, with a classification accuracy of approximately 98%. for the second use case, we obtained a gene expression signature composed of 79 genes, with a classification accuracy of approximately 82%. this methodology was implemented in r language and is available, under the mit licence, at https://github.com/bioinformatics-ua/microges.",AB_0495
