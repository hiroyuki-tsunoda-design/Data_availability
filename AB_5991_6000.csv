AB,NO
"most existing fair classifiers rely on sensitive attributes to achieve fairness. however, for many scenarios, we cannot obtain sensitive attributes due to privacy and legal issues. the lack of sensitive attributes challenges many existing fair classifiers. though we lack sensitive attributes, for many applications, there usually exists features/information of various formats that are relevant to sensitive attributes. for example, a person's purchase history can reflect his/her race, which would help for learning fair classifiers on race. however, the work on exploring relevant features for learning fair models without sensitive attributes is rather limited. therefore, in this paper, we study a novel problem of learning fair models without sensitive attributes by exploring relevant features. we propose a probabilistic generative framework to effectively estimate the sensitive attribute from the training data with relevant features in various formats and utilize the estimated sensitive attribute information to learn fair models. experimental results on real-world datasets show the effectiveness of our framework in terms of both accuracy and fairness. our source code is available at: https://github.com/huaishengzhu/fairws.",AB_0600
"spatial genetic variation is shaped in part by an organism's dispersal ability. we present a deep learning tool, dispersenn2, for estimating the mean per-generation dispersal distance from georeferenced polymorphism data. our neural network performs feature extraction on pairs of genotypes, and uses the geographic information that comes with each sample. these attributes led dispersenn2 to outperform a state-of-the-art deep learning method that does not use explicit spatial information: the mean relative absolute error was reduced by 33% and 48% using sample sizes of 10 and 100 individuals, respectively. dispersenn2 is particularly useful for non-model organisms or systems with sparse genomic resources, as it uses unphased, single nucleotide polymorphisms as its input. the software is open source and available from https://github.com/kr-colab/dispersenn2, with documentation located at https://dispersenn2.readthedocs.io/en/latest/.",AB_0600
"motivation: nuclear magnetic resonance spectroscopy (nmr) is widely used to analyze metabolites in biological samples, but the analysis requires specific expertise, it is time-consuming, and can be inaccurate. here, we present a powerful automate tool, spatial clustering algorithm-statistical total correlation spectroscopy (spa-stocsy), which overcomes challenges faced when analyzing nmr data and identifies metabolites in a sample with high accuracy. results: as a data-driven method, spa-stocsy estimates all parameters from the input dataset. it first investigates the covariance pattern among datapoints and then calculates the optimal threshold with which to cluster datapoints belonging to the same structural unit, i.e. the metabolite. generated clusters are then automatically linked to a metabolite library to identify candidates. to assess spa-stocsy's efficiency and accuracy, we applied it to synthesized spectra and spectra acquired on drosophila melanogaster tissue and human embryonic stem cells. in the synthesized spectra, spa outperformed statistical recoupling of variables (srv), an existing method for clustering spectral peaks, by capturing a higher percentage of the signal regions and the close-to-zero noise regions. in the biological data, spa-stocsy performed comparably to the operator-based chenomx analysis while avoiding operator bias, and it required <7 min of total computation time. overall, spa-stocsy is a fast, accurate, and unbiased tool for untargeted analysis of metabolites in the nmr spectra. it may thus accelerate the use of nmr for scientific discoveries, medical diagnostics, and patient-specific decision making. availability and implementation: the codes of spa-stocsy are available at https://github.com/liuzlab/spa-stocsy.",AB_0600
"motivation: precise identification of cancer cells in patient samples is essential for accurate diagnosis and clinical monitoring but has been a significant challenge in machine learning approaches for cancer precision medicine. in most scenarios, training data are only available with disease annotation at the subject or sample level. traditional approaches separate the classification process into multiple steps that are optimized independently. recent methods either focus on predicting sample-level diagnosis without identifying individual pathologic cells or are less effective for identifying heterogeneous cancer cell phenotypes. results: we developed a generalized end-to-end differentiable model, the cell scoring neural network (csnn), which takes sample-level training data and predicts the diagnosis of the testing samples and the identity of the diagnostic cells in the sample, simultaneously. the cell-level density differences between samples are linked to the sample diagnosis, which allows the probabilities of individual cells being diagnostic to be calculated using backpropagation. we applied csnn to two independent clinical flow cytometry datasets for leukemia diagnosis. in both qualitative and quantitative assessments, csnn outperformed preexisting neural network modeling approaches for both cancer diagnosis and cell-level classification. post hoc decision trees and 2d dot plots were generated for interpretation of the identified cancer cells, showing that the identified cell phenotypes match the cancer endotypes observed clinically in patient cohorts. independent data clustering analysis confirmed the identified cancer cell populations. availability and implementation: the source code of csnn and datasets used in the experiments are publicly available on github (http:// github.com/erobl/csnn). raw fcs files can be downloaded from flowrepository (id: fr-fcm-z6yk).",AB_0600
"motivation: dna-based data storage is a quickly growing field that hopes to harness the massive theoretical information density of dna molecules to produce a competitive next-generation storage medium suitable for archival data. in recent years, many dna-based storage system designs have been proposed. given that no common infrastructure exists for simulating these storage systems, comparing many different designs along with many different error models is increasingly difficult. to address this challenge, we introduce framed, a simulation infrastructure for dna storage systems that leverages the underlying modularity of dna storage system designs to provide a framework to express different designs while being able to reuse common components. results: we demonstrate the utility of framed and the need for a common simulation platform using a case study. our case study compares designs that utilize strand copies differently, some that align strand copies using multiple sequence alignment algorithms and others that do not. we found that the choice to include multiple sequence alignment in the pipeline is dependent on the error rate and the type of errors being injected and is not always beneficial. in addition to supporting a wide range of designs, framed provides the user with transparent parallelism to deal with a large number of reads from sequencing and the need for many fault injection iterations. we believe that framed fills a void in the tools publicly available to the dna storage community by providing a modular and extensible framework with support for massive parallelism. as a result, it will help accelerate the design process of future dna-based storage systems. availability and implementation: the source code for framed along with the data generated during the demonstration of framed is available in a public github repository at https://github.com/dna-storage/framed, (https://dx.doi.org/10.5281/zenodo.7757762).",AB_0600
"motivation evaluating the gene completeness is critical to measuring the quality of a genome assembly. an incomplete assembly can lead to errors in gene predictions, annotation, and other downstream analyses. benchmarking universal single-copy orthologs (busco) is a widely used tool for assessing the completeness of genome assembly by testing the presence of a set of single-copy orthologs conserved across a wide range of taxa. however, busco is slow particularly for large genome assemblies. it is cumbersome to apply busco to a large number of assemblies.results here, we present compleasm, an efficient tool for assessing the completeness of genome assemblies. compleasm utilizes the miniprot protein-to-genome aligner and the conserved orthologous genes from busco. it is 14 times faster than busco for human assemblies and reports a more accurate completeness of 99.6% than busco's 95.7%, which is in close agreement with the annotation completeness of 99.5% for t2t-chm13.availability and implementation https://github.com/huangnengcsu/compleasm.",AB_0600
"in protein evolution, diversification is generally driven by genetic duplication. the hallmarks of this mech-anism are visible in the repeating topology of various proteins. in outer membrane (3-barrels, duplication is visible with (3-hairpins as the repeating unit of the barrel. in contrast to the overall use of duplication in diversification, a computational study hypothesized evolutionary mechanisms other than hairpin duplica-tions leading to increases in the number of strands in outer membrane (3-barrels. specifically, the topology of some 16-and 18-stranded (3-barrels appear to have evolved through a loop to (3-hairpin transition. here we test this novel evolutionary mechanism by creating a chimeric protein from an 18-stranded (3-barrel and an evolutionarily related 16-stranded (3-barrel. the chimeric combination of the two was created by replacing loop l3 of the 16-stranded barrel with the sequentially matched transmembrane (3-hairpin region of the 18-stranded barrel. we find the resulting chimeric protein is stable and has characteristics of increased strand number. this study provides the first experimental evidence supporting the evolution through a loop to (3-hairpin transition.(c) 2023 the author(s). published by elsevier ltd. this is an open access article under the cc by-nc-nd license (http://crea-tivecommons.org/licenses/by-nc-nd/4.0/).",AB_0600
"-purpose: we aimed to catalog past and present clinical trials on immunotherapy treatments for glioblastoma (gbm) and discover relevant trends in this field. -methods: former and ongoing clinical trials involving the use of immunotherapy to treat gbm were queried in july 2022 within the clinicaltrials.gov registry (https:// clinicaltrials.gov/). pertinent trials were categorized by variables including immunotherapy classification, tumor type (newly diagnosed versus recurrent), country of origin, start date, clinical phase, study completion status, esti-mated subject enrollment, design, publication status, and funding source. -results: a list of 173 trials was identified in total. the -umber of immunotherapy clinical trials to treat gbm has increased over time. the largest proportion of trials were gene therapies (97 studies; 56.1%) and viral therapies (37 studies; 21.4%). studies were designated as a biologic (45.1%), drug (43.9%), genetic (2.3%), or procedure (1.2%). trials spanned 19 countries; china, the second largest contributor (5.8%) after the united states (70.0%), has increased clinical trial development in the past years. the average time to completion was 52.3 months. trials were primarily funded by academic centers; however, one-fourth of the trials were funded by industry and 2 were funded by foundations. one-t of the trials were active and over one-third were linked to publications. -conclusions: our findings provide a comprehensive summary of the state of immunotherapy clinical trials for gbm, highlighting the evolving nature and growing scope of this field.",AB_0600
"railroad inspections to identify missing track components are crucial to railroad operational safety. this paper presents a new lightweight computer vision model on edge devices for accurate, real-time rail track inspection. it modifies the teacher-student guidance mechanism in nanodet (https://github.com/rangilyu/nanodet) by introducing a new adaptively weighted loss (awl) to the training process. the awl evaluates the teacher and student model qualities, determines the weight of the student loss, and then balances their loss contributions on-the-fly, gearing the training process toward proper knowledge distillation and guidance. compared to sota models, our awl-nanodet features a tiny model size of less than 10 mb and a computation cost of 1.52 g flops and achieves an processing time of less than 14 ms per frame when tested on nvidia's agx orin. relative to native nanodet, it also notably improves the model's performance by nearly 10%, enabling highly accurate, real-time detection of track components.",AB_0600
"we present the phippery software suite for analyzing data from phage display methods that use immunoprecipitation and deep sequencing to capture antibody binding to peptides, often referred to as phip-seq. it has three main components that can be used separately or in conjunction: (i) a nextflow pipeline, phip-flow, to process raw sequencing data into a compact, multidimensional dataset format and allows for end-to-end automation of reproducible workflows. (ii) a python api, phippery, which provides interfaces for tasks such as count normalization, enrichment calculation, multidimensional scaling, and more, and (iii) a streamlit application, phip-viz, as an interactive interface for visualizing the data as a heatmap in a flexible manner.availability and implementation all software packages are publicly available under the mit license. the phip-flow pipeline: https://github.com/matsengrp/phip-flow. the phippery library: https://github.com/matsengrp/phippery. the phip-viz streamlit application: https://github.com/matsengrp/phip-viz.",AB_0600
